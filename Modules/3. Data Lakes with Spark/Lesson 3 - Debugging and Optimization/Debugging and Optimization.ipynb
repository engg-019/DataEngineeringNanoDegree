{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Local Mode to Cluster Mode\n",
    "Spark provides 3 options working on a cluster.\n",
    "<img src=\"images/spark_modes.png\">\n",
    "\n",
    "MESOS and YARN are for sharing the spark cluster between teams. So we will stick with Standalone mode. \n",
    "\n",
    "With big data the data is too big to fit on the single computer so it is kept on clusters. As a data scientist you'll run the spark jobs on the data stored in external database or a third-party storage rented from a Cloud computing provider like Amazon.\n",
    "\n",
    "<img src=\"images/spark_bigdata.png\">\n",
    "\n",
    "To build a spark cluster you have to options:\n",
    "1. Buy computers and build cluster \n",
    "2. Use cloud platforms like amazon web services and rent a cluster of machines and expand or shrink the cluster size as you need. Just login from anywhere to use the clusters.\n",
    "\n",
    "Our setup will look like this:\n",
    "\n",
    "<img src=\"images/rented_spark_cluster.png\">\n",
    "\n",
    "The Data will be stored on S3 storage and then machines for spark will be rented using EC2 service of AWS services. And then we'll login to the spark cluster remotely and submit the job to the cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Instructions AWS\n",
    "If you want to create a spark cluster manually you can follow this [guide]( https://blog.insightdatascience.com/spinning-up-a-spark-cluster-on-spot-instances-step-by-step-e8ed14ebb3b). However its quite tedious and you have to perform same steps for multiple machines and if you have to update something you have to do it several times. Fortunately AWS offers an easier option called Elastic Map Reduce or EMR for short. EMR provides you EC2 instances with big data technologies installed.\n",
    "Now following are the instructions to setup EMR cluster:\n",
    "1. Create ssh key pair to securely connect to the cluster. To do this go to the EC2 service. Select **`Key pairs`** in **`NETWORK & SECURITY`** and create a key pair. Named it as `spark-cluster` and a `pem` file will be downloaded for you. \n",
    "2. Go to EMR service and create a cluster by naming it and according to the requirements. Make sure to select `Spark` in **`Software configuration`** section. Select the instance type according to your requirements. \n",
    "To compare different EC2 instance type either go [here](https://aws.amazon.com/ec2/instance-types/) or [here](https://ec2instances.info/). Select EC2 key pair and create cluster.\n",
    "For more follow this [tutorial](https://www.youtube.com/watch?v=ZVdAEMGDFdo) on youtube."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Notebooks on your Cluster\n",
    "After the cluster is in `Waiting` state connect to the cluster. Amazon has multiple ways to connect to the cluster. We'll use `Notebook` by clicking on the left side in the menu. Click on `create notebook`. Then name your notebook and attach a cluster to it and leave the rest to default and create the cluster. For more elaboration following this [tutorial](https://www.youtube.com/watch?v=EcIYPkCkehY) from **Udacity** on youtube."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Scipts\n",
    "Up until now jupyter notebooks were used. They have the following advantages:\n",
    "1. Good for prototyping\n",
    "2. Exploring and visualizing the data\n",
    "3. Easily share the results with others\n",
    "\n",
    "But Jupyter notebooks are not good for automating the workflows. That's where scipts come in to play. For more elaboration following this [tutorial](https://www.youtube.com/watch?v=bfOocPv54EI) from **Udacity** on Youtube."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting Spark Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lower_scripts.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lower_scripts.py\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "        example program to show how to submit applications\n",
    "    \"\"\"\n",
    "    spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .appName('LowerSongTitles')\\\n",
    "            .getOrCreate()\n",
    "\n",
    "    log_of_songs = [\n",
    "        \"Despacito\",\n",
    "        \"Nice for what\",\n",
    "        \"No tears left to cry\",\n",
    "        \"Despacito\",\n",
    "        \"Havana\",\n",
    "        \"In my feelings\",\n",
    "        \"Nice for what\",\n",
    "        \"Despacito\",\n",
    "        \"All the stars\"\n",
    "    ]\n",
    "    \n",
    "    distributed_song_log = spark.sparkContext.parallelize(log_of_songs)\n",
    "    \n",
    "    print(distributed_song_log.map(lambda x:x.lower()).collect())\n",
    "    \n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change the pem file permissions using the follwing linux command:\n",
    "\n",
    "    chmod 600 spark-cluster.pem\n",
    "and connect to the master node. Use the following command to run the spark job:\n",
    "\n",
    "    spark-submit --master yarn ./lower_script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Writing to Amazon S3\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
