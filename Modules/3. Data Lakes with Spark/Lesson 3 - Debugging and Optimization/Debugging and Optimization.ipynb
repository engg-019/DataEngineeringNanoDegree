{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Local Mode to Cluster Mode\n",
    "Spark provides 3 options working on a cluster.\n",
    "<img src=\"images/spark_modes.png\">\n",
    "\n",
    "MESOS and YARN are for sharing the spark cluster between teams. So we will stick with Standalone mode. \n",
    "\n",
    "With big data the data is too big to fit on the single computer so it is kept on clusters. As a data scientist you'll run the spark jobs on the data stored in external database or a third-party storage rented from a Cloud computing provider like Amazon.\n",
    "\n",
    "<img src=\"images/spark_bigdata.png\">\n",
    "\n",
    "To build a spark cluster you have to options:\n",
    "1. Buy computers and build cluster \n",
    "2. Use cloud platforms like amazon web services and rent a cluster of machines and expand or shrink the cluster size as you need. Just login from anywhere to use the clusters.\n",
    "\n",
    "Our setup will look like this:\n",
    "\n",
    "<img src=\"images/rented_spark_cluster.png\">\n",
    "\n",
    "The Data will be stored on S3 storage and then machines for spark will be rented using EC2 service of AWS services. And then we'll login to the spark cluster remotely and submit the job to the cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Instructions AWS\n",
    "If you want to create a spark cluster manually you can follow this [guide]( https://blog.insightdatascience.com/spinning-up-a-spark-cluster-on-spot-instances-step-by-step-e8ed14ebb3b). However its quite tedious and you have to perform same steps for multiple machines and if you have to update something you have to do it several times. Fortunately AWS offers an easier option called Elastic Map Reduce or EMR for short. EMR provides you EC2 instances with big data technologies installed.\n",
    "Now following are the instructions to setup EMR cluster:\n",
    "1. Create ssh key pair to securely connect to the cluster. To do this go to the EC2 service. Select **`Key pairs`** in **`NETWORK & SECURITY`** and create a key pair. Named it as `spark-cluster` and a `pem` file will be downloaded for you. \n",
    "2. Go to EMR service and create a cluster by naming it and according to the requirements. Make sure to select `Spark` in **`Software configuration`** section. Select the instance type according to your requirements. \n",
    "To compare different EC2 instance type either go [here](https://aws.amazon.com/ec2/instance-types/) or [here](https://ec2instances.info/). Select EC2 key pair and create cluster.\n",
    "For more follow this [tutorial](https://www.youtube.com/watch?v=ZVdAEMGDFdo) on youtube."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating EMR Cluster Using Boto3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = pd.read_csv('credentials/credentials.csv')\n",
    "ACCESS_KEY = credentials['Access key ID'][0]\n",
    "SECRET_ACCESS_KEY = credentials['Secret access key'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emr = boto3.client('emr',\n",
    "                   region_name = 'us-west-2',\n",
    "                   aws_access_key_id = ACCESS_KEY,\n",
    "                   aws_secret_access_key = SECRET_ACCESS_KEY)\n",
    "\n",
    "ec2 = boto3.resource('ec2', \n",
    "                     region_name = 'us-west-2',\n",
    "                     aws_access_key_id = ACCESS_KEY,\n",
    "                     aws_secret_access_key = SECRET_ACCESS_KEY)\n",
    "client = boto3.client('ec2',\n",
    "                      region_name = 'us-west-2',\n",
    "                      aws_access_key_id = ACCESS_KEY,\n",
    "                      aws_secret_access_key = SECRET_ACCESS_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "VPC = client.describe_vpcs()['Vpcs'][0]['VpcId']\n",
    "subnet = client.create_subnet(CidrBlock='172.31.0.0/16',VpcId=VPC,AvailabilityZone='us-west-2a')\n",
    "SUBNET = subnet['Subnet']['SubnetId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster created with the step... j-22N0Z19JS1VSW\n"
     ]
    }
   ],
   "source": [
    "cluster_id = emr.run_job_flow(\n",
    "    Name='spark-cluster',\n",
    "    LogUri='s3://naqeeb-emr-test/logs',\n",
    "    ReleaseLabel='emr-5.28.0',\n",
    "    Applications=[\n",
    "        {\n",
    "            'Name': 'Spark'\n",
    "        },\n",
    "    ],\n",
    "    Instances={\n",
    "        'InstanceGroups': [\n",
    "            {\n",
    "                'Name': \"Master nodes\",\n",
    "                'Market': 'ON_DEMAND',\n",
    "                'InstanceRole': 'MASTER',\n",
    "                'InstanceType': 'm5.xlarge',\n",
    "                'InstanceCount': 1,\n",
    "            },\n",
    "            {\n",
    "                'Name': \"Slave nodes\",\n",
    "                'Market': 'ON_DEMAND',\n",
    "                'InstanceRole': 'CORE',\n",
    "                'InstanceType': 'm5.xlarge',\n",
    "                'InstanceCount': 3,\n",
    "            }\n",
    "        ],\n",
    "        'Ec2KeyName': 'spark-cluster',\n",
    "        'KeepJobFlowAliveWhenNoSteps': True,\n",
    "        'TerminationProtected': False,\n",
    "        'Ec2SubnetId': SUBNET,\n",
    "    },\n",
    "    VisibleToAllUsers=True,\n",
    "    JobFlowRole='EMR_EC2_DefaultRole',\n",
    "    ServiceRole='EMR_DefaultRole'\n",
    ")\n",
    "\n",
    "print ('cluster created with the step...', cluster_id['JobFlowId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting EMR Cluster Using Boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '677070d1-844b-4019-a0b2-5baf0dbfcd6f',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '677070d1-844b-4019-a0b2-5baf0dbfcd6f',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Thu, 09 Jan 2020 10:43:48 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emr.terminate_job_flows(JobFlowIds=[\n",
    "        cluster_id['JobFlowId'],\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'd5f27889-1087-4813-9720-b2307f7b8ee6',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'content-type': 'text/xml;charset=UTF-8',\n",
       "   'content-length': '225',\n",
       "   'date': 'Thu, 09 Jan 2020 10:33:04 GMT',\n",
       "   'server': 'AmazonEC2'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.delete_subnet(SubnetId=SUBNET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Notebooks on your Cluster\n",
    "After the cluster is in `Waiting` state connect to the cluster. Amazon has multiple ways to connect to the cluster. We'll use `Notebook` by clicking on the left side in the menu. Click on `create notebook`. Then name your notebook and attach a cluster to it and leave the rest to default and create the cluster. For more elaboration following this [tutorial](https://www.youtube.com/watch?v=EcIYPkCkehY) from **Udacity** on youtube.\n",
    "\n",
    "Note: When you are using Notebook change kernel according to the environment of your choice (in this case to PySpark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Scipts\n",
    "Up until now jupyter notebooks were used. They have the following advantages:\n",
    "1. Good for prototyping\n",
    "2. Exploring and visualizing the data\n",
    "3. Easily share the results with others\n",
    "\n",
    "But Jupyter notebooks are not good for automating the workflows. That's where scipts come in to play. For more elaboration following this [tutorial](https://www.youtube.com/watch?v=bfOocPv54EI) from **Udacity** on Youtube."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting Spark Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lower_scripts.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lower_scripts.py\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "        example program to show how to submit applications\n",
    "    \"\"\"\n",
    "    spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .appName('LowerSongTitles')\\\n",
    "            .getOrCreate()\n",
    "\n",
    "    log_of_songs = [\n",
    "        \"Despacito\",\n",
    "        \"Nice for what\",\n",
    "        \"No tears left to cry\",\n",
    "        \"Despacito\",\n",
    "        \"Havana\",\n",
    "        \"In my feelings\",\n",
    "        \"Nice for what\",\n",
    "        \"Despacito\",\n",
    "        \"All the stars\"\n",
    "    ]\n",
    "    \n",
    "    distributed_song_log = spark.sparkContext.parallelize(log_of_songs)\n",
    "    \n",
    "    print(distributed_song_log.map(lambda x:x.lower()).collect())\n",
    "    \n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect to the change the cluster using pem file change the permissions of pem file using the follwing linux command:\n",
    "\n",
    "    chmod 600 your_pem_file.pem\n",
    "Connect to the master node according to the instructions on AWS console and use the following command to run the spark job:\n",
    "\n",
    "    spark-submit --master yarn ./lower_script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing and Retrieving Data on the cloud\n",
    "We'll be using Amazon Simple Storage Service S3 for short. It is:\n",
    "* Safe\n",
    "* Easy to use\n",
    "* Cheap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using spark to read from and write data to S3 bucket\n",
    "Spark can reads from and writes data to S3 bucket just by putting the S3 bucket like following:\n",
    "\n",
    "    spark.read.json('s3n://bucket_name/file.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to HDFS\n",
    "When you use S3 you are separating Data storage from the spark cluster. One of downside is that you have to download the data across the network which can be a bottleneck. Another solution is to store the data on spark cluster with HDFS. But there's a trade off to HDFS i.e. you have to maintain and fix the system yourself. S3 is easier since we don't need to maintain the cluster. Also if you rent cluster from AWS then the data usually doesn't have to go too far in the network since the cluster hardware and S3 hardware are both on Amazon's data centers.Finally spark is smart enough to download a small chunk of data and process that chunk while waiting for the rest to download. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Writing to HDFS\n",
    "Similar to how we can read and write to S3 we can read and write to HDFS. The only difference is the path e.g.\n",
    "\n",
    "    spark.read.csv('hdfs:///user/sparkify_data/data.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging is Hard\n",
    "When you are in local mode the errors show up directly. But once you are in cluster of machines errors in the code can be very hard to diagnose. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntax Errors\n",
    "Suppose you have written your Spark program but there's some bug in you code. The code seems to work just fine but you have to remember the lazy evaluation of spark. Spark waits as long as it can before running your code on data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .config('spark.ui.port',3000)\\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = spark.read.json('data/sparkify_log_small.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist='Showaddywaddy', auth='Logged In', firstName='Kenneth', gender='M', itemInSession=112, lastName='Matthews', length=232.93342, level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration=1509380319284, sessionId=5132, song='Christmas Tears Will Fall', status=200, ts=1513720872284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'wher'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-63abaf51e35a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlog\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'userId'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'firstname'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'page'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'song'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0mwher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muserId\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'1046'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1299\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1300\u001b[0m             raise AttributeError(\n\u001b[1;32m-> 1301\u001b[1;33m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[0;32m   1302\u001b[0m         \u001b[0mjc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'wher'"
     ]
    }
   ],
   "source": [
    "log = logs.select(['userId','firstname','page','song'])\\\n",
    "        .wher(logs.userId == '1046')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the type of the syntax error and it can be debugged very easily as it can be seen that there's a typo in the code if we replace `wher` with `where` the code will work fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logs.select(['userId','firstname','page','song'])\\\n",
    "        .where(logs.userId == '1046')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "userId = log.groupby('userId').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId='1046', count=30)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userId.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs2 = logs.withColumn('artist',logs.artist + 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logs2.crossJoin(logs).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it means we ran out of memory. we'll just take 5 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist=None, auth='Logged In', firstName='Kenneth', gender='M', itemInSession=112, lastName='Matthews', length=232.93342, level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration=1509380319284, sessionId=5132, song='Christmas Tears Will Fall', status=200, ts=1513720872284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046', artist='Showaddywaddy', auth='Logged In', firstName='Kenneth', gender='M', itemInSession=112, lastName='Matthews', length=232.93342, level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration=1509380319284, sessionId=5132, song='Christmas Tears Will Fall', status=200, ts=1513720872284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046'),\n",
       " Row(artist=None, auth='Logged In', firstName='Elizabeth', gender='F', itemInSession=7, lastName='Chase', length=195.23873, level='free', location='Shreveport-Bossier City, LA', method='PUT', page='NextSong', registration=1512718541284, sessionId=5027, song='Cheryl Tweedy', status=200, ts=1513720878284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', userId='1000', artist='Showaddywaddy', auth='Logged In', firstName='Kenneth', gender='M', itemInSession=112, lastName='Matthews', length=232.93342, level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration=1509380319284, sessionId=5132, song='Christmas Tears Will Fall', status=200, ts=1513720872284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046'),\n",
       " Row(artist=None, auth='Logged In', firstName='Vera', gender='F', itemInSession=6, lastName='Blackwell', length=196.20526, level='paid', location='Racine, WI', method='PUT', page='NextSong', registration=1499855749284, sessionId=5516, song='Good Girls Go Bad (Feat.Leighton Meester) (Album Version)', status=200, ts=1513720881284, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.78.2 (KHTML, like Gecko) Version/7.0.6 Safari/537.78.2\"', userId='2219', artist='Showaddywaddy', auth='Logged In', firstName='Kenneth', gender='M', itemInSession=112, lastName='Matthews', length=232.93342, level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration=1509380319284, sessionId=5132, song='Christmas Tears Will Fall', status=200, ts=1513720872284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046'),\n",
       " Row(artist=None, auth='Logged In', firstName='Sophee', gender='F', itemInSession=8, lastName='Barker', length=405.99465, level='paid', location='San Luis Obispo-Paso Robles-Arroyo Grande, CA', method='PUT', page='NextSong', registration=1513009647284, sessionId=2372, song=\"Don't See The Point\", status=200, ts=1513720905284, userAgent='\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', userId='2373', artist='Showaddywaddy', auth='Logged In', firstName='Kenneth', gender='M', itemInSession=112, lastName='Matthews', length=232.93342, level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration=1509380319284, sessionId=5132, song='Christmas Tears Will Fall', status=200, ts=1513720872284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046'),\n",
       " Row(artist=None, auth='Logged In', firstName='Jordyn', gender='F', itemInSession=0, lastName='Jones', length=None, level='free', location='Syracuse, NY', method='GET', page='Home', registration=1513648531284, sessionId=1746, song=None, status=200, ts=1513720913284, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.94 Safari/537.36\"', userId='1747', artist='Showaddywaddy', auth='Logged In', firstName='Kenneth', gender='M', itemInSession=112, lastName='Matthews', length=232.93342, level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration=1509380319284, sessionId=5132, song='Christmas Tears Will Fall', status=200, ts=1513720872284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs2.crossJoin(logs).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = logs.where(logs.page == 'NextSong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist='Showaddywaddy', auth='Logged In', firstName='Kenneth', gender='M', itemInSession=112, lastName='Matthews', length=232.93342, level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration=1509380319284, sessionId=5132, song='Christmas Tears Will Fall', status=200, ts=1513720872284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|userId|       sum(length)|\n",
      "+------+------------------+\n",
      "|  2904|         348.57751|\n",
      "|   691|         808.98476|\n",
      "|  2294|13926.819139999998|\n",
      "|  2162|        8289.81315|\n",
      "|  1436|         633.39011|\n",
      "|  2088|3310.0480000000002|\n",
      "|  2275|         1172.1913|\n",
      "|  2756|1076.6344800000002|\n",
      "|   800|         517.17134|\n",
      "|  1394| 5989.630679999999|\n",
      "|   451|         433.44889|\n",
      "|   926|1087.8414400000001|\n",
      "|  2696|         200.95955|\n",
      "|   870|         463.51583|\n",
      "|     7| 533.9419499999999|\n",
      "|  1903|        1058.81895|\n",
      "|   591|         219.79383|\n",
      "|   613|         419.26439|\n",
      "|   574|        1286.55491|\n",
      "|   307|         281.28608|\n",
      "+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songs.groupby('userId').agg(fn.sum(songs.length)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Errors\n",
    "Even if you have perfectly written code your data might have errors like missing values or unexpected Unicode characters. These type of issues shouldn't crash your program. So suppose you run your code on the subset of the data and it works fine now you run the same code on full data set and it throws the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: Data Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .config('spark.ui.port',3000)\\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs3 = spark.read.json('data/sparkify_log_small_errors.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this file has some records that don't conform to the schema. Let's print the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen that there's a new field corrupt record . Let's look at the corrupt records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_corrupt_record='{\"ts\":a,\"userId\":\"1035\",\"sessionId\":5698,\"page\":\"NextSong\",\"auth\":\"Logged In\",\"method\":\"PUT\",\"status\":200,\"level\":\"paid\",\"itemInSession\":24,\"location\":\"Santa Cruz-Watsonville, CA\",\"userAgent\":\"\\\\\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\\\\\"\",\"lastName\":\"Gillespie\",\"firstName\":\"Connor\",\"registration\":1506639389284,\"gender\":\"M\",\"artist\":\"Spoon\",\"song\":\"Black Like Me\",\"length\":205.94893}', artist=None, auth=None, firstName=None, gender=None, itemInSession=None, lastName=None, length=None, level=None, location=None, method=None, page=None, registration=None, sessionId=None, song=None, status=None, ts=None, userAgent=None, userId=None),\n",
       " Row(_corrupt_record='{\"ts\":b,\"userId\":\"2373\",\"sessionId\":2372,\"page\":\"NextSong\",\"auth\":\"Logged In\",\"method\":\"PUT\",\"status\":200,\"level\":\"paid\",\"itemInSession\":13,\"location\":\"San Luis Obispo-Paso Robles-Arroyo Grande, CA\",\"userAgent\":\"\\\\\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\\\\\"\",\"lastName\":\"Barker\",\"firstName\":\"Sophee\",\"registration\":1513009647284,\"gender\":\"F\",\"artist\":\"Cat Power\",\"song\":\"What Would The Community Think\",\"length\":270.2624}', artist=None, auth=None, firstName=None, gender=None, itemInSession=None, lastName=None, length=None, level=None, location=None, method=None, page=None, registration=None, sessionId=None, song=None, status=None, ts=None, userAgent=None, userId=None),\n",
       " Row(_corrupt_record='{\"ts\":c,\"userId\":\"184\",\"sessionId\":5659,\"page\":\"NextSong\",\"auth\":\"Logged In\",\"method\":\"PUT\",\"status\":200,\"level\":\"free\",\"itemInSession\":1,\"location\":\"Atlanta-Sandy Springs-Roswell, GA\",\"userAgent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:31.0) Gecko/20100101 Firefox/31.0\",\"lastName\":\"Smith\",\"firstName\":\"Kara\",\"registration\":1508946027284,\"gender\":\"F\",\"artist\":\"Kanye West\",\"song\":\"Stronger\",\"length\":311.84934}', artist=None, auth=None, firstName=None, gender=None, itemInSession=None, lastName=None, length=None, level=None, location=None, method=None, page=None, registration=None, sessionId=None, song=None, status=None, ts=None, userAgent=None, userId=None),\n",
       " Row(_corrupt_record='{\"ts\":abc,\"userId\":\"950\",\"sessionId\":5756,\"page\":\"NextSong\",\"auth\":\"Logged In\",\"method\":\"PUT\",\"status\":200,\"level\":\"free\",\"itemInSession\":1,\"location\":\"Riverside-San Bernardino-Ontario, CA\",\"userAgent\":\"\\\\\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\\\\\"\",\"lastName\":\"Mcgee\",\"firstName\":\"Skyler\",\"registration\":1509473552284,\"gender\":\"F\",\"artist\":\"Suicide Silence\",\"song\":\"A Dead Current\",\"length\":221.04771}', artist=None, auth=None, firstName=None, gender=None, itemInSession=None, lastName=None, length=None, level=None, location=None, method=None, page=None, registration=None, sessionId=None, song=None, status=None, ts=None, userAgent=None, userId=None)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs3.where(logs3['_corrupt_record'].isNotNull()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen that the corrupt records have ts value which is string. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging your Code\n",
    "If you are writing traditional python script you might use print statements to output the values held by variables. A typical example would be outputting the i and j variables of a nested for-loop:\n",
    "    \n",
    "    for i in range(500):\n",
    "        for j in range(300):\n",
    "            print(x[i][j])\n",
    "            \n",
    "These print statements can be helpful when debugging your code. But this won't work on spark instead we need to use a special variable. \n",
    "But why can't we use print statements on a cluster? We have a driver node coordinating the tasks of various worker nodes so print statements will only run on those worker nodes. you can't see the output from them because you are not directly connected to them. Also spark makes a copy of input data every time you call a function so the original debugging variables that you created won't actually get loaded into worker nodes. Instead each worker has their own copy of these variables and only these copies get modified. The original variables stored on the driver remain unchanged making them useless for debugging. \n",
    "\n",
    "<img src=\"images/debugging.png\">\n",
    "\n",
    "To get around this limitation spark gives you special kind of variables called accumulators. Accumulators are the global variables for your entire cluster "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use Accumulators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to create an accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_records = SparkContext.accumulator(0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_records.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2nd step is to create a function that will increment the accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_incorrect_record():\n",
    "    global incorrect_records\n",
    "    incorrect_records += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to just define a udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_ts = udf(lambda x: 1 if x is not None else add_incorrect_record())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs3 = logs3\\\n",
    "            .withColumn('ts_digit',correct_ts(logs3.ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_records.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the record is 0 why is that? the reason might be we don't have any corrupt record or Spark uses lazy evaluation so we have to apply some actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+----+---------+------+--------+\n",
      "|     _corrupt_record|artist|auth|firstName|gender|itemInSession|lastName|length|level|location|method|page|registration|sessionId|song|status|  ts|userAgent|userId|ts_digit|\n",
      "+--------------------+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+----+---------+------+--------+\n",
      "|{\"ts\":a,\"userId\":...|  null|null|     null|  null|         null|    null|  null| null|    null|  null|null|        null|     null|null|  null|null|     null|  null|    null|\n",
      "|{\"ts\":b,\"userId\":...|  null|null|     null|  null|         null|    null|  null| null|    null|  null|null|        null|     null|null|  null|null|     null|  null|    null|\n",
      "|{\"ts\":c,\"userId\":...|  null|null|     null|  null|         null|    null|  null| null|    null|  null|null|        null|     null|null|  null|null|     null|  null|    null|\n",
      "|{\"ts\":abc,\"userId...|  null|null|     null|  null|         null|    null|  null| null|    null|  null|null|        null|     null|null|  null|null|     null|  null|    null|\n",
      "+--------------------+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+----+---------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs3.where(logs3['_corrupt_record'].isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_records.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_corrupt_record: string, artist: string, auth: string, firstName: string, gender: string, itemInSession: bigint, lastName: string, length: double, level: string, location: string, method: string, page: string, registration: bigint, sessionId: bigint, song: string, status: bigint, ts: bigint, userAgent: string, userId: string, ts_digit: string]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs3.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_corrupt_record=None, artist='Showaddywaddy', auth='Logged In', firstName='Kenneth', gender='M', itemInSession=112, lastName='Matthews', length=232.93342, level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration=1509380319284, sessionId=5132, song='Christmas Tears Will Fall', status=200, ts=1513720872284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046', ts_digit='1')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs3.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark WebUI\n",
    "Since debugging on a cluster is hard, Spark has a built-in user interface that you an access from your web browser. This interface, known as the web UI, helps you understand what's going on in your cluster without looking for individual workers. Spark's UI is like an EKG machine that helps you measure the health of your Spark jobs. It's a very useful tool for diagnosing issues in your code and your cluster, but it's just a tool. You still need to know how to interpret the output and know where to investigate further. When a doctor measures the patient's heart rate with an EKG, he needs to understand not only how the heart works, but also how the heart relates to other parts of the human anatomy. Understanding the Spark internals like shuffling, DAGs and stages that we discussed earlier for the Spark cluster. So, what does the web UI actually show? The web UI provides the current configuration for the cluster which can be useful for double-checking that your desired settings went into effect. The web UI also shows you the DAG, the recipe of steps for your program that we went through earlier. You'll see the DAG broken up into stages, and within each stage there are individual tasks. Tasks are the steps that the individual worker nodes are assigned. In each stage, the worker node divides up the input data and runs the task for that stage.\n",
    "\n",
    "<img src=\"images/spark-web-ui.png\">\n",
    "\n",
    "The web UI only shows the pages related to current jobs that are running. For example, you won't see any pages related to other libraries like Spark Streaming unless you are also running a streaming job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to the Spark Web UI\n",
    "Connecting to Spark's web UI is a lot like docking boats. When boats come to  a seaport, it's useful to have various types of ports so that each type of boat can dock efficiently and safely. FOr a cruise ship, you'll need a port for passenger loading. While cargo ship need a port equipped with cranes and personal speed boat would only requires a small dock or peer. As a result, they have different ports with different procedures for docking.\n",
    "\n",
    "<img src=\"images/boat-ports.png\">\n",
    "\n",
    "In the same way, it's useful to have several ways to connect data with a machine. When you transfer private data through a secure shell known as SSH, you follow a different protocol than when transferring public HTML data for a webpage using HTTP. FOr this reason, we use different ports: port `22` for SSH and port `80` for HTTP to indicate that we're transferring data using different network protocols. These commonly numbered ports follow a convention that engineers agreed upon. It's like having an agreed upon layout for each support in the world.\n",
    "\n",
    "<img src=\"images/ports-machine.png\">\n",
    "\n",
    "Spark uses several agreed upon ports for sharing information. Some ports are for machines to communicate with each other and aren't intended for users.  For example, the Spark master uses port `7077` to communicate with the worker nodes, but we'll never use it. There are few common ports that we'll use from time to time. For example, we've already seen that we usually connect to Jupyter notebooks on port `8888`. Another important port is `4040` which shows active Spark jobs. But the most useful port for you to memorize is the web UI for master node on port 8080. The web UI on `8080` shows the status of your cluster, configurations and the status of any recent jobs. \n",
    "\n",
    "<img src=\"images/spark-ports.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Familiar with the Spark UI\n",
    "Here to connect to the Spark UI as we've configured the port to `3000` by going to the link: http://localhost:3000 . Under **`Environment`** tab, we can see the different parameters of our application. The Java version, the Scala version, the name of the application and so forth so on. \n",
    "\n",
    "<img src=\"images/web-ui-environment.png\">\n",
    "\n",
    "The **`Executors`** tab gives you information about the executors, what resources do they have, how many tasks they have run successfully. In this particular case, we're looking at a Spark Local Executor, so there is only one of them. It has ran `241` tasks out of which `237` tasks are completed.\n",
    "\n",
    "<img src=\"images/web-ui-executors.png\">\n",
    "\n",
    "The storage tab is currently empty but if you have cached Rdds in your application, you can find that information here. \n",
    "\n",
    "<img src=\"images/web-ui-storage.png\">\n",
    "\n",
    "A Spark application consists of as many jobs, as many actions regarding the code. An action can be saving a data frame to a database or taking some records back to the drive for inspection. So for example after loading your data frame where we call head, that's an action triggering a job. We can see the jobs that we were around here. Jobs have further been broken into stages. If we click on the stage we can get access to the stages this job consists of.\n",
    "\n",
    "<img src= \"images/web-ui-jobs.png\">\n",
    "\n",
    "Stages are units of work that depends on one another and can be further paralyzed. For example, before joining two Data Frames, we need to finish transforming them both and just after that we can perform the actual join. The smallest unit within a stage is a task. \n",
    "\n",
    "<img src=\"images/web-ui-jobs2.png\">\n",
    "\n",
    "Tasks are a series of Spark transformations that can be run parallel on the different partitions of out Dataframe. So when we have 10 partitions we will run 10 of the same tasks to complete a stage. So in this particular case we had 200 partitions case so we ended up with 200 tasks. \n",
    "\n",
    "<img src=\"images/web-ui-stage.png\">\n",
    "\n",
    "For further understanding check this [tutorial](https://www.youtube.com/watch?v=88JQIalP84M) by Udacity on youtybe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of the Log Data\n",
    "Using log files is a bit more difficult when we are using Spark in cluster, compared to when we are running it locally. The log file is just as everything else as speed across the different nodes. Fortunately, the Spark UI provides a convenient way to look them up, so we don't need to directly access the various workers via SSH. If we had a cluster rather than local Spark running here, we would have another column called logs with two links to the standard out and standard error files for each of the workers they had. \n",
    "\n",
    "<img src=\"images/web-ui-executors.png\">\n",
    "\n",
    "In this case sine this is a local Spark application, we only have Thread Dump for the driver. Spark uses Log4j, as standard JVM library for logging. We can configure the logging level in two different ways: \n",
    "1. Edit the Log4j properties files in the `conf` directory\n",
    "2. Set in the Spark context\n",
    "\n",
    "So if we set spark log level to error we'll only see error messages in the log files. The code for doing this is the following:\n",
    "    \n",
    "    spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "If we would like to have more verbose logging then set this level to `INFO` and we'll have more information about the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
