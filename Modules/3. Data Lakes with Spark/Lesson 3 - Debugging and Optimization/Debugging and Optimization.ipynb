{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Local Mode to Cluster Mode\n",
    "Spark provides 3 options working on a cluster.\n",
    "<img src=\"images/spark_modes.png\">\n",
    "\n",
    "MESOS and YARN are for sharing the spark cluster between teams. So we will stick with Standalone mode. \n",
    "\n",
    "With big data the data is too big to fit on the single computer so it is kept on clusters. As a data scientist you'll run the spark jobs on the data stored in external database or a third-party storage rented from a Cloud computing provider like Amazon.\n",
    "\n",
    "<img src=\"images/spark_bigdata.png\">\n",
    "\n",
    "To build a spark cluster you have to options:\n",
    "1. Buy computers and build cluster \n",
    "2. Use cloud platforms like amazon web services and rent a cluster of machines and expand or shrink the cluster size as you need. Just login from anywhere to use the clusters.\n",
    "\n",
    "Our setup will look like this:\n",
    "\n",
    "<img src=\"images/rented_spark_cluster.png\">\n",
    "\n",
    "The Data will be stored on S3 storage and then machines for spark will be rented using EC2 service of AWS services. And then we'll login to the spark cluster remotely and submit the job to the cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Instructions AWS\n",
    "If you want to create a spark cluster manually you can follow this [guide]( https://blog.insightdatascience.com/spinning-up-a-spark-cluster-on-spot-instances-step-by-step-e8ed14ebb3b). However its quite tedious and you have to perform same steps for multiple machines and if you have to update something you have to do it several times. Fortunately AWS offers an easier option called Elastic Map Reduce or EMR for short. EMR provides you EC2 instances with big data technologies installed.\n",
    "Now following are the instructions to setup EMR cluster:\n",
    "1. Create ssh key pair to securely connect to the cluster. To do this go to the EC2 service. Select **`Key pairs`** in **`NETWORK & SECURITY`** and create a key pair. Named it as `spark-cluster` and a `pem` file will be downloaded for you. \n",
    "2. Go to EMR service and create a cluster by naming it and according to the requirements. Make sure to select `Spark` in **`Software configuration`** section. Select the instance type according to your requirements. \n",
    "To compare different EC2 instance type either go [here](https://aws.amazon.com/ec2/instance-types/) or [here](https://ec2instances.info/). Select EC2 key pair and create cluster.\n",
    "For more follow this [tutorial](https://www.youtube.com/watch?v=ZVdAEMGDFdo) on youtube."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating EMR Cluster Using Boto3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = pd.read_csv('credentials/credentials.csv')\n",
    "ACCESS_KEY = credentials['Access key ID'][0]\n",
    "SECRET_ACCESS_KEY = credentials['Secret access key'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emr = boto3.client('emr',\n",
    "                   region_name = 'us-west-2',\n",
    "                   aws_access_key_id = ACCESS_KEY,\n",
    "                   aws_secret_access_key = SECRET_ACCESS_KEY)\n",
    "\n",
    "ec2 = boto3.resource('ec2', \n",
    "                     region_name = 'us-west-2',\n",
    "                     aws_access_key_id = ACCESS_KEY,\n",
    "                     aws_secret_access_key = SECRET_ACCESS_KEY)\n",
    "client = boto3.client('ec2',\n",
    "                      region_name = 'us-west-2',\n",
    "                      aws_access_key_id = ACCESS_KEY,\n",
    "                      aws_secret_access_key = SECRET_ACCESS_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "VPC = client.describe_vpcs()['Vpcs'][0]['VpcId']\n",
    "subnet = client.create_subnet(CidrBlock='172.31.0.0/16',VpcId=VPC,AvailabilityZone='us-west-2a')\n",
    "SUBNET = subnet['Subnet']['SubnetId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster created with the step... j-22N0Z19JS1VSW\n"
     ]
    }
   ],
   "source": [
    "cluster_id = emr.run_job_flow(\n",
    "    Name='spark-cluster',\n",
    "    LogUri='s3://naqeeb-emr-test/logs',\n",
    "    ReleaseLabel='emr-5.28.0',\n",
    "    Applications=[\n",
    "        {\n",
    "            'Name': 'Spark'\n",
    "        },\n",
    "    ],\n",
    "    Instances={\n",
    "        'InstanceGroups': [\n",
    "            {\n",
    "                'Name': \"Master nodes\",\n",
    "                'Market': 'ON_DEMAND',\n",
    "                'InstanceRole': 'MASTER',\n",
    "                'InstanceType': 'm5.xlarge',\n",
    "                'InstanceCount': 1,\n",
    "            },\n",
    "            {\n",
    "                'Name': \"Slave nodes\",\n",
    "                'Market': 'ON_DEMAND',\n",
    "                'InstanceRole': 'CORE',\n",
    "                'InstanceType': 'm5.xlarge',\n",
    "                'InstanceCount': 3,\n",
    "            }\n",
    "        ],\n",
    "        'Ec2KeyName': 'spark-cluster',\n",
    "        'KeepJobFlowAliveWhenNoSteps': True,\n",
    "        'TerminationProtected': False,\n",
    "        'Ec2SubnetId': SUBNET,\n",
    "    },\n",
    "    VisibleToAllUsers=True,\n",
    "    JobFlowRole='EMR_EC2_DefaultRole',\n",
    "    ServiceRole='EMR_DefaultRole'\n",
    ")\n",
    "\n",
    "print ('cluster created with the step...', cluster_id['JobFlowId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting EMR Cluster Using Boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '677070d1-844b-4019-a0b2-5baf0dbfcd6f',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '677070d1-844b-4019-a0b2-5baf0dbfcd6f',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Thu, 09 Jan 2020 10:43:48 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emr.terminate_job_flows(JobFlowIds=[\n",
    "        cluster_id['JobFlowId'],\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'd5f27889-1087-4813-9720-b2307f7b8ee6',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'content-type': 'text/xml;charset=UTF-8',\n",
       "   'content-length': '225',\n",
       "   'date': 'Thu, 09 Jan 2020 10:33:04 GMT',\n",
       "   'server': 'AmazonEC2'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.delete_subnet(SubnetId=SUBNET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Notebooks on your Cluster\n",
    "After the cluster is in `Waiting` state connect to the cluster. Amazon has multiple ways to connect to the cluster. We'll use `Notebook` by clicking on the left side in the menu. Click on `create notebook`. Then name your notebook and attach a cluster to it and leave the rest to default and create the cluster. For more elaboration following this [tutorial](https://www.youtube.com/watch?v=EcIYPkCkehY) from **Udacity** on youtube.\n",
    "\n",
    "Note: When you are using Notebook change kernel according to the environment of your choice (in this case to PySpark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Scipts\n",
    "Up until now jupyter notebooks were used. They have the following advantages:\n",
    "1. Good for prototyping\n",
    "2. Exploring and visualizing the data\n",
    "3. Easily share the results with others\n",
    "\n",
    "But Jupyter notebooks are not good for automating the workflows. That's where scipts come in to play. For more elaboration following this [tutorial](https://www.youtube.com/watch?v=bfOocPv54EI) from **Udacity** on Youtube."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting Spark Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lower_scripts.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lower_scripts.py\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "        example program to show how to submit applications\n",
    "    \"\"\"\n",
    "    spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .appName('LowerSongTitles')\\\n",
    "            .getOrCreate()\n",
    "\n",
    "    log_of_songs = [\n",
    "        \"Despacito\",\n",
    "        \"Nice for what\",\n",
    "        \"No tears left to cry\",\n",
    "        \"Despacito\",\n",
    "        \"Havana\",\n",
    "        \"In my feelings\",\n",
    "        \"Nice for what\",\n",
    "        \"Despacito\",\n",
    "        \"All the stars\"\n",
    "    ]\n",
    "    \n",
    "    distributed_song_log = spark.sparkContext.parallelize(log_of_songs)\n",
    "    \n",
    "    print(distributed_song_log.map(lambda x:x.lower()).collect())\n",
    "    \n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect to the change the cluster using pem file change the permissions of pem file using the follwing linux command:\n",
    "\n",
    "    chmod 600 your_pem_file.pem\n",
    "Connect to the master node according to the instructions on AWS console and use the following command to run the spark job:\n",
    "\n",
    "    spark-submit --master yarn ./lower_script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing and Retrieving Data on the cloud\n",
    "We'll be using Amazon Simple Storage Service S3 for short. It is:\n",
    "* Safe\n",
    "* Easy to use\n",
    "* Cheap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using spark to read from and write data to S3 bucket\n",
    "Spark can reads from and writes data to S3 bucket just by putting the S3 bucket like following:\n",
    "\n",
    "    spark.read.json('s3n://bucket_name/file.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to HDFS\n",
    "When you use S3 you are separating Data storage from the spark cluster. One of downside is that you have to download the data across the network which can be a bottleneck. Another solution is to store the data on spark cluster with HDFS. But there's a trade off to HDFS i.e. you have to maintain and fix the system yourself. S3 is easier since we don't need to maintain the cluster. Also if you rent cluster from AWS then the data usually doesn't have to go too far in the network since the cluster hardware and S3 hardware are both on Amazon's data centers.Finally spark is smart enough to download a small chunk of data and process that chunk while waiting for the rest to download. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Writing to HDFS\n",
    "Similar to how we can read and write to S3 we can read and write to HDFS. The only difference is the path e.g.\n",
    "\n",
    "    spark.read.csv('hdfs:///user/sparkify_data/data.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging is Hard\n",
    "When you are in local mode the errors show up directly. But once you are in cluster of machines errors in the code can be very hard to diagnose. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntax Errors\n",
    "Suppose you have written your Spark program but there's some bug in you code. The code seems to work just fine but you have to remember the lazy evaluation of spark. Spark waits as long as it can before running your code on data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .config('spark.ui.port',3000)\\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = spark.read.json('data/sparkify_log_small.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist='Showaddywaddy', auth='Logged In', firstName='Kenneth', gender='M', itemInSession=112, lastName='Matthews', length=232.93342, level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration=1509380319284, sessionId=5132, song='Christmas Tears Will Fall', status=200, ts=1513720872284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'wher'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-63abaf51e35a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlog\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'userId'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'firstname'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'page'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'song'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0mwher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muserId\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'1046'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1299\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1300\u001b[0m             raise AttributeError(\n\u001b[1;32m-> 1301\u001b[1;33m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[0;32m   1302\u001b[0m         \u001b[0mjc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'wher'"
     ]
    }
   ],
   "source": [
    "log = logs.select(['userId','firstname','page','song'])\\\n",
    "        .wher(logs.userId == '1046')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the type of the syntax error and it can be debugged very easily as it can be seen that there's a typo in the code if we replace `wher` with `where` the code will work fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logs.select(['userId','firstname','page','song'])\\\n",
    "        .where(logs.userId == '1046')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "userId = log.groupby('userId').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId='1046', count=30)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userId.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs2 = logs.withColumn('artist',logs.artist + 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logs2.crossJoin(logs).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it means we ran out of memory. we'll just take 5 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist=None, auth='Logged In', firstName='Kenneth', gender='M', itemInSession=112, lastName='Matthews', length=232.93342, level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration=1509380319284, sessionId=5132, song='Christmas Tears Will Fall', status=200, ts=1513720872284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046', artist='Showaddywaddy', auth='Logged In', firstName='Kenneth', gender='M', itemInSession=112, lastName='Matthews', length=232.93342, level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration=1509380319284, sessionId=5132, song='Christmas Tears Will Fall', status=200, ts=1513720872284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046'),\n",
       " Row(artist=None, auth='Logged In', firstName='Elizabeth', gender='F', itemInSession=7, lastName='Chase', length=195.23873, level='free', location='Shreveport-Bossier City, LA', method='PUT', page='NextSong', registration=1512718541284, sessionId=5027, song='Cheryl Tweedy', status=200, ts=1513720878284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', userId='1000', artist='Showaddywaddy', auth='Logged In', firstName='Kenneth', gender='M', itemInSession=112, lastName='Matthews', length=232.93342, level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration=1509380319284, sessionId=5132, song='Christmas Tears Will Fall', status=200, ts=1513720872284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046'),\n",
       " Row(artist=None, auth='Logged In', firstName='Vera', gender='F', itemInSession=6, lastName='Blackwell', length=196.20526, level='paid', location='Racine, WI', method='PUT', page='NextSong', registration=1499855749284, sessionId=5516, song='Good Girls Go Bad (Feat.Leighton Meester) (Album Version)', status=200, ts=1513720881284, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.78.2 (KHTML, like Gecko) Version/7.0.6 Safari/537.78.2\"', userId='2219', artist='Showaddywaddy', auth='Logged In', firstName='Kenneth', gender='M', itemInSession=112, lastName='Matthews', length=232.93342, level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration=1509380319284, sessionId=5132, song='Christmas Tears Will Fall', status=200, ts=1513720872284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046'),\n",
       " Row(artist=None, auth='Logged In', firstName='Sophee', gender='F', itemInSession=8, lastName='Barker', length=405.99465, level='paid', location='San Luis Obispo-Paso Robles-Arroyo Grande, CA', method='PUT', page='NextSong', registration=1513009647284, sessionId=2372, song=\"Don't See The Point\", status=200, ts=1513720905284, userAgent='\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', userId='2373', artist='Showaddywaddy', auth='Logged In', firstName='Kenneth', gender='M', itemInSession=112, lastName='Matthews', length=232.93342, level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration=1509380319284, sessionId=5132, song='Christmas Tears Will Fall', status=200, ts=1513720872284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046'),\n",
       " Row(artist=None, auth='Logged In', firstName='Jordyn', gender='F', itemInSession=0, lastName='Jones', length=None, level='free', location='Syracuse, NY', method='GET', page='Home', registration=1513648531284, sessionId=1746, song=None, status=200, ts=1513720913284, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.94 Safari/537.36\"', userId='1747', artist='Showaddywaddy', auth='Logged In', firstName='Kenneth', gender='M', itemInSession=112, lastName='Matthews', length=232.93342, level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration=1509380319284, sessionId=5132, song='Christmas Tears Will Fall', status=200, ts=1513720872284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs2.crossJoin(logs).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = logs.where(logs.page == 'NextSong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist='Showaddywaddy', auth='Logged In', firstName='Kenneth', gender='M', itemInSession=112, lastName='Matthews', length=232.93342, level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration=1509380319284, sessionId=5132, song='Christmas Tears Will Fall', status=200, ts=1513720872284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|userId|       sum(length)|\n",
      "+------+------------------+\n",
      "|  2904|         348.57751|\n",
      "|   691|         808.98476|\n",
      "|  2294|13926.819139999998|\n",
      "|  2162|        8289.81315|\n",
      "|  1436|         633.39011|\n",
      "|  2088|3310.0480000000002|\n",
      "|  2275|         1172.1913|\n",
      "|  2756|1076.6344800000002|\n",
      "|   800|         517.17134|\n",
      "|  1394| 5989.630679999999|\n",
      "|   451|         433.44889|\n",
      "|   926|1087.8414400000001|\n",
      "|  2696|         200.95955|\n",
      "|   870|         463.51583|\n",
      "|     7| 533.9419499999999|\n",
      "|  1903|        1058.81895|\n",
      "|   591|         219.79383|\n",
      "|   613|         419.26439|\n",
      "|   574|        1286.55491|\n",
      "|   307|         281.28608|\n",
      "+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songs.groupby('userId').agg(fn.sum(songs.length)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Errors\n",
    "Even if you have perfectly written code your data might have errors like missing values or unexpected Unicode characters. These type of issues shouldn't crash your program. So suppose you run your code on the subset of the data and it works fine now you run the same code on full data set and it throws the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: Data Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .config('spark.ui.port',3000)\\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs3 = spark.read.json('data/sparkify_log_small_errors.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this file has some records that don't conform to the schema. Let's print the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen that there's a new field corrupt record . Let's look at the corrupt records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_corrupt_record='{\"ts\":a,\"userId\":\"1035\",\"sessionId\":5698,\"page\":\"NextSong\",\"auth\":\"Logged In\",\"method\":\"PUT\",\"status\":200,\"level\":\"paid\",\"itemInSession\":24,\"location\":\"Santa Cruz-Watsonville, CA\",\"userAgent\":\"\\\\\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\\\\\"\",\"lastName\":\"Gillespie\",\"firstName\":\"Connor\",\"registration\":1506639389284,\"gender\":\"M\",\"artist\":\"Spoon\",\"song\":\"Black Like Me\",\"length\":205.94893}', artist=None, auth=None, firstName=None, gender=None, itemInSession=None, lastName=None, length=None, level=None, location=None, method=None, page=None, registration=None, sessionId=None, song=None, status=None, ts=None, userAgent=None, userId=None),\n",
       " Row(_corrupt_record='{\"ts\":b,\"userId\":\"2373\",\"sessionId\":2372,\"page\":\"NextSong\",\"auth\":\"Logged In\",\"method\":\"PUT\",\"status\":200,\"level\":\"paid\",\"itemInSession\":13,\"location\":\"San Luis Obispo-Paso Robles-Arroyo Grande, CA\",\"userAgent\":\"\\\\\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\\\\\"\",\"lastName\":\"Barker\",\"firstName\":\"Sophee\",\"registration\":1513009647284,\"gender\":\"F\",\"artist\":\"Cat Power\",\"song\":\"What Would The Community Think\",\"length\":270.2624}', artist=None, auth=None, firstName=None, gender=None, itemInSession=None, lastName=None, length=None, level=None, location=None, method=None, page=None, registration=None, sessionId=None, song=None, status=None, ts=None, userAgent=None, userId=None),\n",
       " Row(_corrupt_record='{\"ts\":c,\"userId\":\"184\",\"sessionId\":5659,\"page\":\"NextSong\",\"auth\":\"Logged In\",\"method\":\"PUT\",\"status\":200,\"level\":\"free\",\"itemInSession\":1,\"location\":\"Atlanta-Sandy Springs-Roswell, GA\",\"userAgent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:31.0) Gecko/20100101 Firefox/31.0\",\"lastName\":\"Smith\",\"firstName\":\"Kara\",\"registration\":1508946027284,\"gender\":\"F\",\"artist\":\"Kanye West\",\"song\":\"Stronger\",\"length\":311.84934}', artist=None, auth=None, firstName=None, gender=None, itemInSession=None, lastName=None, length=None, level=None, location=None, method=None, page=None, registration=None, sessionId=None, song=None, status=None, ts=None, userAgent=None, userId=None),\n",
       " Row(_corrupt_record='{\"ts\":abc,\"userId\":\"950\",\"sessionId\":5756,\"page\":\"NextSong\",\"auth\":\"Logged In\",\"method\":\"PUT\",\"status\":200,\"level\":\"free\",\"itemInSession\":1,\"location\":\"Riverside-San Bernardino-Ontario, CA\",\"userAgent\":\"\\\\\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\\\\\"\",\"lastName\":\"Mcgee\",\"firstName\":\"Skyler\",\"registration\":1509473552284,\"gender\":\"F\",\"artist\":\"Suicide Silence\",\"song\":\"A Dead Current\",\"length\":221.04771}', artist=None, auth=None, firstName=None, gender=None, itemInSession=None, lastName=None, length=None, level=None, location=None, method=None, page=None, registration=None, sessionId=None, song=None, status=None, ts=None, userAgent=None, userId=None)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs3.where(logs3['_corrupt_record'].isNotNull()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen that the corrupt records have ts value which is string. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging your Code\n",
    "If you are writing traditional python script you might use print statements to output the values held by variables. A typical example would be outputting the i and j variables of a nested for-loop:\n",
    "    \n",
    "    for i in range(500):\n",
    "        for j in range(300):\n",
    "            print(x[i][j])\n",
    "            \n",
    "These print statements can be helpful when debugging your code. But this won't work on spark instead we need to use a special variable. \n",
    "But why can't we use print statements on a cluster? We have a driver node coordinating the tasks of various worker nodes so print statements will only run on those worker nodes. you can't see the output from them because you are not directly connected to them. Also spark makes a copy of input data every time you call a function so the original debugging variables that you created won't actually get loaded into worker nodes. Instead each worker has their own copy of these variables and only these copies get modified. The original variables stored on the driver remain unchanged making them useless for debugging. \n",
    "\n",
    "<img src=\"images/debugging.png\">\n",
    "\n",
    "To get around this limitation spark gives you special kind of variables called accumulators. Accumulators are the global variables for your entire cluster "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use Accumulators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to create an accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_records = SparkContext.accumulator(0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_records.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2nd step is to create a function that will increment the accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_incorrect_record():\n",
    "    global incorrect_records\n",
    "    incorrect_records += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to just define a udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_ts = udf(lambda x: 1 if x is not None else add_incorrect_record())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs3 = logs3\\\n",
    "            .withColumn('ts_digit',correct_ts(logs3.ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_records.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the record is 0 why is that? the reason might be we don't have any corrupt record or Spark uses lazy evaluation so we have to apply some actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+----+---------+------+--------+\n",
      "|     _corrupt_record|artist|auth|firstName|gender|itemInSession|lastName|length|level|location|method|page|registration|sessionId|song|status|  ts|userAgent|userId|ts_digit|\n",
      "+--------------------+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+----+---------+------+--------+\n",
      "|{\"ts\":a,\"userId\":...|  null|null|     null|  null|         null|    null|  null| null|    null|  null|null|        null|     null|null|  null|null|     null|  null|    null|\n",
      "|{\"ts\":b,\"userId\":...|  null|null|     null|  null|         null|    null|  null| null|    null|  null|null|        null|     null|null|  null|null|     null|  null|    null|\n",
      "|{\"ts\":c,\"userId\":...|  null|null|     null|  null|         null|    null|  null| null|    null|  null|null|        null|     null|null|  null|null|     null|  null|    null|\n",
      "|{\"ts\":abc,\"userId...|  null|null|     null|  null|         null|    null|  null| null|    null|  null|null|        null|     null|null|  null|null|     null|  null|    null|\n",
      "+--------------------+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+----+---------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs3.where(logs3['_corrupt_record'].isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_records.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_corrupt_record: string, artist: string, auth: string, firstName: string, gender: string, itemInSession: bigint, lastName: string, length: double, level: string, location: string, method: string, page: string, registration: bigint, sessionId: bigint, song: string, status: bigint, ts: bigint, userAgent: string, userId: string, ts_digit: string]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs3.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_corrupt_record=None, artist='Showaddywaddy', auth='Logged In', firstName='Kenneth', gender='M', itemInSession=112, lastName='Matthews', length=232.93342, level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration=1509380319284, sessionId=5132, song='Christmas Tears Will Fall', status=200, ts=1513720872284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046', ts_digit='1')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs3.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark WebUI\n",
    "Since debugging on a cluster is hard, Spark has a built-in user interface that you an access from your web browser. This interface, known as the web UI, helps you understand what's going on in your cluster without looking for individual workers. Spark's UI is like an EKG machine that helps you measure the health of your Spark jobs. It's a very useful tool for diagnosing issues in your code and your cluster, but it's just a tool. You still need to know how to interpret the output and know where to investigate further. When a doctor measures the patient's heart rate with an EKG, he needs to understand not only how the heart works, but also how the heart relates to other parts of the human anatomy. Understanding the Spark internals like shuffling, DAGs and stages that we discussed earlier for the Spark cluster. So, what does the web UI actually show? The web UI provides the current configuration for the cluster which can be useful for double-checking that your desired settings went into effect. The web UI also shows you the DAG, the recipe of steps for your program that we went through earlier. You'll see the DAG broken up into stages, and within each stage there are individual tasks. Tasks are the steps that the individual worker nodes are assigned. In each stage, the worker node divides up the input data and runs the task for that stage.\n",
    "\n",
    "<img src=\"images/spark-web-ui.png\">\n",
    "\n",
    "The web UI only shows the pages related to current jobs that are running. For example, you won't see any pages related to other libraries like Spark Streaming unless you are also running a streaming job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to the Spark Web UI\n",
    "Connecting to Spark's web UI is a lot like docking boats. When boats come to  a seaport, it's useful to have various types of ports so that each type of boat can dock efficiently and safely. FOr a cruise ship, you'll need a port for passenger loading. While cargo ship need a port equipped with cranes and personal speed boat would only requires a small dock or peer. As a result, they have different ports with different procedures for docking.\n",
    "\n",
    "<img src=\"images/boat-ports.png\">\n",
    "\n",
    "In the same way, it's useful to have several ways to connect data with a machine. When you transfer private data through a secure shell known as SSH, you follow a different protocol than when transferring public HTML data for a webpage using HTTP. FOr this reason, we use different ports: port `22` for SSH and port `80` for HTTP to indicate that we're transferring data using different network protocols. These commonly numbered ports follow a convention that engineers agreed upon. It's like having an agreed upon layout for each support in the world.\n",
    "\n",
    "<img src=\"images/ports-machine.png\">\n",
    "\n",
    "Spark uses several agreed upon ports for sharing information. Some ports are for machines to communicate with each other and aren't intended for users.  For example, the Spark master uses port `7077` to communicate with the worker nodes, but we'll never use it. There are few common ports that we'll use from time to time. For example, we've already seen that we usually connect to Jupyter notebooks on port `8888`. Another important port is `4040` which shows active Spark jobs. But the most useful port for you to memorize is the web UI for master node on port 8080. The web UI on `8080` shows the status of your cluster, configurations and the status of any recent jobs. \n",
    "\n",
    "<img src=\"images/spark-ports.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Familiar with the Spark UI\n",
    "Here to connect to the Spark UI as we've configured the port to `3000` by going to the link: http://localhost:3000 . Under **`Environment`** tab, we can see the different parameters of our application. The Java version, the Scala version, the name of the application and so forth so on. \n",
    "\n",
    "<img src=\"images/web-ui-environment.png\">\n",
    "\n",
    "The **`Executors`** tab gives you information about the executors, what resources do they have, how many tasks they have run successfully. In this particular case, we're looking at a Spark Local Executor, so there is only one of them. It has ran `241` tasks out of which `237` tasks are completed.\n",
    "\n",
    "<img src=\"images/web-ui-executors.png\">\n",
    "\n",
    "The storage tab is currently empty but if you have cached Rdds in your application, you can find that information here. \n",
    "\n",
    "<img src=\"images/web-ui-storage.png\">\n",
    "\n",
    "A Spark application consists of as many jobs, as many actions regarding the code. An action can be saving a data frame to a database or taking some records back to the drive for inspection. So for example after loading your data frame where we call head, that's an action triggering a job. We can see the jobs that we were around here. Jobs have further been broken into stages. If we click on the stage we can get access to the stages this job consists of.\n",
    "\n",
    "<img src= \"images/web-ui-jobs.png\">\n",
    "\n",
    "Stages are units of work that depends on one another and can be further paralyzed. For example, before joining two Data Frames, we need to finish transforming them both and just after that we can perform the actual join. The smallest unit within a stage is a task. \n",
    "\n",
    "<img src=\"images/web-ui-jobs2.png\">\n",
    "\n",
    "Tasks are a series of Spark transformations that can be run parallel on the different partitions of out Dataframe. So when we have 10 partitions we will run 10 of the same tasks to complete a stage. So in this particular case we had 200 partitions case so we ended up with 200 tasks. \n",
    "\n",
    "<img src=\"images/web-ui-stage.png\">\n",
    "\n",
    "For further understanding check this [tutorial](https://www.youtube.com/watch?v=88JQIalP84M) by Udacity on youtybe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of the Log Data\n",
    "Using log files is a bit more difficult when we are using Spark in cluster, compared to when we are running it locally. The log file is just as everything else as speed across the different nodes. Fortunately, the Spark UI provides a convenient way to look them up, so we don't need to directly access the various workers via SSH. If we had a cluster rather than local Spark running here, we would have another column called logs with two links to the standard out and standard error files for each of the workers they had. \n",
    "\n",
    "<img src=\"images/web-ui-executors.png\">\n",
    "\n",
    "In this case sine this is a local Spark application, we only have Thread Dump for the driver. Spark uses Log4j, as standard JVM library for logging. We can configure the logging level in two different ways: \n",
    "1. Edit the Log4j properties files in the `conf` directory\n",
    "2. Set in the Spark context\n",
    "\n",
    "So if we set spark log level to error we'll only see error messages in the log files. The code for doing this is the following:\n",
    "    \n",
    "    spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "If we would like to have more verbose logging then set this level to `INFO` and we'll have more information about the application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnosing Errors Part I\n",
    "We'll start with some simple stats, to tell us how Sparkify customers use our service. Let's begin with the daily listening time, for an average user. In other words how long is the average Sparkify user actually listening to music per day. After that we'll calculate the number of songs per day, including any repeat or partial listens, for the average user. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnosing Errors Part 2\n",
    "Next, let's focus on some core metrics for understanding the current state of the Sparkify business. These numbers would be so important to your entire team. They'd be displayed on dashboards throughout your company and every team would check them regularly. In fact your CEO would probably check them first  every morning when she eats her breakfast. \n",
    "Following are the Key matrices for Sparkify business:\n",
    "* Monthly Active Users\n",
    "* Daily active Users in Past Month\n",
    "* Total Paid and Unpaid Users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnosing Errors Part 3\n",
    "Next we'll tackle the problem of understanding how the users interact with the service over time. `Cohort Analysis`. The main idea of cohort analysis is that someone who joins Sparkify a year ago probably behaves differently than a brand new user. So we should analyze them in separate groups which we'll call cohorts .\n",
    "\n",
    "<img src=\"images/cohorts.png\">\n",
    "\n",
    "We'll calculate the number of users who upgraded to paid accounts within their first 3 months and the percentage of cancels within the first 3 months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Introduction\n",
    "We'll now debug some issues that are unique to working with big data. For these issues the code will work fine on small or medium datasets but will fail when we try to scale up the data. In these cases we'll have to optimize the code to find a better approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Data Skew\n",
    "It is the major headache in the big data. The problem is not the overall size of the data, but the skewed distribution of the data that can bring down the Spark cluster. A big chunk of the data comes from a small number of songs and because we're dividing the workload for each worker by the song, those workers are slowing down the whole group. \n",
    "\n",
    "<img src=\"images/data-skew.png\">\n",
    "\n",
    "Data skew comes up in many domains. Sometimes 80% of your data comes from 20% of users. This effect happens so often that this is called `PARETO PRINCIPE`. How do we know if you'll encounter Pareto's 80-20 rule? Occasionally we know this ahead of time because we're familiar with our users or we have an intuition about the distribution of the data. The best way to catch this is to run a quick Spark job to get a summary of the data. We can do this by sampling 5% of the data to avoid long Spark Job. Once the data skew is identified there are two main ways to solve it:\n",
    "* **Change workload division** - Instead of splitting the data by the song title divide up the data by another field like user or time-stamp. \n",
    "* **Partition the data** - By splitting the data into smaller pieces and spreading those pirces around, we reduce the chance that a single machine gets stuck with the bulk of the work.v "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Big O Complexity\n",
    "Another reason spark might have a problem in the larger data set is the bad algorithm in the code. If a chunk of data takes 30 secs to analyze then if we double the data it'd take 1 minutes and so on. But if you code has quadratic time complexity that it would've taken 2 minutes to complete the calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Issues and How to Address them\n",
    "\n",
    "In this lesson, we walked through various examples of issues you can debug based on error messages, loglines and stack traces.\n",
    "\n",
    "We have also touched on another very common issue with Spark jobs that can be harder to address: everything working fine but just taking a very long time. So what do you do when your Spark job is (too) slow?\n",
    "\n",
    "### Insufficient resources\n",
    "Often while there are some possible ways of improvement, processing large data sets just takes a lot longer time than smaller ones even without any big problem in the code or job tuning. Using more resources, either by increasing the number of executors or using more powerful machines, might just not be possible. When you have a slow job it's useful to understand\n",
    "\n",
    "* how much data you're actually processing (compressed file formats can be tricky to interpret),\n",
    "* if you can decrease the amount of data to be processed by filtering or aggregating to lower cardinality,\n",
    "* and if resource utilization is reasonable.\n",
    "\n",
    "There are many cases where different stages of a Spark job differ greatly in their resource needs: loading data is typically I/O heavy, some stages might require a lot of memory, others might need a lot of CPU. Understanding these differences might help to optimize the overall performance. Use the Spark UI and logs to collect information on these metrics.\n",
    "\n",
    "If you run into out of memory errors you might consider increasing the number of partitions. If the memory errors occur over time you can look into why the size of certain objects is increasing too much during the run and if the size can be contained. Also, look for ways of freeing up resources if garbage collection metrics are high.\n",
    "\n",
    "Certain algorithms (especially ML ones) use the driver to store data the workers share and update during the run. If you see memory issues on the driver check if the algorithm you're using is pushing too much data there.\n",
    "\n",
    "### Data skew\n",
    "If you drill down on the Spark UI to the task level you can see if certain partitions process significantly more data than others and if they are lagging behind. Such symptoms usually indicate a skewed data set. Consider implementing the techniques mentioned in this lesson:\n",
    "\n",
    "* add an intermediate data processing step with an alternative key\n",
    "* adjust the spark.sql.shuffle.partitions parameter if necessary\n",
    "\n",
    "The problem with data skew is that it's very specific to a data set. You might know ahead of time that certain customers or accounts are expected to generate a lot more activity but the solution for dealing with the skew might strongly depend on how the data looks like. If you need to implement a more general solution (for example for an automated pipeline) it's recommended to take a more conservative approach (so assume that your data will be skewed) and then monitor how bad the skew really is.\n",
    "\n",
    "### Inefficient queries\n",
    "Once your Spark application works it's worth spending some time to analyze the query it runs. You can use the Spark UI to check the DAG and the jobs and stages it's built of.\n",
    "\n",
    "Spark's query optimizer is called Catalyst. While Catalyst is a powerful tool to turn Python code to an optimized query plan that can run on the JVM it has some limitations when optimizing your code. It will for example push filters in a particular stage as early as possible in the plan but won't move a filter across stages. It's your job to make sure that if early filtering is possible without compromising the business logic than you perform this filtering where it's more appropriate.\n",
    "\n",
    "It also can't decide for you how much data you're shuffling across the cluster. Remember from the first lesson how expensive sending data through the network is. As much as possible try to avoid shuffling unnecessary data. In practice, this means that you need to perform joins and grouped aggregations as late as possible.\n",
    "\n",
    "When it comes to joins there is more than one strategy to choose from. If one of your data frames are small consider using broadcast hash join instead of a hash join.\n",
    "\n",
    "## Further reading\n",
    "Debugging and tuning your Spark application can be a daunting task. There is an ever growing community out there though always sharing new ideas and working on improving Spark itself and tooling that makes using Spark easier. So if you have a complicated issue don't hesitate to reach out to others (via user mailing lists, forums, and Q&A sites).\n",
    "\n",
    "You can find more information on tuning [Spark](https://spark.apache.org/docs/latest/tuning.html) and [Spark SQL](https://spark.apache.org/docs/latest/sql-performance-tuning.html) in the documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
