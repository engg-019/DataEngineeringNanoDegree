{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Data Lakes: Evolution of the Data Warehouse\n",
    "\n",
    "### Evolution of the Data Warehouse\n",
    "Q: Is there anything wrong with the data warehouse that we need something different?\n",
    "\n",
    "No, data warehousing is a rather **mature field** with lots of cumulative experience over the years, **tried-and-true technologies**. **Dimensional modeling is still extremely relevant** to this day.\n",
    "\n",
    "**For many organizations, a data warehouse is still the best way to go**, perhaps, the biggest change would be going from an on-premise deployment to a cloud deployment. \n",
    "\n",
    "Q: So, why do we need a data lake?\n",
    "\n",
    "In recent years, many factors drove the evolution of the data warehouse, to name a few:\n",
    "* The abundance of unstructured data (text, xml, json, logs, sensor data, images, voice, etc..)\n",
    "* Unprecedented data volumes (social, IOT, machine-generated, etc..)\n",
    "* The rise of Big Data technologies like HDFS, Spark, etc..\n",
    "* New types of data analysis gaining momentum, e.g. predictive analytics, recommender systems, graph analytics, etc..\n",
    "* Emergence of new roles like the data scientist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Data Lakes: Unstructured & Big Data\n",
    "\n",
    "### Abundance of Unstructured Data\n",
    "Q: Can we have unstructured data in the data warehouse?\n",
    "\n",
    "* Might be possible in the ETL process. FOr instance we might be able to distill sine elements from json data and put it in a tabular format.\n",
    "* But later, we might decide we want to transform it differently, so deciding on a particular form of transformation is a **strong commitment without enough knowledge**. E.g we start by recording # of replicas in a facebook of comments and then we interested in the frequency of angry words.\n",
    "* Some data is hard to put in a tabular format like **deep json structures**.\n",
    "* Some data like text/pdf documents could be stored as \"**blobs**\" of data in a relational database but totally **useless useless processed to extract metrics** .\n",
    "* The Hadoop file system (HDFS) made it possible to Peta Bytes of data on commodity hardware. **Much lower cost per TB** compared to MPP(Massively parallel processing) databases. \n",
    "* Associated processing tools starting from MapReduce, Pig, Hive, Impala, and Spark, to name a few, made it possible to **process this data at scale on the same hardware used for storage**.\n",
    "* It is possible to make data analysis without inserting into a predefined schema. One can load a CSV file and make a query without creating a table, inserting the data in the table. Similarly one can process unstructured text. This approach is know as \"**Schema-On-Read**\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Data Lakes: New Roles & Advanced Analytics\n",
    "* The data warehouse by design follows a **very well-architured** path yo make a **clean, consistent and performant model** that business users can easily use to gain insights and make decisions.\n",
    "* As data became an asset of highest value (**Data is the new oil**), a role like the **data scientist** started to emerge seeking value from data\n",
    "* The data scientist job is almost impossible conforming to a **single rigid representation of data**. He needs freedom to represent data, join data sets together, retrieve new external data sources and more.\n",
    "* The type of analytics such as , e.g. **machine learning, natural language processing** need to access the raw data in forms totally different from a star schema.\n",
    "\n",
    "### The Data Lake is the new Data Warehouse\n",
    "* The data lake shares the goals of the data warehouse of supporting business insights beyond the day-today transactional data handling.\n",
    "* The Data lake is new form of data warehouse that evolved to cope with:\n",
    "    * The **variety of data formats** and structuring\n",
    "    * The agile and ad-hoc nature of **data exploration** activities needed by new roles like **data scientist**\n",
    "    * THe wide data spectrum data transformation needed by **advanced analytics** like machine learning, graph analytics, and recommender systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data Effects: Low Costs, ETL Offloading\n",
    "<img src=\"images/big_data_effects.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data Effects: Schema-on-Read\n",
    "* Traditionally, data in a database has been much easier to process than data in plain files\n",
    "* Big Data tools in the hadoop ecosystem e.g. Hive & SPark made it easy to work with a file as easy as it is to work with a database without:\n",
    "    * ~Creating a database~\n",
    "    * ~ Inserting the data into database~\n",
    "* **Schema on-read**: as for the schema of a table (simple file on disk):\n",
    "    * It is either inferred\n",
    "    * Or specified and the data is not inserted into it, but upon read the data is checked against the specified schema\n",
    "    \n",
    "<img src=\"images/schema_on_read_1.png\">\n",
    "<img src=\"images/schema_on_read_2.png\">\n",
    "<img src=\"images/schema_on_read_3.png\">\n",
    "<img src=\"images/schema_on_read_4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data Effects: (Un-/Semi-)Structured support\n",
    "* Spark has the ability to read/write files in:\n",
    "    * Text-based many formats, csv, josn, text\n",
    "    * Binary formats such as Avro (saves space) and Parquet (columnar)\n",
    "    * Compressed formats e.g. gzip & snappy\n",
    "    \n",
    "       \n",
    "    dfLog = spark.read.text(\"data/NASA_access_log_Jul95.gz\")\n",
    "    \n",
    "    dfRaw = spark.read.csv(\"data/news_worldnews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
