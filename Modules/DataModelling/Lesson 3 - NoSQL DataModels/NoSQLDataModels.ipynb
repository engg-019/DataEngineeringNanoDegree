{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Relational Databases\n",
    "### When to Use NoSQL\n",
    "* **Need high Availability in the data**: Indicates the system is always up and there is no downtime\n",
    "* **Have Large Amounts of Data**\n",
    "* **Need Linear Scalability**: The need to add more nodes to the system so performance will increase linearly\n",
    "* **Low Latency**: Shorter delay before the data is transferred once the instruction for the transfer has been received.\n",
    "* **Need fast reads and write**\n",
    "\n",
    "### Apache Cassandra\n",
    "* Open Source NoSQL DB -- go download the code!\n",
    "* Masterless Architecture\n",
    "* High Availability\n",
    "* Linear Scalability\n",
    "* Used by Uber, Netflix, Hulu, Twitter, Facebook, etc\n",
    "* Major contributors to the project: DataStax, Facebook, Twitter, Apple\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Databases\n",
    "In a **distributed database**, in order to have hight availability, you will need copies of your data.\n",
    "\n",
    "### Eventual Consistency\n",
    "Over time (if no new changes are made) each copy of the data will be the same, but if there are new changes, the data may be different in different locations. The data may be inconsistent for only milliseconds. There are workarounds in place to prevent getting stale data.\n",
    "\n",
    "### Commonly Asked Questions:\n",
    "**What does the network look like? Can you share any examples?**\n",
    "\n",
    "In Apache Cassandra every node is connected to every node -- it's peer to peer database architecture.\n",
    "\n",
    "**Is data deployment strategy an important element of data modeling in Apache Cassandra?**\n",
    "\n",
    "Deployment strategies are a great topic, but have very little to do with data modeling. Developing deployment strategies focuses on determining how many clusters to create or determining how many nodes are needed. These are topics generally covered under database architecture, database deployment and operations, which we will not cover in this lesson. Here is a useful link to learn more about it for [Apache Cassandra](https://docs.datastax.com/en/dse-planning/doc/).\n",
    "\n",
    "In general, the size of your data and your data model can affect your deployment strategies. You need to think about how to create a cluster, how many nodes should be in that cluster, how to do the actual installation. More information about deployment strategies can be found on this [DataStax documentation page](https://docs.datastax.com/en/dse-planning/doc/).\n",
    "\n",
    "### Cassandra Architecture\n",
    "We are not going into a lot of details about the Apache Cassandra Architecture. However, if you would like to learn more about it for your job, here are some links that you may find useful.\n",
    "\n",
    "**Apache Cassandra Data Architecture:**\n",
    "\n",
    "* [Understanding the architecture](https://docs.datastax.com/en/cassandra/3.0/cassandra/architecture/archTOC.html)\n",
    "* [Cassandra Architecture](https://www.tutorialspoint.com/cassandra/cassandra_architecture.htm)\n",
    "\n",
    "The following link will go more in-depth about the Apache Cassandra Data Model, how Cassandra reads, writes, updates, and deletes data.\n",
    "\n",
    "* [Cassandra Documentation](https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlIntro.html)\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAP Theorem\n",
    "A theorem in computer science that states that it is **impossible** for a distributed data store to **simultaneously provide** more than two out of the following three guarantees of **consistency**, **availability** and **partition tolerance**.\n",
    "\n",
    "* **Consistency**: Every read from the database gets the latest (and correct) piece of data or an error\n",
    "\n",
    "* **Availability**: Every request is received and a response is given -- without a guarantee that the data is the latest update\n",
    "\n",
    "* **Partition Tolerance**: The system continues to work regardless of losing network connectivity between nodes\n",
    "\n",
    "### Commonly Asked Questions:\n",
    "**Is Eventual Consistency the opposite of what is promised by SQL database per the ACID principle?**\n",
    "\n",
    "Much has been written about how Consistency is interpreted in the ACID principle and the CAP theorem. Consistency in the ACID principle refers to the requirement that only transactions that abide by constraints and database rules are written into the database, otherwise the database keeps previous state. In other words, the data should be correct across all rows and tables. However, consistency in the CAP theorem refers to every read from the database getting the latest piece of data or an error.\n",
    "\n",
    "To learn more, you may find this discussion useful:\n",
    "\n",
    "* [Discussion about ACID vs. CAP](https://www.voltdb.com/blog/2015/10/22/disambiguating-acid-cap/)\n",
    "\n",
    "**Which of these combinations is desirable for a production system - Consistency and Availability, Consistency and Partition Tolerance, or Availability and Partition Tolerance?**\n",
    "\n",
    "As the CAP Theorem Wikipedia entry says, \"The CAP theorem implies that in the presence of a network partition, one has to choose between consistency and availability.\" So there is no such thing as Consistency and Availability in a distributed database since it must always tolerate network issues. You can only have Consistency and Partition Tolerance (CP) or Availability and Partition Tolerance (AP). Remember, relational and non-relational databases do different things, and that's why most companies have both types of database systems.\n",
    "\n",
    "**Does Cassandra meet just Availability and Partition Tolerance in the CAP theorem?**\n",
    "\n",
    "According to the CAP theorem, a database can actually only guarantee two out of the three in CAP. So supporting Availability and Partition Tolerance makes sense, since Availability and Partition Tolerance are the biggest requirements.\n",
    "\n",
    "**If Apache Cassandra is not built for consistency, won't the analytics pipeline break?**\n",
    "\n",
    "If I am trying to do analysis, such as determining a trend over time, e.g., how many friends does John have on Twitter, and if you have one less person counted because of \"eventual consistency\" (the data may not be up-to-date in all locations), that's OK. In theory, that can be an issue but only if you are not constantly updating. If the pipeline pulls data from one node and it has not been updated, then you won't get it. Remember, in Apache Cassandra it is about **Eventual Consistency**.\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 1\n",
    "### Question 1 of 2\n",
    "Match the CAP theorem term to its definition.\n",
    "\n",
    "|CAP THEOREM TERM| DEFINITION|\n",
    "|----------------|-----------|\n",
    "|Consistency|Database will get the latest piece of data requested|\n",
    "|Availability|Every request is received and a response is given.|\n",
    "|Partition Tolerance|Network connectivity does not affect the system|\n",
    "\n",
    "### Question 2 of 2\n",
    "Apache Cassandra is...\n",
    "(Check all that apply)\n",
    "- [x] A Highly Available Database\n",
    "- [x] Linearly scalable\n",
    "- [ ] Used when ACID transactions are needed\n",
    "- [x] An Open Source project supported by The Apache Foundation\n",
    "- [ ] A Consistency and Partition Tolerant Database during network failures.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De-normalization in Apache Cassandra\n",
    "De-normalization of tables in Apache Cassandra is absolutely critical. The biggest take away when doing data modeling in Apache Cassandra is to think about your **queries** first. There are no `JOINS` in Apache Cassandra.\n",
    "\n",
    "### Data Modeling in Apache Cassandra:\n",
    "* Denormalization is not just okay -- it's a must\n",
    "* Denormalization must be done for fast reads\n",
    "* Apache Cassandra has been optimized for fast writes\n",
    "* ALWAYS think Queries first\n",
    "* One table per query is a great strategy\n",
    "* Apache Cassandra does not allow for JOINs between tables\n",
    "\n",
    "### Commonly Asked Questions:\n",
    "* **I see certain downsides of this approach, since in a production application, requirements change quickly and I may need to improve my queries later. Isn't that a downside of Apache Cassandra?**\n",
    "In Apache Cassandra, you want to model your data to your queries, and if your business need calls for quickly changing requirements, you need to create a new table to process the data. That is a requirement of Apache Cassandra. If your business needs calls for ad-hoc queries, these are not a strength of Apache Cassandra. However keep in mind that it is easy to create a new table that will fit your new query.\n",
    "\n",
    "### Additional Resource:\n",
    "Here is a reference to the DataStax documents on [Apache Cassandra](https://docs.datastax.com/en/dse/6.7/cql/cql/ddl/dataModelingApproach.html)\n",
    "\n",
    "### Question 1 of 3\n",
    "True or False: Apache Cassandra denormalization of tables in data modeling is required.\n",
    "- [x] True\n",
    "- [ ] False\n",
    "\n",
    "### Question 2 of 3\n",
    "True or False: When doing data modeling in Apache Cassandra 1 table per 1 query is a very acceptable practice.\n",
    "- [x] True\n",
    "- [ ] False\n",
    "\n",
    "### Question 3 of 3\n",
    "True or False: When doing data modeling in Apache Cassandra knowing your queries first and modeling to those queries is essential.\n",
    "- [x] True\n",
    "- [ ] False\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cassandra Query Language: CQL\n",
    "Cassandra query language is the way to interact with the database and is similar to SQL. JOINS, GROUP BY, or sub-queries are not in CQL and are not supported by CQL.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Lesson 3 Demo 1: 2 Queries 2 Tables\n",
    "\n",
    "### In this demo we are going to walk through the basics of creating a table in Apache Cassandra, inserting rows of data, and doing a simple SQL query to validate the information. We will talk about the importance of Denormalization, and that 1 table per 1 query is an encouraged practice with Apache Cassandra. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use a python wrapper/ python driver called cassandra to run the Apache Cassandra queries. This library should be preinstalled but in the future to install this library you can run this command in a notebook to install locally: \n",
    "! pip install cassandra-driver\n",
    "#### More documentation can be found here:  https://datastax.github.io/python-driver/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Apache Cassandra python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cassandra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First let's create a connection to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "try:\n",
    "    cluster = Cluster(['127.0.0.1']) #If you have a locally installed Apache Cassandra instance\n",
    "    session = cluster.connect()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Let's create a keyspace to do our work in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS udacity\n",
    "    WITH REPLICATION = \n",
    "    {'class':'SimpleStrategy', 'replication_factor':1}\n",
    "    \"\"\"\n",
    "    )\n",
    "except Error as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to our Keyspace. Compare this to how we had to create a new session in PostgreSQL.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    session.set_keyspace('udacity')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's imagine we would like to start creating a Music Library of albums. \n",
    "\n",
    "### We want to ask 2 questions of our data\n",
    "#### 1. Give me every album in my music library that was released in a given year\n",
    "`select * from music_library WHERE YEAR=1970`\n",
    "#### 2. Give me every album in my music library that was created by a given artist  \n",
    "`select * from artist_library WHERE artist_name=\"The Beatles\"`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because I want to do two different queries, I am going to do need different tables that partition the data differently. \n",
    "* My music library table will be by year that will become my partition key, and artist name will be my clustering column to make each Primary Key unique. \n",
    "* My album library table will be by artist name that will be my partition key, and year will be my clustering column to make each Primary Key unique. More on Primary keys in the next lesson and demo. \n",
    "\n",
    "`Table Name: music_library\n",
    "column 1: Year\n",
    "column 2: Artist Name\n",
    "column 3: Album Name\n",
    "PRIMARY KEY(year, artist name)`\n",
    "\n",
    "\n",
    "` Table Name: album_library \n",
    "column 1: Artist Name\n",
    "column 2: Year\n",
    "column 3: Album Name\n",
    "PRIMARY KEY (artist name, year)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    session.execute(\"\"\"\n",
    "                        CREATE TABLE IF NOT EXISTS music_library(\n",
    "                            year INT,\n",
    "                            artist_name TEXT,\n",
    "                            album_name TEXT,\n",
    "                            PRIMARY KEY(year,artist_name))        \n",
    "    \"\"\")\n",
    "except Error as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    session.execute(\"\"\"\n",
    "                        CREATE TABLE IF NOT EXISTS album_library(\n",
    "                            artist_name TEXT,\n",
    "                            year INT,\n",
    "                            album_name TEXT,\n",
    "                            PRIMARY KEY(artist_name,year))        \n",
    "    \"\"\")\n",
    "except Error as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's insert some data into both tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"INSERT INTO music_library (year, artist_name, album_name)\"\n",
    "query = query + \" VALUES (%s, %s, %s)\"\n",
    "\n",
    "query1 = \"INSERT INTO album_library (artist_name, year, album_name)\"\n",
    "query1 = query1 + \" VALUES (%s, %s, %s)\"\n",
    "\n",
    "try:\n",
    "    session.execute(query, (1970, \"The Beatles\", \"Let it Be\"))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    session.execute(query, (1965, \"The Beatles\", \"Rubber Soul\"))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    session.execute(query, (1965, \"The Who\", \"My Generation\"))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    session.execute(query, (1966, \"The Monkees\", \"The Monkees\"))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    session.execute(query, (1970, \"The Carpenters\", \"Close To You\"))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    session.execute(query1, (\"The Beatles\", 1970, \"Let it Be\"))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    session.execute(query1, (\"The Beatles\", 1965, \"Rubber Soul\"))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    session.execute(query1, (\"The Who\", 1965, \"My Generation\"))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    session.execute(query1, (\"The Monkees\", 1966, \"The Monkees\"))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    session.execute(query1, (\"The Carpenters\", 1970, \"Close To You\"))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This might have felt unnatural to insert duplicate data into two tables. If I just normalized these tables, I wouldn't have to have extra copies! While this is true, remember there are no `JOINS` in Apache Cassandra. For the benefit of high availibity and scalabity denormalization must be how this is done. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Validate our Data Model\n",
    "\n",
    "`select * from music_library WHERE YEAR=1970`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1970 The Beatles Let it Be\n",
      "1970 The Carpenters Close To You\n"
     ]
    }
   ],
   "source": [
    "query = \"select * from music_library WHERE YEAR=1970\"\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print (row.year, row.artist_name, row.album_name,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the sake of the demo, I will drop the table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"drop table music_library\"\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "query = \"drop table album_library\"\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And Finally close the session and cluster connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary Key\n",
    "* Must be unique\n",
    "* The PRIMARY KEY is made up of either just the PARTITION KEY or may also include additional CLUSTERING COLUMNS\n",
    "* A Simple PRIMARY KEY is just one column that is also the PARTITION KEY. A Composite PRIMARY KEY is made up of more than one column and will assist in creating a unique value and in your retrieval queries\n",
    "* The PARTITION KEY will determine the distribution of data across the system\n",
    "* May have one or more clustering columns.\n",
    "\n",
    "### Partition Key\n",
    "* The partition key's row value will be hashed (turned into a number) and stored on the the node in the system that holds the range of values.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Primary Key\n",
    "### Question 1 of 2\n",
    "True or False: Apache Cassandra supports duplicate rows.\n",
    "\n",
    "- [ ] True\n",
    "- [x] False\n",
    "\n",
    "### Question 2 of 2\n",
    "Which is better: Simple or Composite Primary Keys?\n",
    "- [x] It depends on the data you have and the queries you will do\n",
    "- [ ] Simple -- Simple is always better\n",
    "- [ ] Composite\n",
    "- [ ] Neither, it is better to use a Relational Database\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Lesson 3 Demo 2: Focus on Primary Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this demo we are going to walk through the basics of creating a table with a good Primary Key in Apache Cassandra, inserting rows of data, and doing a simple SQL query to validate the information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use a python wrapper/ python driver called cassandra to run the Apache Cassandra queries. This library should be preinstalled but to install this library  in the future you can run this command in a notebook to install locally: \n",
    "! pip install cassandra-driver\n",
    "#### More documentation can be found here:  https://datastax.github.io/python-driver/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Apache Cassandra python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cassandra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First let's create a connection to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "try: \n",
    "    cluster = Cluster(['127.0.0.1']) #If you have a locally installed Apache Cassandra instance\n",
    "    session = cluster.connect()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a keyspace to do our work in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS udacity \n",
    "    WITH REPLICATION = \n",
    "    { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\"\"\"\n",
    ")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to our Keyspace. Compare this to how we had to create a new session in PostgreSQL.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    session.set_keyspace('udacity')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's imagine we would like to start creating a new Music Library of albums. We are going to work with one of the queries from Exercise 1.\n",
    "\n",
    "### We want to ask 1 question of our data\n",
    "#### 1. Give me every album in my music library that was released in a given year\n",
    "`select * from music_library WHERE YEAR=1970`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is our Collection of Data\n",
    "### How should we model this data? What should be our Primary Key and Partition Key? Since our data is looking for the YEAR let's start with that. Is Partitioning our data by year a good idea? In this case our data is very small, but if we had a larger data set of albums partitions by YEAR might be a find choice. We would need to validate from our dataset. We want an equal spread of the data. \n",
    "\n",
    "`Table Name: music_library\n",
    "column 1: Year\n",
    "column 2: Artist Name\n",
    "column 3: Album Name\n",
    "Column 4: City\n",
    "PRIMARY KEY(year)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"CREATE TABLE IF NOT EXISTS music_library \"\n",
    "query = query + \"(year int, artist_name text, album_name text, city text, PRIMARY KEY (year))\"\n",
    "try:\n",
    "    session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's insert our data into of table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"INSERT INTO music_library (year, artist_name, album_name, city)\"\n",
    "query = query + \" VALUES (%s, %s, %s, %s)\"\n",
    "\n",
    "try:\n",
    "    session.execute(query, (1970, \"The Beatles\", \"Let it Be\", \"Liverpool\"))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    session.execute(query, (1965, \"The Beatles\", \"Rubber Soul\", \"Oxford\"))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    session.execute(query, (1965, \"The Who\", \"My Generation\", \"London\"))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    session.execute(query, (1966, \"The Monkees\", \"The Monkees\", \"Los Angeles\"))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    session.execute(query, (1970, \"The Carpenters\", \"Close To You\", \"San Diego\"))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Validate our Data Model -- Did it work?? If we look for Albums from 1965 we should expect to see 2 rows.\n",
    "\n",
    "`select * from music_library WHERE YEAR=1965`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1965 The Who My Generation London\n"
     ]
    }
   ],
   "source": [
    "query = \"select * from music_library WHERE YEAR=1965\"\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print (row.year, row.artist_name, row.album_name, row.city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That didn't work out as planned! Why is that? Because we did not create a unique primary key. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Try Again. Let's focus on making the PRIMARY KEY unique. Look at our dataset do we have anything that is unique for each row? We have a couple of options (City and Album Name) but that will not get us the query we need which is looking for album's in a particular year. Let's make a composite key of the `YEAR` AND `ALBUM NAME`. This is assuming that an album name is unique to the year it was released (not a bad bet). --But remember this is just a demo, you will need to understand your dataset fully (no betting!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"CREATE TABLE IF NOT EXISTS music_library1 \"\n",
    "query = query + \"(year int, artist_name text, album_name text, city text, PRIMARY KEY (year, album_name))\"\n",
    "try:\n",
    "    session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"INSERT INTO music_library1 (year, artist_name, album_name, city)\"\n",
    "query = query + \" VALUES (%s, %s, %s, %s)\"\n",
    "\n",
    "try:\n",
    "    session.execute(query, (1970, \"The Beatles\", \"Let it Be\", \"Liverpool\"))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    session.execute(query, (1965, \"The Beatles\", \"Rubber Soul\", \"Oxford\"))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    session.execute(query, (1965, \"The Who\", \"My Generation\", \"London\"))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    session.execute(query, (1966, \"The Monkees\", \"The Monkees\", \"Los Angeles\"))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    session.execute(query, (1970, \"The Carpenters\", \"Close To You\", \"San Diego\"))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Validate our Data Model -- Did it work?? If we look for Albums from 1965 we should expect to see 2 rows.\n",
    "\n",
    "`select * from music_library WHERE YEAR=1965`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1965 The Who My Generation London\n",
      "1965 The Beatles Rubber Soul Oxford\n"
     ]
    }
   ],
   "source": [
    "query = \"select * from music_library1 WHERE YEAR=1965\"\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print (row.year, row.artist_name, row.album_name, row.city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Success it worked! We created a unique Primary key that evenly distributed our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the sake of the demo, I will drop the table. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"drop table music_library\"\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "query = \"drop table music_library1\"\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And Finally close the session and cluster connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Columns\n",
    "The **PRIMARY KEY** is made up of either just the **PARTITION KEY** or with the addition of **CLUSTERING COLUMNS**. The **CLUSTERING COLUMN** will determine the sort order within a Partition.\n",
    "\n",
    "* The clustering column will sort the data in sorted **ascending order**, e.g., alphabetical order.\n",
    "* More than one clustering column can be added (or none!)\n",
    "* From there the clustering columns will sort in order of how they were added to the primary key\n",
    "\n",
    "        CREATE TABLE\n",
    "        music_library\n",
    "        (year INT, \n",
    "         artist_name TEXT,\n",
    "         album_name TEXT,\n",
    "         PRIMARY KEY ((year),\n",
    "         artist_name, album_name)\n",
    "\n",
    "|year|artist_name|album_name|\n",
    "|----|-----------|----------|\n",
    "|1965|Elvis      |Blue Hawaii|\n",
    "|1965|The Beatles|Rubber Soul|\n",
    "|1965|The Beatles|Showing order|\n",
    "|1965|The Monkees|Meet the Monkees|\n",
    "\n",
    "Here Primary key is the combination of partition key which is `year` and two clustering columns `artist_name` and `album_name`\n",
    "\n",
    "### Commonly Asked Questions:\n",
    "**How many clustering columns can we add?**\n",
    "You can use as many clustering columns as you would like. You cannot use the clustering columns out of order in the SELECT statement. You may choose to omit using a clustering column in your SELECT statement. That's OK. Just remember to use them in order when you are using the SELECT statement.\n",
    "\n",
    "### QUESTION 1 OF 2\n",
    "The PRIMARY KEY is made up of...\n",
    "- [ ] The composite key, the primary key and the clustering key\n",
    "- [ ] The composite key, the partition key and the clustering columns\n",
    "- [x] The partition key and the clustering columns\n",
    "\n",
    "### QUESTION 2 OF 2\n",
    "A Clustering Column is required in the Primary Key\n",
    "- [ ] True\n",
    "- [x] False\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 3 Demo 3: Focus on Clustering Columns\n",
    "\n",
    "### In this demo we are going to walk through the basics of creating a table with a good Primary Key and Clustering Columns in Apache Cassandra, inserting rows of data, and doing a simple SQL query to validate the information.\n",
    "\n",
    "#### We will use a python wrapper/ python driver called cassandra to run the Apache Cassandra queries. This library should be preinstalled but in the future to install this library you can run this command in a notebook to install locally: \n",
    "! pip install cassandra-driver\n",
    "#### More documentation can be found here:  https://datastax.github.io/python-driver/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Apache Cassandra python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cassandra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First let's create a connection to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "try: \n",
    "    cluster = Cluster(['127.0.0.1']) #If you have a locally installed Apache Cassandra instance\n",
    "    session = cluster.connect()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a keyspace to do our work in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS udacity \n",
    "    WITH REPLICATION = \n",
    "    { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\"\"\"\n",
    ")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to our Keyspace. Compare this to how we had to create a new session in PostgreSQL.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    session.set_keyspace('udacity')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's imagine we would like to start creating a new Music Library of albums. \n",
    "\n",
    "### We want to ask 1 question of our data\n",
    "#### 1. Give me every album in my music library that was released by an Artist with Albumn Name in `DESC` Order and City In `DESC` Order\n",
    "`select * from music_library WHERE ARTIST_NAME=\"The Beatles\"`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How should we model this data? What should be our Primary Key and Partition Key? Since our data is looking for the `ARTIST_NAME` let's start with that. From there we will need to add other elements to make sure the Key is unique. We also need to add the `CITY` and `ALBUM_NAME` as Clustering Columns to sort the data. That should be enough to make the row key unique\n",
    "\n",
    "`Table Name: music_library\n",
    "column 1: Year\n",
    "column 2: Artist Name\n",
    "column 3: Album Name\n",
    "Column 4: City\n",
    "PRIMARY KEY(artist name, album name, city)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"CREATE TABLE IF NOT EXISTS music_library \"\n",
    "query = query + \"(year int, artist_name text, album_name text, city text, PRIMARY KEY (artist_name, album_name, city))\"\n",
    "try:\n",
    "    session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's insert our data into of table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"INSERT INTO music_library (year, artist_name, album_name, city)\"\n",
    "query = query + \" VALUES (%s, %s, %s, %s)\"\n",
    "\n",
    "try:\n",
    "    session.execute(query, (1970, \"The Beatles\", \"Let it Be\", \"Liverpool\"))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    session.execute(query, (1965, \"The Beatles\", \"Rubber Soul\", \"Oxford\"))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    session.execute(query, (1964, \"The Beatles\", \"Beatles For Sale\", \"London\"))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    session.execute(query, (1966, \"The Monkees\", \"The Monkees\", \"Los Angeles\"))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    session.execute(query, (1970, \"The Carpenters\", \"Close To You\", \"San Diego\"))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Validate our Data Model -- Did it work?? If we look for Albums from The Beatles we should expect to see 3 rows.\n",
    "\n",
    "`select * from music_library WHERE ARTIST_NAME=\"The Beatles\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Beatles Beatles For Sale London 1964\n",
      "The Beatles Let it Be Liverpool 1970\n",
      "The Beatles Rubber Soul Oxford 1965\n"
     ]
    }
   ],
   "source": [
    "query = \"select * from music_library WHERE ARTIST_NAME='The Beatles'\"\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print (row.artist_name, row.album_name, row.city, row.year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Success it worked! We created a unique Primary key that evenly distributed our data, with clustering columns that sorted our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the sake of the demo, I will drop the table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"drop table music_library\"\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And Finally close the session and cluster connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WHERE Clause\n",
    "* Data Modeling in Apache Cassandra is query focused, and that focus needs to be om the **WHERE** clause.\n",
    "* The **PARTITION KEY** must be included in your query and any **CLUSTERING COLUMNS** can be used in the order they appear in your **PRIMARY KEY**.\n",
    "\n",
    "<img src=\"images/where_sample_queries.png\">\n",
    "\n",
    "### Select * from table\n",
    "The `WHERE` clause must be included to execute queries. It is recommended that one partition be queried at a time for performance implications. It is possible to do a select * from table if you add a configuration `ALLOW FILTERING` to your query. This is risky, but available if absolute necessary.\n",
    "\n",
    "AVOID using \"ALLOW FILTERING\": Here is a reference in [DataStax](https://www.datastax.com/dev/blog/allow-filtering-explained-2) that explains ALLOW FILTERING and why you should not use it.\n",
    "\n",
    "### Commonly Asked Questions:\n",
    "**Why do we need to use a WHERE statement since we are not concerned about analytics? Is it only for debugging purposes?**\n",
    "The `WHERE` statement is allowing us to do the fast reads. With Apache Cassandra, we are talking about big data -- think terabytes of data -- so we are making it fast for read purposes. Data is spread across all the nodes. By using the `WHERE` statement, we know which node to go to, from which node to get that data and serve it back. For example, imagine we have 10 years of data on 10 nodes or servers. So 1 year's data is on a separate node. By using the `WHERE year = 1` statement we know which node to visit fast to pull the data from.\n",
    "\n",
    "### QUIZ QUESTION\n",
    "Can you do SELECT * FROM myTable in Apache Cassandra?\n",
    "- [ ] Yes\n",
    "- [ ] No\n",
    "- [x] It is highly discouraged as performance will be slow (or may just fail) but is possible with a configuration setting.\n",
    "- [ ] Yes, and no one should worry about it.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 3 Demo 4: Using the WHERE Clause\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this demo we are going to walk through the basics of using the WHERE clause in Apache Cassandra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use a python wrapper/ python driver called cassandra to run the Apache Cassandra queries. This library should be preinstalled but in the future to install this library you can run this command in a notebook to install locally: \n",
    "! pip install cassandra-driver\n",
    "#### More documentation can be found here:  https://datastax.github.io/python-driver/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Apache Cassandra python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cassandra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First let's create a connection to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "try: \n",
    "    cluster = Cluster(['127.0.0.1']) #If you have a locally installed Apache Cassandra instance\n",
    "    session = cluster.connect()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a keyspace to do our work in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS udacity \n",
    "    WITH REPLICATION = \n",
    "    { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\"\"\"\n",
    ")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to our Keyspace. Compare this to how we had to create a new session in PostgreSQL.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    session.set_keyspace('udacity')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's imagine we would like to start creating a new Music Library of albums. \n",
    "### We want to ask 4 question of our data\n",
    "#### 1. Give me every album in my music library that was released in a given year\n",
    "`select * from music_library WHERE YEAR=1970`\n",
    "#### 2. Give me the album that is in my music library that was released in a given year by \"The Beatles\"\n",
    "`select * from music_library WHERE YEAR = 1970 AND ARTIST_NAME = \"The Beatles\"`\n",
    "#### 3. Give me all the albums released in a given year in a give location \n",
    "`select * from music_library WHERE YEAR = 1970 AND LOCATION = \"Liverpool\"`\n",
    "#### 4. Give me city that the albumn \"Let It Be\" was recorded\n",
    "`select city from music_library WHERE YEAR = \"1970\" AND ARTIST_NAME = \"The Beatles\" AND ALBUM_NAME=\"Let it Be\"`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is our Collection of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How should we model this data? What should be our Primary Key and Partition Key? Since our data is looking for the YEAR let's start with that. From there we will add clustering columns on Artist Name and Album Name.\n",
    "\n",
    "`Table Name: music_library\n",
    "column 1: Year\n",
    "column 2: Artist Name\n",
    "column 3: Album Name\n",
    "Column 4: City\n",
    "PRIMARY KEY(year, artist_name, album_name)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"CREATE TABLE IF NOT EXISTS music_library \"\n",
    "query = query + \"(year int, artist_name text, album_name text, city text, PRIMARY KEY (year, artist_name, album_name))\"\n",
    "try:\n",
    "    session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's insert our data into of table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"INSERT INTO music_library (year, artist_name, album_name, city)\"\n",
    "query = query + \" VALUES (%s, %s, %s, %s)\"\n",
    "\n",
    "try:\n",
    "    session.execute(query, (1970, \"The Beatles\", \"Let it Be\", \"Liverpool\"))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    session.execute(query, (1965, \"The Beatles\", \"Rubber Soul\", \"Oxford\"))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    session.execute(query, (1965, \"The Who\", \"My Generation\", \"London\"))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    session.execute(query, (1966, \"The Monkees\", \"The Monkees\", \"Los Angeles\"))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    session.execute(query, (1970, \"The Carpenters\", \"Close To You\", \"San Diego\"))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Validate our Data Model with our 4 queries.\n",
    "\n",
    "`select * from music_library WHERE YEAR=1970`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1970 The Beatles Let it Be Liverpool\n",
      "1970 The Carpenters Close To You San Diego\n"
     ]
    }
   ],
   "source": [
    "query = \"select * from music_library WHERE YEAR=1970\"\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print (row.year, row.artist_name, row.album_name, row.city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Success it worked! Let's try the 2nd query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`select * from music_library WHERE YEAR = 1970 AND ARTIST_NAME = \"The Beatles\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1970 The Beatles Let it Be Liverpool\n"
     ]
    }
   ],
   "source": [
    "query = \"select * from music_library WHERE YEAR=1970 AND ARTIST_NAME = 'The Beatles'\"\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print (row.year, row.artist_name, row.album_name, row.city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Success it worked! Let's try the 3rd query.\n",
    "`select * from music_library WHERE YEAR = 1970 AND LOCATION = \"Liverpool\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error from server: code=2200 [Invalid query] message=\"Cannot execute this query as it might involve data filtering and thus may have unpredictable performance. If you want to execute this query despite the performance unpredictability, use ALLOW FILTERING\"\n"
     ]
    }
   ],
   "source": [
    "query = \"select * from music_library WHERE YEAR = 1970 AND city = 'Liverpool'\"\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print (row.year, row.artist_name, row.album_name, row.city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error! You can not try to access a column or a clustering column if you have not used the other defined clustering column. Let's see if we can try it a different way. \n",
    "`select city from music_library WHERE YEAR = \"1970\" AND ARTIST_NAME = \"The Beatles\" AND ALBUM_NAME=\"Let it Be\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liverpool\n"
     ]
    }
   ],
   "source": [
    "query = \"select city from music_library WHERE YEAR = 1970 AND ARTIST_NAME = 'The Beatles' AND ALBUM_NAME='Let it Be'\"\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print (row.city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the sake of the demo, I will drop the table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error from server: code=2200 [Invalid query] message=\"unconfigured table music_library1\"\n"
     ]
    }
   ],
   "source": [
    "query = \"drop table music_library\"\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "query = \"drop table music_library1\"\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And Finally close the session and cluster connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
