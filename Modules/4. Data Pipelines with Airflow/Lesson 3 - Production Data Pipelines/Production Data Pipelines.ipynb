{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending Airflow with Plugins\n",
    "### Extending Airflow\n",
    "Airflow was built with the intention of allowing its users to extend and customize its functionality through <u>**plugins**</u>\n",
    "\n",
    "Custom **Operators** are typically used to capture frequently used operations into a reusable form. \n",
    "\n",
    "<img src=\"images/python_operator.png\">\n",
    "\n",
    "Replacing those usages of Python operator in our DAGs with custom operators can simplify the code and out DAG definition.  \n",
    "\n",
    "<img src=\"images/custom_operator.png\">\n",
    "\n",
    "### Airflow Plugins\n",
    "Airflow was built with the intention of allowing its users to extend and customize its functionality through plugins. The most common types of user-created plugins for Airflow are Operators and Hooks. These plugins make DAGs reusable and simpler to maintain.\n",
    "\n",
    "To create custom operator, follow the steps:\n",
    "\n",
    "1. Identify Operators that perform similar functions and can be consolidated\n",
    "2. Define a new Operator in the plugins folder\n",
    "3. Replace the original Operators with your new custom one, re-parameterize, and instantiate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow Contrib\n",
    "Airflow has a rich and vibrant open source community. This community is constantly adding new functionality and extending the capabilities of Airflow. As an Airflow user, you should always check [Airflow contrib](https://github.com/apache/airflow/tree/master/airflow/contrib) before building your own airflow plugins, to see if what you need already exists.\n",
    "\n",
    "Operators and hooks for common data tools like Apache Spark and Cassandra, as well as vendor specific integrations for Amazon Web Services, Azure, and Google Cloud Platform can be found in Airflow contrib. If the functionality exists and its not quite what you want, that’s a great opportunity to add that functionality through an open source contribution.\n",
    "\n",
    "[Check out Airflow Contrib](https://github.com/apache/airflow/tree/master/airflow/contrib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Operator Plugins\n",
    "**Instructions** : In this exercise, we’ll consolidate repeated code into Operator Plugins\n",
    "1. Move the data quality check logic into a custom operator\n",
    "2. Replace the data quality check PythonOperators with our new custom operator\n",
    "3. Consolidate both the S3 to RedShift functions into a custom operator\n",
    "4. Replace the S3 to RedShift PythonOperators with our new custom operator\n",
    "5. Execute the DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Plugins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p exercises/plugins/operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exercises/plugins/operators/has_rows.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exercises/plugins/operators/has_rows.py\n",
    "\n",
    "import logging\n",
    "\n",
    "from airflow.hooks.postgres_hook import PostgresHook\n",
    "from airflow.models import BaseOperator\n",
    "from airflow.utils.decorators import apply_defaults\n",
    "\n",
    "class HasRowsOperator(BaseOperator):\n",
    "    @apply_defaults\n",
    "    def __init__(self,\n",
    "                 redshift_conn_id=\"\",\n",
    "                 table=\"\",\n",
    "                 *args, **kwargs):\n",
    "        super(HasRowsOperator, self).__init__(*args, **kwargs)\n",
    "        self.table = table\n",
    "        self.redshift_conn_id = redshift_conn_id\n",
    "    \n",
    "    def execute(self,context):\n",
    "        redshift_hook = PostgresHook(self.redshift_conn_id)\n",
    "        records = redshift_hook.get_records(f\"SELECT COUNT(*) FROM {self.table}\")\n",
    "        if len(records) < 1 or len(records[0]) < 1:\n",
    "            raise ValueError((f\"Data quality check failed. {self.table} returned no results\"))\n",
    "        \n",
    "        num_records = records[0][0]\n",
    "        \n",
    "        if num_records < 1:\n",
    "            raise ValueError(f\"Data quality check failed. {self.table} contained 0 rows\")\n",
    "        \n",
    "        logging.info(f\"Data quality on table {self.table} check passed with {records[0][0]} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exercises/plugins/operators/s3_to_redshift.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exercises/plugins/operators/s3_to_redshift.py\n",
    "\n",
    "from airflow.contrib.hooks.aws_hook import AwsHook\n",
    "from airflow.hooks.postgres_hook import PostgresHook\n",
    "from airflow.models import BaseOperator\n",
    "from airflow.utils.decorators import apply_defaults\n",
    "\n",
    "\n",
    "class S3ToRedshiftOperator(BaseOperator):\n",
    "    template_fields = (\"s3_key\",)\n",
    "    copy_sql = \"\"\"\n",
    "        COPY {}\n",
    "        FROM '{}'\n",
    "        ACCESS_KEY_ID '{}'\n",
    "        SECRET_ACCESS_KEY '{}'\n",
    "        IGNOREHEADER {}\n",
    "        DELIMITER '{}'\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    @apply_defaults\n",
    "    def __init__(self,\n",
    "                 redshift_conn_id=\"\",\n",
    "                 aws_credentials_id=\"\",\n",
    "                 table=\"\",\n",
    "                 s3_bucket=\"\",\n",
    "                 s3_key=\"\",\n",
    "                 delimiter=\",\",\n",
    "                 ignore_headers=1,\n",
    "                 *args, **kwargs):\n",
    "\n",
    "        super(S3ToRedshiftOperator, self).__init__(*args, **kwargs)\n",
    "        self.table = table\n",
    "        self.redshift_conn_id = redshift_conn_id\n",
    "        self.s3_bucket = s3_bucket\n",
    "        self.s3_key = s3_key\n",
    "        self.delimiter = delimiter\n",
    "        self.ignore_headers = ignore_headers\n",
    "        self.aws_credentials_id = aws_credentials_id\n",
    "\n",
    "    def execute(self, context):\n",
    "        aws_hook = AwsHook(self.aws_credentials_id)\n",
    "        credentials = aws_hook.get_credentials()\n",
    "        redshift = PostgresHook(postgres_conn_id=self.redshift_conn_id)\n",
    "\n",
    "        self.log.info(\"Clearing data from destination Redshift table\")\n",
    "        redshift.run(\"DELETE FROM {}\".format(self.table))\n",
    "\n",
    "        self.log.info(\"Copying data from S3 to Redshift\")\n",
    "        rendered_key = self.s3_key.format(**context)\n",
    "        s3_path = \"s3://{}/{}\".format(self.s3_bucket, rendered_key)\n",
    "        formatted_sql = S3ToRedshiftOperator.copy_sql.format(\n",
    "            self.table,\n",
    "            s3_path,\n",
    "            credentials.access_key,\n",
    "            credentials.secret_key,\n",
    "            self.ignore_headers,\n",
    "            self.delimiter\n",
    "        )\n",
    "        redshift.run(formatted_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exercises/plugins/operators/facts_calculator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exercises/plugins/operators/facts_calculator.py\n",
    "\n",
    "import logging\n",
    "\n",
    "from airflow.hooks.postgres_hook import PostgresHook\n",
    "from airflow.models import BaseOperator\n",
    "from airflow.utils.decorators import apply_defaults\n",
    "\n",
    "\n",
    "class FactsCalculatorOperator(BaseOperator):\n",
    "    facts_sql_template = \"\"\"\n",
    "    DROP TABLE IF EXISTS {destination_table};\n",
    "    CREATE TABLE {destination_table} AS\n",
    "    SELECT\n",
    "        {groupby_column},\n",
    "        MAX({fact_column}) AS max_{fact_column},\n",
    "        MIN({fact_column}) AS min_{fact_column},\n",
    "        AVG({fact_column}) AS average_{fact_column}\n",
    "    FROM {origin_table}\n",
    "    GROUP BY {groupby_column};\n",
    "    \"\"\"\n",
    "\n",
    "    @apply_defaults\n",
    "    def __init__(self,\n",
    "                 redshift_conn_id=\"\",\n",
    "                 origin_table=\"\",\n",
    "                 destination_table=\"\",\n",
    "                 fact_column=\"\",\n",
    "                 groupby_column=\"\",\n",
    "                 *args, **kwargs):\n",
    "\n",
    "        super(FactsCalculatorOperator, self).__init__(*args, **kwargs)\n",
    "        #\n",
    "        # TODO: Set attributes from __init__ instantiation arguments\n",
    "        #\n",
    "\n",
    "    def execute(self, context):\n",
    "        #\n",
    "        # TODO: Fetch the redshift hook\n",
    "        #\n",
    "\n",
    "        #\n",
    "        # TODO: Format the `facts_sql_template` and run the query against redshift\n",
    "        #\n",
    "\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exercises/plugins/operators/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exercises/plugins/operators/__init__.py\n",
    "\n",
    "from operators.facts_calculator import FactsCalculatorOperator\n",
    "from operators.has_rows import HasRowsOperator\n",
    "from operators.s3_to_redshift import S3ToRedshiftOperator\n",
    "\n",
    "__all__ = [\n",
    "    'FactsCalculatorOperator',\n",
    "    'HasRowsOperator',\n",
    "    'S3ToRedshiftOperator'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exercises/plugins/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exercises/plugins/__init__.py\n",
    "\n",
    "from airflow.plugins_manager import AirflowPlugin\n",
    "\n",
    "import operators\n",
    "\n",
    "# Defining the plugin class\n",
    "class UdacityPlugin(AirflowPlugin):\n",
    "    name = \"udacity_plugin\"\n",
    "    operators = [\n",
    "        operators.FactsCalculatorOperator,\n",
    "        operators.HasRowsOperator,\n",
    "        operators.S3ToRedshiftOperator\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r exercises/plugins/ $AIRFLOW_HOME/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exercises/lesson3_exercise1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exercises/lesson3_exercise1.py\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.contrib.hooks.aws_hook import AwsHook\n",
    "from airflow.hooks.postgres_hook import PostgresHook\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.contrib.hooks.aws_hook import AwsHook\n",
    "from airflow.hooks.postgres_hook import PostgresHook\n",
    "\n",
    "from operators.has_rows import HasRowsOperator\n",
    "from operators.s3_to_redshift import S3ToRedshiftOperator\n",
    "from airflow.operators import (\n",
    "    PostgresOperator,\n",
    "    PythonOperator,\n",
    ")\n",
    "\n",
    "import sql_statements\n",
    "\n",
    "dag = DAG(\n",
    "    \"lesson3.exercise1\",\n",
    "    start_date=datetime.datetime(2018, 1, 1, 0, 0, 0, 0),\n",
    "    end_date=datetime.datetime(2018, 12, 1, 0, 0, 0, 0),\n",
    "    schedule_interval=\"@monthly\",\n",
    "    max_active_runs=1\n",
    ")\n",
    "\n",
    "create_trips_table = PostgresOperator(\n",
    "    task_id=\"create_trips_table\",\n",
    "    dag=dag,\n",
    "    postgres_conn_id=\"redshift\",\n",
    "    sql=sql_statements.CREATE_TRIPS_TABLE_SQL\n",
    ")\n",
    "\n",
    "copy_trips_task = S3ToRedshiftOperator(\n",
    "    task_id=\"load_trips_from_s3_to_redshift\",\n",
    "    dag=dag,\n",
    "    table=\"trips\",\n",
    "    redshift_conn_id=\"redshift\",\n",
    "    aws_credentials_id=\"aws_credentials\",\n",
    "    s3_bucket=\"udacity-dend\",\n",
    "    s3_key=\"data-pipelines/divvy/partitioned/{execution_date.year}/{execution_date.month}/divvy_trips.csv\"\n",
    ")\n",
    "\n",
    "check_trips = HasRowsOperator(\n",
    "    task_id='check_trips_data',\n",
    "    dag=dag,\n",
    "    redshift_conn_id=\"redshift\",\n",
    "    table=\"trips\"\n",
    ")\n",
    "\n",
    "create_stations_table = PostgresOperator(\n",
    "    task_id=\"create_stations_table\",\n",
    "    dag=dag,\n",
    "    postgres_conn_id=\"redshift\",\n",
    "    sql=sql_statements.CREATE_STATIONS_TABLE_SQL,\n",
    ")\n",
    "\n",
    "copy_stations_task = S3ToRedshiftOperator(\n",
    "    task_id=\"load_stations_from_s3_to_redshift\",\n",
    "    dag=dag,\n",
    "    redshift_conn_id=\"redshift\",\n",
    "    aws_credentials_id=\"aws_credentials\",\n",
    "    s3_bucket=\"udacity-dend\",\n",
    "    s3_key=\"data-pipelines/divvy/unpartitioned/divvy_stations_2017.csv\",\n",
    "    table=\"stations\"\n",
    ")\n",
    "\n",
    "check_stations = HasRowsOperator(\n",
    "    task_id='check_stations_data',\n",
    "    dag=dag,\n",
    "    redshift_conn_id=\"redshift\",\n",
    "    table=\"stations\"\n",
    ")\n",
    "\n",
    "create_trips_table >> copy_trips_task\n",
    "create_stations_table >> copy_stations_task\n",
    "copy_stations_task >> check_stations\n",
    "copy_trips_task >> check_trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exercises/sql_statements.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exercises/sql_statements.py\n",
    "\n",
    "CREATE_TRIPS_TABLE_SQL = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS trips (\n",
    "trip_id INTEGER NOT NULL,\n",
    "start_time TIMESTAMP NOT NULL,\n",
    "end_time TIMESTAMP NOT NULL,\n",
    "bikeid INTEGER NOT NULL,\n",
    "tripduration DECIMAL(16,2) NOT NULL,\n",
    "from_station_id INTEGER NOT NULL,\n",
    "from_station_name VARCHAR(100) NOT NULL,\n",
    "to_station_id INTEGER NOT NULL,\n",
    "to_station_name VARCHAR(100) NOT NULL,\n",
    "usertype VARCHAR(20),\n",
    "gender VARCHAR(6),\n",
    "birthyear INTEGER,\n",
    "PRIMARY KEY(trip_id))\n",
    "DISTSTYLE ALL;\n",
    "\"\"\"\n",
    "\n",
    "CREATE_STATIONS_TABLE_SQL = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS stations (\n",
    "id INTEGER NOT NULL,\n",
    "name VARCHAR(250) NOT NULL,\n",
    "city VARCHAR(100) NOT NULL,\n",
    "latitude DECIMAL(9, 6) NOT NULL,\n",
    "longitude DECIMAL(9, 6) NOT NULL,\n",
    "dpcapacity INTEGER NOT NULL,\n",
    "online_date TIMESTAMP NOT NULL,\n",
    "PRIMARY KEY(id))\n",
    "DISTSTYLE ALL;\n",
    "\"\"\"\n",
    "\n",
    "COPY_SQL = \"\"\"\n",
    "COPY {}\n",
    "FROM '{}'\n",
    "ACCESS_KEY_ID '{{}}'\n",
    "SECRET_ACCESS_KEY '{{}}'\n",
    "IGNOREHEADER 1\n",
    "DELIMITER ','\n",
    "\"\"\"\n",
    "\n",
    "COPY_MONTHLY_TRIPS_SQL = COPY_SQL.format(\n",
    "    \"trips\",\n",
    "    \"s3://udacity-dend/data-pipelines/divvy/partitioned/{year}/{month}/divvy_trips.csv\"\n",
    ")\n",
    "\n",
    "COPY_ALL_TRIPS_SQL = COPY_SQL.format(\n",
    "    \"trips\",\n",
    "    \"s3://udacity-dend/data-pipelines/divvy/unpartitioned/divvy_trips_2018.csv\"\n",
    ")\n",
    "\n",
    "COPY_STATIONS_SQL = COPY_SQL.format(\n",
    "    \"stations\",\n",
    "    \"s3://udacity-dend/data-pipelines/divvy/unpartitioned/divvy_stations_2017.csv\"\n",
    ")\n",
    "\n",
    "LOCATION_TRAFFIC_SQL = \"\"\"\n",
    "BEGIN;\n",
    "DROP TABLE IF EXISTS station_traffic;\n",
    "CREATE TABLE station_traffic AS\n",
    "SELECT\n",
    "    DISTINCT(t.from_station_id) AS station_id,\n",
    "    t.from_station_name AS station_name,\n",
    "    num_departures,\n",
    "    num_arrivals\n",
    "FROM trips t\n",
    "JOIN (\n",
    "    SELECT\n",
    "        from_station_id,\n",
    "        COUNT(from_station_id) AS num_departures\n",
    "    FROM trips\n",
    "    GROUP BY from_station_id\n",
    ") AS fs ON t.from_station_id = fs.from_station_id\n",
    "JOIN (\n",
    "    SELECT\n",
    "        to_station_id,\n",
    "        COUNT(to_station_id) AS num_arrivals\n",
    "    FROM trips\n",
    "    GROUP BY to_station_id\n",
    ") AS ts ON t.from_station_id = ts.to_station_id\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp exercises/sql_statements.py $AIRFLOW_HOME/dags\n",
    "!cp exercises/lesson3_exercise1.py $AIRFLOW_HOME/dags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Boundaries\n",
    "DAG tasks should be designed such that they are:\n",
    "\n",
    "* Atomic and have a single purpose\n",
    "* Maximize parallelism\n",
    "* Make failure states obvious\n",
    "Every task in your dag should perform **only one job**.\n",
    "\n",
    "> Write programs that do one thing and do it well.\n",
    "> -- <cite>Ken Thompson’s Unix Philosophy</cite>\n",
    "\n",
    "### Benefits of Task Boundaries\n",
    "* Re-visitable: Task boundaries are useful for you if you revisit a pipeline you wrote after a 6 month absence. You'll have a much easier time understanding how it works and the lineage of the data if the boundaries between tasks are clear and well defined. This is true in the code itself, and within the Airflow UI.\n",
    "* Tasks that do just one thing are often more easily parallelized. This parallelization can offer a significant speedup in the execution of our DAGs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Refactor a DAG\n",
    "\n",
    "**Instructions**: In this exercise, we’ll refactor a DAG with a single overloaded task into a DAG with several tasks with well-defined boundaries\n",
    "1. Read through the DAG and identify points in the DAG that could be split apart\n",
    "2. Split the DAG into multiple PythonOperators\n",
    "3. Run the DAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exercises/lesson3_exercise2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exercises/lesson3_exercise2.py\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.hooks.postgres_hook import PostgresHook\n",
    "\n",
    "from airflow.operators.postgres_operator import PostgresOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "\n",
    "#\n",
    "# TODO: Finish refactoring this function into the appropriate set of tasks,\n",
    "#       instead of keeping this one large task.\n",
    "#\n",
    "def load_and_analyze(*args, **kwargs):\n",
    "    redshift_hook = PostgresHook(\"redshift\")\n",
    "\n",
    "    # Find all trips where the rider was under 18\n",
    "    redshift_hook.run(\"\"\"\n",
    "        BEGIN;\n",
    "        DROP TABLE IF EXISTS younger_riders;\n",
    "        CREATE TABLE younger_riders AS (\n",
    "            SELECT * FROM trips WHERE birthyear > 2000\n",
    "        );\n",
    "        COMMIT;\n",
    "    \"\"\")\n",
    "    records = redshift_hook.get_records(\"\"\"\n",
    "        SELECT birthyear FROM younger_riders ORDER BY birthyear DESC LIMIT 1\n",
    "    \"\"\")\n",
    "    if len(records) > 0 and len(records[0]) > 0:\n",
    "        logging.info(f\"Youngest rider was born in {records[0][0]}\")\n",
    "\n",
    "\n",
    "    # Find out how often each bike is ridden\n",
    "    redshift_hook.run(\"\"\"\n",
    "        BEGIN;\n",
    "        DROP TABLE IF EXISTS lifetime_rides;\n",
    "        CREATE TABLE lifetime_rides AS (\n",
    "            SELECT bikeid, COUNT(bikeid)\n",
    "            FROM trips\n",
    "            GROUP BY bikeid\n",
    "        );\n",
    "        COMMIT;\n",
    "    \"\"\")\n",
    "\n",
    "    # Count the number of stations by city\n",
    "    redshift_hook.run(\"\"\"\n",
    "        BEGIN;\n",
    "        DROP TABLE IF EXISTS city_station_counts;\n",
    "        CREATE TABLE city_station_counts AS(\n",
    "            SELECT city, COUNT(city)\n",
    "            FROM stations\n",
    "            GROUP BY city\n",
    "        );\n",
    "        COMMIT;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def log_oldest():\n",
    "    redshift_hook = PostgresHook(\"redshift\")\n",
    "    records = redshift_hook.get_records(\"\"\"\n",
    "        SELECT birthyear FROM older_riders ORDER BY birthyear ASC LIMIT 1\n",
    "    \"\"\")\n",
    "    if len(records) > 0 and len(records[0]) > 0:\n",
    "        logging.info(f\"Oldest rider was born in {records[0][0]}\")\n",
    "\n",
    "\n",
    "dag = DAG(\n",
    "    \"lesson3.exercise2\",\n",
    "    start_date=datetime.datetime.utcnow()\n",
    ")\n",
    "\n",
    "load_and_analyze = PythonOperator(\n",
    "    task_id='load_and_analyze',\n",
    "    dag=dag,\n",
    "    python_callable=load_and_analyze,\n",
    "    provide_context=True,\n",
    ")\n",
    "\n",
    "create_oldest_task = PostgresOperator(\n",
    "    task_id=\"create_oldest\",\n",
    "    dag=dag,\n",
    "    sql=\"\"\"\n",
    "        BEGIN;\n",
    "        DROP TABLE IF EXISTS older_riders;\n",
    "        CREATE TABLE older_riders AS (\n",
    "            SELECT * FROM trips WHERE birthyear > 0 AND birthyear <= 1945\n",
    "        );\n",
    "        COMMIT;\n",
    "    \"\"\",\n",
    "    postgres_conn_id=\"redshift\"\n",
    ")\n",
    "\n",
    "log_oldest_task = PythonOperator(\n",
    "    task_id=\"log_oldest\",\n",
    "    dag=dag,\n",
    "    python_callable=log_oldest\n",
    ")\n",
    "\n",
    "load_and_analyze >> create_oldest_task\n",
    "create_oldest_task >> log_oldest_task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exercises/lesson3_exercise2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exercises/lesson3_exercise2.py\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.hooks.postgres_hook import PostgresHook\n",
    "\n",
    "from airflow.operators.postgres_operator import PostgresOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "\n",
    "def log_younger():\n",
    "    redshift_hook = PostgresHook(\"redshift\")\n",
    "    records = redshift_hook.get_records(\"\"\"\n",
    "        SELECT birthyear FROM younger_riders ORDER BY birthyear DESC LIMIT 1\n",
    "    \"\"\")\n",
    "    if len(records) > 0 and len(records[0]) > 0:\n",
    "        logging.info(f\"Youngest rider was born in {records[0][0]}\")\n",
    "        \n",
    "def log_oldest():\n",
    "    redshift_hook = PostgresHook(\"redshift\")\n",
    "    records = redshift_hook.get_records(\"\"\"\n",
    "        SELECT birthyear FROM older_riders ORDER BY birthyear ASC LIMIT 1\n",
    "    \"\"\")\n",
    "    if len(records) > 0 and len(records[0]) > 0:\n",
    "        logging.info(f\"Oldest rider was born in {records[0][0]}\")\n",
    "\n",
    "dag = DAG(\n",
    "    \"lesson3.exercise2\",\n",
    "    start_date=datetime.datetime.now() - datetime.timedelta(days = 1)\n",
    ")\n",
    "\n",
    "#  Creates young riders table\n",
    "young_riders = PostgresOperator(\n",
    "    task_id='create_younger',\n",
    "    dag=dag,\n",
    "    sql=\"\"\"\n",
    "        BEGIN;\n",
    "        DROP TABLE IF EXISTS younger_riders;\n",
    "        CREATE TABLE younger_riders AS (\n",
    "            SELECT * FROM trips WHERE birthyear > 2000\n",
    "        );\n",
    "        COMMIT;\n",
    "    \"\"\",\n",
    "    postgres_conn_id=\"redshift\"\n",
    ")\n",
    "\n",
    "# Quality checks\n",
    "log_young_tasks= PythonOperator(\n",
    "    task_id=\"log_younger\",\n",
    "    dag=dag,\n",
    "    python_callable=log_younger\n",
    ")\n",
    "\n",
    "# Create lifetime_rides\n",
    "lifetime_rides = PostgresOperator(\n",
    "    task_id='lifetime_rides',\n",
    "    dag=dag,\n",
    "    sql=\"\"\"\n",
    "        BEGIN;\n",
    "        DROP TABLE IF EXISTS lifetime_rides;\n",
    "        CREATE TABLE lifetime_rides AS (\n",
    "            SELECT bikeid, COUNT(bikeid)\n",
    "            FROM trips\n",
    "            GROUP BY bikeid\n",
    "        );\n",
    "        COMMIT;\n",
    "    \"\"\",\n",
    "    postgres_conn_id=\"redshift\"\n",
    ")\n",
    "\n",
    "# Create station_counts\n",
    "station_counts = PostgresOperator(\n",
    "    task_id='station_counts',\n",
    "    dag=dag,\n",
    "    sql=\"\"\"\n",
    "        BEGIN;\n",
    "        DROP TABLE IF EXISTS city_station_counts;\n",
    "        CREATE TABLE city_station_counts AS(\n",
    "            SELECT city, COUNT(city)\n",
    "            FROM stations\n",
    "            GROUP BY city\n",
    "        );\n",
    "        COMMIT;\n",
    "    \"\"\",\n",
    "    postgres_conn_id=\"redshift\"\n",
    ")\n",
    "\n",
    "# creates oldest riders\n",
    "create_oldest_task = PostgresOperator(\n",
    "    task_id=\"create_oldest\",\n",
    "    dag=dag,\n",
    "    sql=\"\"\"\n",
    "        BEGIN;\n",
    "        DROP TABLE IF EXISTS older_riders;\n",
    "        CREATE TABLE older_riders AS (\n",
    "            SELECT * FROM trips WHERE birthyear > 0 AND birthyear <= 1945\n",
    "        );\n",
    "        COMMIT;\n",
    "    \"\"\",\n",
    "    postgres_conn_id=\"redshift\"\n",
    ")\n",
    "\n",
    "# Quality checks\n",
    "log_oldest_tasks= PythonOperator(\n",
    "    task_id=\"log_oldest\",\n",
    "    dag=dag,\n",
    "    python_callable=log_oldest\n",
    ")\n",
    "\n",
    "young_riders >> log_young_tasks\n",
    "log_young_tasks >> lifetime_rides\n",
    "lifetime_rides >> station_counts\n",
    "station_counts >> create_oldest_task\n",
    "create_oldest_task >> log_oldest_tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp exercises/lesson3_exercise2.py $AIRFLOW_HOME/dags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SubDAGs\n",
    "Commonly repeated series of tasks within DAGs can be captured as reusable SubDAGs. Benefits include:\n",
    "\n",
    "* Decrease the amount of code we need to write and maintain to create a new DAG\n",
    "* Easier to understand the high level goals of a DAG\n",
    "* Bug fixes, speedups, and other enhancements can be made more quickly and  distributed to all DAGs that use that SubDAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawbacks of Using SubDAGs\n",
    "* Limit the visibility within the Airflow UI\n",
    "* Abstraction makes understanding what the DAG is doing more difficult\n",
    "* Encourages premature optimization\n",
    "\n",
    "### Common Questions\n",
    "\n",
    "**Can Airflow nest subDAGs?**  Yes, you can nest subDAGs. However, you should have a really good reason to do so because it makes it much harder to understand what's going on in the code. Generally, subDAGs are not necessary at all, let alone subDAGs within subDAGs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Subdags\n",
    "\n",
    "<img src=\"images/subdag-quiz.png\">\n",
    "\n",
    "### QUESTION 1 OF 3\n",
    "The diagram above shows two DAGs and their tasks. Which tasks could be placed into a SubDAG for reuse?\n",
    "- [x] HTTP Data Fetch\n",
    "- [x] Data Check\n",
    "- [ ] Merge\n",
    "- [ ] Check API Status\n",
    "\n",
    "### QUESTION 2 OF 3\n",
    "Which of the following are advantages of SubDAGs?\n",
    "- [ ] Improves code visibility\n",
    "- [x] Reduces duplicated code in DAGs\n",
    "- [ ] Improves visibility into DAG steps in Airflow UI\n",
    "- [x] Allows for simple reusability of commonly occurring task patterns\n",
    "\n",
    "### QUESTION 3 OF 3\n",
    "Which of the following are drawbacks of SubDAGs?\n",
    "\n",
    "- [x] Reduces visibility in the Airflow UI\n",
    "- [ ] Sharing of common patterns between DAGs\n",
    "- [x] Harder to understand and maintain if improperly scoped\n",
    "- [x] Encourages Premature Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: SubDAGs\n",
    "**Instructions** : In this exercise, we’ll place our S3 to RedShift Copy operations into a SubDag.\n",
    "1. Consolidate HasRowsOperator into the SubDag\n",
    "2. Reorder the tasks to take advantage of the SubDag Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('exercises/lesson3_exercise3/'):\n",
    "    os.makedirs('exercises/lesson3_exercise3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exercises/lesson3_exercise3/subdag.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exercises/lesson3_exercise3/subdag.py\n",
    "\n",
    "import datetime\n",
    "\n",
    "from airflow import DAG\n",
    "from operators.has_rows import HasRowsOperator\n",
    "from operators.s3_to_redshift import S3ToRedshiftOperator\n",
    "from airflow.operators.postgres_operator import PostgresOperator\n",
    "\n",
    "import sql\n",
    "\n",
    "# Returns a DAG which creates a table if it does not exist, and then proceeds\n",
    "# to load data into that table from S3. When the load is complete, a data\n",
    "# quality  check is performed to assert that at least one row of data is\n",
    "# present.\n",
    "def get_s3_to_redshift_dag(\n",
    "        parent_dag_name,\n",
    "        task_id,\n",
    "        redshift_conn_id,\n",
    "        aws_credentials_id,\n",
    "        table,\n",
    "        create_sql_stmt,\n",
    "        s3_bucket,\n",
    "        s3_key,\n",
    "        *args, **kwargs):\n",
    "    dag = DAG(\n",
    "        f\"{parent_dag_name}.{task_id}\",\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    create_task = PostgresOperator(\n",
    "        task_id=f\"create_{table}_table\",\n",
    "        dag=dag,\n",
    "        postgres_conn_id=redshift_conn_id,\n",
    "        sql=create_sql_stmt\n",
    "    )\n",
    "\n",
    "    copy_task = S3ToRedshiftOperator(\n",
    "        task_id=f\"load_{table}_from_s3_to_redshift\",\n",
    "        dag=dag,\n",
    "        table=table,\n",
    "        redshift_conn_id=redshift_conn_id,\n",
    "        aws_credentials_id=aws_credentials_id,\n",
    "        s3_bucket=s3_bucket,\n",
    "        s3_key=s3_key\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # TODO: Move the HasRowsOperator task here from the DAG\n",
    "    #\n",
    "    check_trips = HasRowsOperator(\n",
    "        task_id=f\"check_{table}_data\",\n",
    "        dag=dag,\n",
    "        redshift_conn_id=\"redshift\",\n",
    "        table=table\n",
    "    )\n",
    "\n",
    "    create_task >> copy_task\n",
    "    #\n",
    "    # TODO: Use DAG ordering to place the check task\n",
    "    #\n",
    "    copy_task >> check_trips\n",
    "\n",
    "    return dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exercises/lesson3_exercise3/dag.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exercises/lesson3_exercise3/dag.py\n",
    "import datetime\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.postgres_operator import PostgresOperator\n",
    "\n",
    "from operators.has_rows import HasRowsOperator\n",
    "from operators.s3_to_redshift import S3ToRedshiftOperator\n",
    "from airflow.operators.subdag_operator import SubDagOperator\n",
    "from lesson3_exercise3.subdag import get_s3_to_redshift_dag\n",
    "\n",
    "import sql_statements\n",
    "\n",
    "\n",
    "start_date = datetime.datetime.now() - datetime.timedelta(days=1)\n",
    "\n",
    "dag = DAG(\n",
    "    \"lesson3.exercise3\",\n",
    "    start_date=start_date,\n",
    ")\n",
    "\n",
    "trips_task_id = \"trips_subdag\"\n",
    "trips_subdag_task = SubDagOperator(\n",
    "    subdag=get_s3_to_redshift_dag(\n",
    "        \"lesson3.exercise3\",\n",
    "        trips_task_id,\n",
    "        \"redshift\",\n",
    "        \"aws_credentials\",\n",
    "        \"trips\",\n",
    "        sql_statements.CREATE_TRIPS_TABLE_SQL,\n",
    "        s3_bucket=\"udacity-dend\",\n",
    "        s3_key=\"data-pipelines/divvy/unpartitioned/divvy_trips_2018.csv\",\n",
    "        start_date=start_date,\n",
    "    ),\n",
    "    task_id=trips_task_id,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "stations_task_id = \"stations_subdag\"\n",
    "stations_subdag_task = SubDagOperator(\n",
    "    subdag=get_s3_to_redshift_dag(\n",
    "        \"lesson3.exercise3\",\n",
    "        stations_task_id,\n",
    "        \"redshift\",\n",
    "        \"aws_credentials\",\n",
    "        \"stations\",\n",
    "        sql_statements.CREATE_STATIONS_TABLE_SQL,\n",
    "        s3_bucket=\"udacity-dend\",\n",
    "        s3_key=\"data-pipelines/divvy/unpartitioned/divvy_stations_2017.csv\",\n",
    "        start_date=start_date,\n",
    "    ),\n",
    "    task_id=stations_task_id,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "\n",
    "location_traffic_task = PostgresOperator(\n",
    "    task_id=\"calculate_location_traffic\",\n",
    "    dag=dag,\n",
    "    postgres_conn_id=\"redshift\",\n",
    "    sql=sql_statements.LOCATION_TRAFFIC_SQL\n",
    ")\n",
    "\n",
    "#\n",
    "# TODO: Reorder the Graph once you have moved the checks\n",
    "#\n",
    "trips_subdag_task >> location_traffic_task\n",
    "stations_subdag_task >> location_traffic_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r exercises/lesson3_exercise3/ $AIRFLOW_HOME/dags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Monitoring\n",
    "Airflow can surface metrics and emails to help you stay on top of pipeline issues.\n",
    "\n",
    "### SLAs\n",
    "Airflow DAGs may optionally specify an SLA, or “Service Level Agreement”, which is defined as **a time by which a DAG must complete**. For time-sensitive applications these features are critical for developing trust amongst your pipeline customers and ensuring that data is delivered while it is still meaningful. Slipping SLAs can also be **early indicators of performance problems**, or a need to scale up the size of your Airflow cluster\n",
    "\n",
    "### Emails and Alerts\n",
    "Airflow can be configured to send emails on DAG and task state changes. These state changes may include successes, failures, or retries. Failure emails can allow you to easily trigger alerts. It is common for alerting systems like PagerDuty to accept emails as a source of alerts. If a mission-critical data pipeline fails, you will need to know as soon as possible to get online and get it fixed.\n",
    "\n",
    "### Metrics\n",
    "Airflow comes out of the box with the ability to send system metrics using a metrics aggregator called statsd. Statsd can be coupled with metrics visualization tools like [Grafana](https://grafana.com/) to provide you and your team high level insights into the overall performance of your DAGs, jobs, and tasks. These systems can be integrated into your alerting system, such as pagerduty, so that you can ensure problems are dealt with immediately. These Airflow system-level metrics allow you and your team to stay ahead of issues before they even occur by watching long-term trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Building a Full DAG\n",
    "In this exercise you will construct a DAG and custom operator end-to-end on your own. Our bikeshare company would like to create a trips facts table every time we update the trips data. You've decided to make the facts table creation a custom operator so that it can be reused for other tables in the future.\n",
    "\n",
    "The skeleton of the custom operator, as well as the facts SQL statement has been created for you and can be found in `plugins/operators/facts_calculator.py`. The DAG itself will be defined in `dags/lesson3/exercise4.py`.\n",
    "\n",
    "Using the previous exercises as examples, follow the instructions in the DAG and Operator file to complete the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exercises/plugins/operators/facts_calculator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exercises/plugins/operators/facts_calculator.py\n",
    "\n",
    "import loggin\n",
    "from airflow.hooks.postgres_hook import PostgresHook\n",
    "from airflow.models import BaseOperator\n",
    "from airflow.utils.decorators import apply_defaults\n",
    "\n",
    "class FactsCalculatorOperator(BaseOperator):\n",
    "    facts_sql_template = \"\"\"\n",
    "    DROP TABLE IF EXISTS {destination_table};\n",
    "    CREATE TABLE {destination_table} AS\n",
    "    SELECT\n",
    "        {groupby_column},\n",
    "        MAX({fact_column}) AS max_{fact_column},\n",
    "        MIN({fact_column}) AS min_{fact_column},\n",
    "        AVG({fact_column}) AS average_{fact_column}\n",
    "    FROM {origin_table}\n",
    "    GROUP BY {groupby_column};\n",
    "    \"\"\"\n",
    "    \n",
    "    @apply_defaults\n",
    "    def __init__(self,\n",
    "                 redshift_conn_id=\"\",\n",
    "                 origin_table=\"\",\n",
    "                 destination_table=\"\",\n",
    "                 fact_column=\"\",\n",
    "                 groupby_column=\"\",\n",
    "                 *args, **kwargs):\n",
    "\n",
    "        super(FactsCalculatorOperator, self).__init__(*args, **kwargs)\n",
    "        self.redshift_conn_id = redshift_conn_id\n",
    "        self.origin_table = origin_table\n",
    "        self.destination_table = destination_table\n",
    "        self.fact_column = fact_column\n",
    "        self.groupby_column = groupby_column\n",
    "\n",
    "    def execute(self, context):\n",
    "        redshift = PostgresHook(postgres_conn_id=self.redshift_conn_id)\n",
    "        facts_sql = FactsCalculatorOperator.facts_sql_template.format(\n",
    "            origin_table=self.origin_table,\n",
    "            destination_table=self.destination_table,\n",
    "            fact_column=self.fact_column,\n",
    "            groupby_column=self.groupby_column\n",
    "        )\n",
    "        redshift.run(facts_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r exercises/plugins/ $AIRFLOW_HOME/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exercises/lesson3_exercise4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exercises/lesson3_exercise4.py\n",
    "\n",
    "import datetime\n",
    "\n",
    "from airflow import DAG\n",
    "\n",
    "from operators.has_rows import HasRowsOperator \n",
    "from operators.facts_calculator import FactsCalculatorOperator\n",
    "from operators.s3_to_redshift import S3ToRedshiftOperator\n",
    "\n",
    "\n",
    "#\n",
    "# The following DAG performs the following functions:\n",
    "#\n",
    "#       1. Loads Trip data from S3 to RedShift\n",
    "#       2. Performs a data quality check on the Trips table in RedShift\n",
    "#       3. Uses the FactsCalculatorOperator to create a Facts table in Redshift\n",
    "#           a. **NOTE**: to complete this step you must complete the FactsCalcuatorOperator\n",
    "#              skeleton defined in plugins/operators/facts_calculator.py\n",
    "#\n",
    "dag = DAG(\"lesson3.exercise4\", start_date=datetime.datetime.utcnow())\n",
    "\n",
    "#\n",
    "# The following code will load trips data from S3 to RedShift. Use the s3_key\n",
    "#       \"data-pipelines/divvy/unpartitioned/divvy_trips_2018.csv\"\n",
    "#       and the s3_bucket \"udacity-dend\"\n",
    "#\n",
    "copy_trips_task = S3ToRedshiftOperator(\n",
    "    task_id=\"load_trips_from_s3_to_redshift\",\n",
    "    dag=dag,\n",
    "    table=\"trips\",\n",
    "    redshift_conn_id=\"redshift\",\n",
    "    aws_credentials_id=\"aws_credentials\",\n",
    "    s3_bucket=\"udacity-dend\",\n",
    "    s3_key=\"data-pipelines/divvy/unpartitioned/divvy_trips_2018.csv\"\n",
    ")\n",
    "\n",
    "#\n",
    "#  Data quality check on the Trips table\n",
    "#\n",
    "check_trips = HasRowsOperator(\n",
    "    task_id=\"check_trips_data\",\n",
    "    dag=dag,\n",
    "    redshift_conn_id=\"redshift\",\n",
    "    table=\"trips\"\n",
    ")\n",
    "\n",
    "#\n",
    "# We use the FactsCalculatorOperator to create a Facts table in RedShift. The fact column is\n",
    "#  `tripduration` and the groupby_column is `bikeid`\n",
    "#\n",
    "calculate_facts = FactsCalculatorOperator(\n",
    "    task_id=\"calculate_facts_trips\",\n",
    "    dag=dag,\n",
    "    redshift_conn_id=\"redshift\",\n",
    "    origin_table=\"trips\",\n",
    "    destination_table=\"trips_facts\",\n",
    "    fact_column=\"tripduration\",\n",
    "    groupby_column=\"bikeid\"\n",
    ")\n",
    "\n",
    "#\n",
    "# Task ordering for the DAG tasks \n",
    "#\n",
    "copy_trips_task >> check_trips\n",
    "check_trips >> calculate_facts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
