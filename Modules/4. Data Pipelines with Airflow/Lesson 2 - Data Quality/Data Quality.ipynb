{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Lineage\n",
    "### Definition\n",
    "The data lineage of a dataset describes the discrete steps involved in the creation, movement, and calculation of that dataset.\n",
    "\n",
    "### Why is Data Lineage important?\n",
    "1. **Instilling Confidence**: Being able to describe the data lineage of a particular dataset or analysis will build confidence in data consumers (engineers, analysts, data scientists, etc.) that our data pipeline is creating meaningful results using the correct datasets. If the data lineage is unclear, its less likely that the data consumers will trust or use the data.\n",
    "2. **Defining Metrics**: Another major benefit of surfacing data lineage is that it allows everyone in the organization to agree on the definition of how a particular metric is calculated.\n",
    "3. **Debugging**: Data lineage helps data engineers track down the root of errors when they occur. If each step of the data movement and transformation process is well described, it's easy to find problems when they occur.\n",
    "\n",
    "In general, data lineage has important implications for a business. Each department or business unit's success is tied to data and to the flow of data between departments. For e.g., sales departments rely on data to make sales forecasts, while at the same time the finance department would need to track sales and revenue. Each of these departments and roles depend on data, and knowing where to find the data. Data flow and data lineage tools enable data engineers and architects to track the flow of this large web of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Data Lineage\n",
    "As we already know that airflow parses our DAG and surfaces a visualization of the graph. Airflow also keep tracks of all the runs of a particular DAG. Airflow also shows us the rendered code for each task. One thing to keep in mind that airflow keeps a record of historical DAGs and task executions but it does not store the data from those historical runs. Whatever the latest code is in your DAG definition is what airflow will render for you in the browser. So, be careful of making the assumption of what was running historically. So if you modify the DAG then the code for the historical run will also change. \n",
    "To see the demo watch this [tutorial](https://www.youtube.com/watch?v=1IGTicTXeUQ) by Udacity on youtube.\n",
    "\n",
    "### QUESTION 1 OF 3\n",
    "What is the data lineage of a dataset?\n",
    "- [ ] The starting location of a dataset\n",
    "- [x] Description of the discrete steps involved in the creation, movement, and calculation of that dataset\n",
    "- [ ] The final destination of a dataset\n",
    "- [ ] The calculation steps involved in producing a dataset\n",
    "\n",
    "### QUESTION 2 OF 3\n",
    "Which of the following are benefits of visualizing data lineage?\n",
    "- [ ] Allows business users to modify our data pipelines\n",
    "- [x] Builds confidence in our users that our data pipelines are designed properly\n",
    "- [x] Helps organizations surface and agree on dataset definitions\n",
    "- [ ] Provides easy access to credential management\n",
    "- [x] Makes locating errors more obvious\n",
    "\n",
    "### QUESTION 3 OF 3\n",
    "Which components of Airflow can be used to track data lineage?\n",
    "- [ ] Connections Configuration Page\n",
    "- [x] Rendered code tab for a task\n",
    "- [x] Graph view for a DAG\n",
    "- [x] Historical runs under the tree view\n",
    "- [ ] Airflow home page\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating exercises folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('exercises'):\n",
    "    os.makedirs('exercises')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Data Lineage in Airflow\n",
    "\n",
    "**Instructions**:\n",
    "1. Run the DAG as it is first, and observe the Airflow UI\n",
    "2. Next, open up the DAG and add the copy and load tasks\n",
    "3. Reload the Airflow UI and run the DAG once more, observing the Airflow UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exercises/lesson2_exercise1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exercises/lesson2_exercise1.py\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.contrib.hooks.aws_hook import AwsHook\n",
    "from airflow.hooks.postgres_hook import PostgresHook\n",
    "from airflow.operators.postgres_operator import PostgresOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "import sql_statements\n",
    "\n",
    "def load_trip_data_to_redshift(*args, **kwargs):\n",
    "    aws_hook = AwsHook(\"aws_credentials\")\n",
    "    credentials = aws_hook.get_credentials()\n",
    "    redshift_hook = PostgresHook(\"redshift\")\n",
    "    sql_stmt = sql_statements.COPY_ALL_TRIPS_SQL.format(\n",
    "        credentials.access_key,\n",
    "        credentials.secret_key,\n",
    "    )\n",
    "    redshift_hook.run(sql_stmt)\n",
    "\n",
    "\n",
    "def load_station_data_to_redshift(*args, **kwargs):\n",
    "    aws_hook = AwsHook(\"aws_credentials\")\n",
    "    credentials = aws_hook.get_credentials()\n",
    "    redshift_hook = PostgresHook(\"redshift\")\n",
    "    sql_stmt = sql_statements.COPY_STATIONS_SQL.format(\n",
    "        credentials.access_key,\n",
    "        credentials.secret_key,\n",
    "    )\n",
    "    redshift_hook.run(sql_stmt)\n",
    "\n",
    "\n",
    "dag = DAG(\n",
    "    'lesson2.exercise1',\n",
    "    start_date=datetime.datetime.now() - datetime.timedelta(days=1)\n",
    ")\n",
    "\n",
    "create_trips_table = PostgresOperator(\n",
    "    task_id=\"create_trips_table\",\n",
    "    dag=dag,\n",
    "    postgres_conn_id=\"redshift\",\n",
    "    sql=sql_statements.CREATE_TRIPS_TABLE_SQL\n",
    ")\n",
    "\n",
    "copy_trips_task = PythonOperator(\n",
    "    task_id='load_trips_from_s3_to_redshift',\n",
    "    dag=dag,\n",
    "    python_callable=load_trip_data_to_redshift,\n",
    ")\n",
    "\n",
    "create_stations_table = PostgresOperator(\n",
    "    task_id=\"create_stations_table\",\n",
    "    dag=dag,\n",
    "    postgres_conn_id=\"redshift\",\n",
    "    sql=sql_statements.CREATE_STATIONS_TABLE_SQL,\n",
    ")\n",
    "\n",
    "copy_stations_task = PythonOperator(\n",
    "    task_id='load_stations_from_s3_to_redshift',\n",
    "    dag=dag,\n",
    "    python_callable=load_station_data_to_redshift,\n",
    ")\n",
    "\n",
    "create_trips_table >> copy_trips_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exercises/sql_statements.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exercises/sql_statements.py\n",
    "\n",
    "CREATE_TRIPS_TABLE_SQL = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS trips (\n",
    "trip_id INTEGER NOT NULL,\n",
    "start_time TIMESTAMP NOT NULL,\n",
    "end_time TIMESTAMP NOT NULL,\n",
    "bikeid INTEGER NOT NULL,\n",
    "tripduration DECIMAL(16,2) NOT NULL,\n",
    "from_station_id INTEGER NOT NULL,\n",
    "from_station_name VARCHAR(100) NOT NULL,\n",
    "to_station_id INTEGER NOT NULL,\n",
    "to_station_name VARCHAR(100) NOT NULL,\n",
    "usertype VARCHAR(20),\n",
    "gender VARCHAR(6),\n",
    "birthyear INTEGER,\n",
    "PRIMARY KEY(trip_id))\n",
    "DISTSTYLE ALL;\n",
    "\"\"\"\n",
    "\n",
    "CREATE_STATIONS_TABLE_SQL = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS stations (\n",
    "id INTEGER NOT NULL,\n",
    "name VARCHAR(250) NOT NULL,\n",
    "city VARCHAR(100) NOT NULL,\n",
    "latitude DECIMAL(9, 6) NOT NULL,\n",
    "longitude DECIMAL(9, 6) NOT NULL,\n",
    "dpcapacity INTEGER NOT NULL,\n",
    "online_date TIMESTAMP NOT NULL,\n",
    "PRIMARY KEY(id))\n",
    "DISTSTYLE ALL;\n",
    "\"\"\"\n",
    "\n",
    "COPY_SQL = \"\"\"\n",
    "COPY {}\n",
    "FROM '{}'\n",
    "ACCESS_KEY_ID '{{}}'\n",
    "SECRET_ACCESS_KEY '{{}}'\n",
    "IGNOREHEADER 1\n",
    "DELIMITER ','\n",
    "\"\"\"\n",
    "\n",
    "COPY_MONTHLY_TRIPS_SQL = COPY_SQL.format(\n",
    "    \"trips\",\n",
    "    \"s3://udacity-dend/data-pipelines/divvy/partitioned/{year}/{month}/divvy_trips.csv\"\n",
    ")\n",
    "\n",
    "COPY_ALL_TRIPS_SQL = COPY_SQL.format(\n",
    "    \"trips\",\n",
    "    \"s3://udacity-dend/data-pipelines/divvy/unpartitioned/divvy_trips_2018.csv\"\n",
    ")\n",
    "\n",
    "COPY_STATIONS_SQL = COPY_SQL.format(\n",
    "    \"stations\",\n",
    "    \"s3://udacity-dend/data-pipelines/divvy/unpartitioned/divvy_stations_2017.csv\"\n",
    ")\n",
    "\n",
    "LOCATION_TRAFFIC_SQL = \"\"\"\n",
    "BEGIN;\n",
    "DROP TABLE IF EXISTS station_traffic;\n",
    "CREATE TABLE station_traffic AS\n",
    "SELECT\n",
    "    DISTINCT(t.from_station_id) AS station_id,\n",
    "    t.from_station_name AS station_name,\n",
    "    num_departures,\n",
    "    num_arrivals\n",
    "FROM trips t\n",
    "JOIN (\n",
    "    SELECT\n",
    "        from_station_id,\n",
    "        COUNT(from_station_id) AS num_departures\n",
    "    FROM trips\n",
    "    GROUP BY from_station_id\n",
    ") AS fs ON t.from_station_id = fs.from_station_id\n",
    "JOIN (\n",
    "    SELECT\n",
    "        to_station_id,\n",
    "        COUNT(to_station_id) AS num_arrivals\n",
    "    FROM trips\n",
    "    GROUP BY to_station_id\n",
    ") AS ts ON t.from_station_id = ts.to_station_id\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp exercises/sql_statements.py $AIRFLOW_HOME/dags\n",
    "!cp exercises/lesson2_exercise1.py $AIRFLOW_HOME/dags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the Airflow UI and run this DAG once.<br>\n",
    "Next, configure the task ordering for stations data to have the create run before\n",
    "the copy. <br>\n",
    "Then, run this DAG once more and inspect the run history. to see the\n",
    "differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exercises/lesson2_exercise1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exercises/lesson2_exercise1.py\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.contrib.hooks.aws_hook import AwsHook\n",
    "from airflow.hooks.postgres_hook import PostgresHook\n",
    "from airflow.operators.postgres_operator import PostgresOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "import sql_statements\n",
    "\n",
    "def load_trip_data_to_redshift(*args, **kwargs):\n",
    "    aws_hook = AwsHook(\"aws_credentials\")\n",
    "    credentials = aws_hook.get_credentials()\n",
    "    redshift_hook = PostgresHook(\"redshift\")\n",
    "    sql_stmt = sql_statements.COPY_ALL_TRIPS_SQL.format(\n",
    "        credentials.access_key,\n",
    "        credentials.secret_key,\n",
    "    )\n",
    "    redshift_hook.run(sql_stmt)\n",
    "\n",
    "\n",
    "def load_station_data_to_redshift(*args, **kwargs):\n",
    "    aws_hook = AwsHook(\"aws_credentials\")\n",
    "    credentials = aws_hook.get_credentials()\n",
    "    redshift_hook = PostgresHook(\"redshift\")\n",
    "    sql_stmt = sql_statements.COPY_STATIONS_SQL.format(\n",
    "        credentials.access_key,\n",
    "        credentials.secret_key,\n",
    "    )\n",
    "    redshift_hook.run(sql_stmt)\n",
    "\n",
    "\n",
    "dag = DAG(\n",
    "    'lesson2.exercise1',\n",
    "    start_date=datetime.datetime.now() - datetime.timedelta(days=1)\n",
    ")\n",
    "\n",
    "create_trips_table = PostgresOperator(\n",
    "    task_id=\"create_trips_table\",\n",
    "    dag=dag,\n",
    "    postgres_conn_id=\"redshift\",\n",
    "    sql=sql_statements.CREATE_TRIPS_TABLE_SQL\n",
    ")\n",
    "\n",
    "copy_trips_task = PythonOperator(\n",
    "    task_id='load_trips_from_s3_to_redshift',\n",
    "    dag=dag,\n",
    "    python_callable=load_trip_data_to_redshift,\n",
    ")\n",
    "\n",
    "create_stations_table = PostgresOperator(\n",
    "    task_id=\"create_stations_table\",\n",
    "    dag=dag,\n",
    "    postgres_conn_id=\"redshift\",\n",
    "    sql=sql_statements.CREATE_STATIONS_TABLE_SQL,\n",
    ")\n",
    "\n",
    "copy_stations_task = PythonOperator(\n",
    "    task_id='load_stations_from_s3_to_redshift',\n",
    "    dag=dag,\n",
    "    python_callable=load_station_data_to_redshift,\n",
    ")\n",
    "\n",
    "create_trips_table >> copy_trips_task\n",
    "\n",
    "create_stations_table >> copy_stations_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp exercises/lesson2_exercise1.py $AIRFLOW_HOME/dags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline Schedules\n",
    "\n",
    "### Schedules\n",
    "Pipelines are often driven by schedules which determine what data should be analyzed and when.\n",
    "\n",
    "### Why Schedules\n",
    "* Pipeline schedules can reduce the amount of data that needs to be processed in a given run. It helps scope the job to only run the data for the time period since the data pipeline last ran. In a naive analysis, with no scope, we would analyze all of the data at all times.\n",
    "* Using schedules to select only data relevant to the time period of the given pipeline execution can help improve the quality and accuracy of the analyses performed by our pipeline.\n",
    "Running pipelines on a schedule will decrease the time it takes the pipeline to run.\n",
    "* An analysis of larger scope can leverage already-completed work. For. e.g., if the aggregates for all months prior to now have already been done by a scheduled job, then we only need to perform the aggregation for the current month and add it to the existing totals.\n",
    "\n",
    "### Selecting the time period\n",
    "Determining the appropriate time period for a schedule is based on a number of factors which you need to consider as the pipeline designer.\n",
    "\n",
    "1. **What is the size of data, on average, for a time period?** If an entire years worth of data is only a few kb or mb, then perhaps its fine to load the entire dataset. If an hours worth of data is hundreds of mb or even in the gbs then likely you will need to schedule your pipeline more frequently.\n",
    "\n",
    "2. **How frequently is data arriving, and how often does the analysis need to be performed?** If our bikeshare company needs trip data every hour, that will be a driving factor in determining the schedule. Alternatively, if we have to load hundreds of thousands of tiny records, even if they don't add up to much in terms of mb or gb, the file access alone will slow down our analysis and we’ll likely want to run it more often.\n",
    "\n",
    "3. **What's the frequency on related datasets?** A good rule of thumb is that the frequency of a pipeline’s schedule should be determined by the dataset in our pipeline which requires the most frequent analysis. This isn’t universally the case, but it's a good starting assumption. For example, if our trips data is updating every hour, but our bikeshare station table only updates once a quarter, we’ll probably want to run our trip analysis every hour, and not once a quarter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedules in Airflow\n",
    "### Start Date\n",
    "Airflow will begin running pipelines on the start date selected. Whenever the start date of a DAG is in the past, and the time difference between the start date and now includes more than one schedule intervals, Airflow will automatically schedule and execute a DAG run to satisfy each one of those intervals. This feature is useful in almost all enterprise settings, where companies have established years of data that may need to be retroactively analyzed.\n",
    "\n",
    "### End Date\n",
    "Airflow pipelines can also have end dates. You can use an end_date with your pipeline to let Airflow know when to stop running the pipeline. End_dates can also be useful when you want to perform an overhaul or redesign of an existing pipeline. Update the old pipeline with an end_date and then have the new pipeline start on the end date of the old pipeline.\n",
    "\n",
    "### QUESTION 1 OF 5\n",
    "How are schedules used by data pipelines?\n",
    "\n",
    "- [x] Determine what data should be analyzed and when\n",
    "- [ ] Determine when to interact with data sources\n",
    "- [ ] Determine when to run particular tasks\n",
    "- [ ] Determine when to email observers\n",
    "\n",
    "### QUESTION 2 OF 5\n",
    "Which of the following are used by Airflow to determine schedules?\n",
    "\n",
    "- [x] start_date\n",
    "- [ ] interval_date\n",
    "- [x] end_date\n",
    "- [x] schedule_interval\n",
    "\n",
    "### QUESTION 3 OF 5\n",
    "True or False: End date is required by Airflow Schedules.\n",
    "- [ ] True\n",
    "- [x] False\n",
    "\n",
    "### QUESTION 4 OF 5\n",
    "True or False: Start date is required by Airflow Schedules.\n",
    "- [x] True\n",
    "- [x] False\n",
    "\n",
    "### QUESTION 5 OF 5\n",
    "True or False: Schedule interval is required by Airflow Schedules.\n",
    "- [ ] True\n",
    "- [x] False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating DAGs\n",
    "\n",
    "### Common Questions\n",
    "**Wouldn't creating a new DAG for every feature change become cumbersome because feature changes or bugs happen all the time?**\n",
    "\n",
    "Yes, it really just comes down to the kind of analysis we need to perform as a team. So the question is : \"What is the change that I'm making? Is it the fundamental change to the DAG that actually changes the meaning of what we're trying to do? or are we adding at something that can be easily reflected in the existing data pipeline that we've already designed? So if we go back to Airflow, if we were to add a marketing email send at the end of the DAG . The problem is that if we didn't update or rerun the DAG to reflect that then they can cause issues for other people who might come by later and that this DAG has been running or whatever the time period is, we sent all those emails. But in fact, we haven't right? So what we can do here is to design a new DAG. This is the simplest solution. The other option is that you can actually clear the history of DAG runs. There is another important consideration when we are going to do this. Airflow has no concept of the data stores that you have outside of Airflow. So if rerunning a task is destructive or rerunning a task triggers some kind of downstream action, this can be really dangerous for the internal usage of Airflow and for other data consumers. So it is possible to rerun a DAG or clear the history of a DAG, but there's gonna be repercussions of doing that. So we need to know the repercussions of doing that. So that's why we need to know the pros and cons of the idea that we need a new DAG or do we need to go back and wipe history and try again,\n",
    "For more watch this [tutorial](https://www.youtube.com/watch?v=zuLPBN9SZRc) by Udacity on youtube.\n",
    "\n",
    "**Is time the only type of partition? Can you partition other types, such as events or values?**\n",
    "Yes there are like logical partitioning and also data size partitioning. We can also partition around events and other type of things but yes time is the common one but it's not the only kind of partitioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Schedules and Backfills in Airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exercises/lesson2_exercise2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exercises/lesson2_exercise2.py\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.contrib.hooks.aws_hook import AwsHook\n",
    "from airflow.hooks.postgres_hook import PostgresHook\n",
    "from airflow.operators.postgres_operator import PostgresOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "import sql_statements\n",
    "\n",
    "\n",
    "def load_trip_data_to_redshift(*args, **kwargs):\n",
    "    aws_hook = AwsHook(\"aws_credentials\")\n",
    "    credentials = aws_hook.get_credentials()\n",
    "    redshift_hook = PostgresHook(\"redshift\")\n",
    "    sql_stmt = sql_statements.COPY_ALL_TRIPS_SQL.format(\n",
    "        credentials.access_key,\n",
    "        credentials.secret_key,\n",
    "    )\n",
    "    redshift_hook.run(sql_stmt)\n",
    "\n",
    "\n",
    "def load_station_data_to_redshift(*args, **kwargs):\n",
    "    aws_hook = AwsHook(\"aws_credentials\")\n",
    "    credentials = aws_hook.get_credentials()\n",
    "    redshift_hook = PostgresHook(\"redshift\")\n",
    "    sql_stmt = sql_statements.COPY_STATIONS_SQL.format(\n",
    "        credentials.access_key,\n",
    "        credentials.secret_key,\n",
    "    )\n",
    "    redshift_hook.run(sql_stmt)\n",
    "\n",
    "\n",
    "dag = DAG(\n",
    "    'lesson2.exercise2',\n",
    "    start_date=datetime.datetime(2018, 1, 1, 0, 0, 0, 0),\n",
    "    # TODO: Set the end date to February first\n",
    "    end_date=datetime.datetime(2018, 2, 1, 0, 0, 0, 0),\n",
    "    # TODO: Set the schedule to be monthly\n",
    "    schedule_interval='@monthly',\n",
    "    # TODO: set the number of max active runs to 1\n",
    "    max_active_runs=1\n",
    ")\n",
    "\n",
    "create_trips_table = PostgresOperator(\n",
    "    task_id=\"create_trips_table\",\n",
    "    dag=dag,\n",
    "    postgres_conn_id=\"redshift\",\n",
    "    sql=sql_statements.CREATE_TRIPS_TABLE_SQL\n",
    ")\n",
    "\n",
    "copy_trips_task = PythonOperator(\n",
    "    task_id='load_trips_from_s3_to_redshift',\n",
    "    dag=dag,\n",
    "    python_callable=load_trip_data_to_redshift,\n",
    "    provide_context=True,\n",
    ")\n",
    "\n",
    "create_stations_table = PostgresOperator(\n",
    "    task_id=\"create_stations_table\",\n",
    "    dag=dag,\n",
    "    postgres_conn_id=\"redshift\",\n",
    "    sql=sql_statements.CREATE_STATIONS_TABLE_SQL,\n",
    ")\n",
    "\n",
    "copy_stations_task = PythonOperator(\n",
    "    task_id='load_stations_from_s3_to_redshift',\n",
    "    dag=dag,\n",
    "    python_callable=load_station_data_to_redshift,\n",
    ")\n",
    "\n",
    "create_trips_table >> copy_trips_task\n",
    "create_stations_table >> copy_stations_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp exercises/lesson2_exercise2.py $AIRFLOW_HOME/dags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Partitioning\n",
    "### Schedule partitioning\n",
    "Not only are schedules great for reducing the amount of data our pipelines have to process, but they also help us guarantee that we can meet timing guarantees that our data consumers may need.\n",
    "\n",
    "### Logical partitioning\n",
    "Conceptually related data can be partitioned into discrete segments and processed separately. This process of separating data based on its conceptual relationship is called logical partitioning. With logical partitioning, unrelated things belong in separate steps. Consider your dependencies and separate processing around those boundaries.\n",
    "\n",
    "Also worth mentioning, the data location is another form of logical partitioning. For example, if our data is stored in a key-value store like Amazon's S3 in a format such as: `s3://<bucket>/<year>/<month>/<day>` we could say that our date is logically partitioned by time.\n",
    "\n",
    "### Size Partitioning\n",
    "Size partitioning separates data for processing based on desired or required storage limits. This essentially sets the amount of data included in a data pipeline run. Size partitioning is critical to understand when working with large datasets, especially with Airflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Data Partitioning?\n",
    "Pipelines designed to work with partitioned data fail more gracefully. Smaller datasets, smaller time periods, and related concepts are easier to debug than big datasets, large time periods, and unrelated concepts. Partitioning makes debugging and rerunning failed tasks much simpler. It also enables easier redos of work, reducing cost and time.\n",
    "\n",
    "Another great thing about Airflow is that if your data is partitioned appropriately, your tasks will naturally have fewer dependencies on each other. Because of this, Airflow will be able to parallelize execution of your DAGs to produce your results even faster.\n",
    "\n",
    "### QUESTION 1 OF 4\n",
    "What are four common types of data partitioning?\n",
    "\n",
    "- [x] Location\n",
    "- [x] Logical\n",
    "- [x] Size\n",
    "- [ ] Cloud\n",
    "- [x] Time\n",
    "\n",
    "### QUESTION 2 OF 4\n",
    "Logical Partitioning is the process of...\n",
    "- [ ] Processing data based on its location in a datastore\n",
    "- [ ] Separating data for processing based on desired or required storage limits\n",
    "- [ ] Processing data based on a schedule or when it was created\n",
    "- [x] Breaking conceptually related data into discrete groups for processing\n",
    "\n",
    "### QUESTION 3 OF 4\n",
    "Time Partitioning is the process of...\n",
    "- [ ] Processing data based on its location in a datastore\n",
    "- [ ] Separating data for processing based on desired or required storage limits\n",
    "- [x] Processing data based on a schedule or when it was created\n",
    "- [ ] Breaking conceptually related data into discrete groups for processing\n",
    "\n",
    "### QUESTION 4 OF 4\n",
    "Size Partitioning is the process of\n",
    "- [ ] Processing data based on its location in a datastore\n",
    "- [x] Separating data for processing based on desired or required storage limits\n",
    "- [ ] Processing data based on a schedule or when it was created\n",
    "- [ ] Breaking conceptually related data into discrete groups for processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "**Instructions**:\n",
    "1. Modify the bikeshare DAG to load data month by month, instead of loading it all at once, every time. \n",
    "2. Use time partitioning to parallelize the execution of the DAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exercises/lesson2_exercise3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exercises/lesson2_exercise3.py\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.contrib.hooks.aws_hook import AwsHook\n",
    "from airflow.hooks.postgres_hook import PostgresHook\n",
    "from airflow.operators.postgres_operator import PostgresOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "import sql_statements\n",
    "\n",
    "def load_trip_data_to_redshift(*args, **kwargs):\n",
    "    aws_hook = AwsHook(\"aws_credentials\")\n",
    "    credentials = aws_hook.get_credentials()\n",
    "    redshift_hook = PostgresHook('redshift')\n",
    "    execution_date = kwargs['execution_date']\n",
    "    sql_stmt = sql_statements.COPY_MONTHLY_TRIPS_SQL.format(\n",
    "        credentials.access_key,\n",
    "        credentials.secret_key,\n",
    "        year=execution_date.year,\n",
    "        month=execution_date.month\n",
    "    )\n",
    "    redshift_hook.run(sql_stmt)\n",
    "    \n",
    "def load_station_data_to_redshift(*args, **kwargs):\n",
    "    aws_hook = AwsHook(\"aws_credentials\")\n",
    "    credentials = aws_hook.get_credentials()\n",
    "    redshift_hook = PostgresHook('redshift')\n",
    "    sql_smt = sql_statements.COPY_STATIONS_SQL.format(\n",
    "        credentials.access_key,\n",
    "        credentials.secret_key\n",
    "    )\n",
    "    redshift_hook.run(sql_smt)\n",
    "    \n",
    "dag = DAG(\n",
    "        'lesson2.exercise3',\n",
    "        start_date=datetime.datetime(2018, 1, 1, 0, 0, 0, 0),\n",
    "        end_date=datetime.datetime(2019, 1, 1, 0, 0, 0, 0),\n",
    "        schedule_interval='@monthly',\n",
    "        max_active_runs=1\n",
    ")\n",
    "\n",
    "create_trips_table = PostgresOperator(\n",
    "    task_id='create_trips_table',\n",
    "    dag=dag,\n",
    "    postgres_conn_id='redshift',\n",
    "    sql=sql_statements.CREATE_TRIPS_TABLE_SQL\n",
    ")\n",
    "\n",
    "copy_trips_task = PythonOperator(\n",
    "    task_id='load_trips_from_s3_to_redshift',\n",
    "    dag=dag,\n",
    "    python_callable=load_trip_data_to_redshift,\n",
    "    provide_context=True\n",
    ")\n",
    "\n",
    "create_stations_table = PostgresOperator(\n",
    "    task_id=\"create_stations_table\",\n",
    "    dag=dag,\n",
    "    postgres_conn_id=\"redshift\",\n",
    "    sql=sql_statements.CREATE_STATIONS_TABLE_SQL,\n",
    ")\n",
    "\n",
    "copy_stations_task = PythonOperator(\n",
    "    task_id='load_stations_from_s3_to_redshift',\n",
    "    dag=dag,\n",
    "    python_callable=load_station_data_to_redshift,\n",
    ")\n",
    "\n",
    "create_trips_table >> copy_trips_task\n",
    "create_stations_table >> copy_stations_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp exercises/lesson2_exercise3.py $AIRFLOW_HOME/dags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality\n",
    "**Data quality** is the measure of how well a dataset satisfies its intended use. When we, talk about the intended use of data, we're typically referring to how our downstream data consumers are going to utilize this data.\n",
    "\n",
    "Adherence to a set of **requirement** is a good starting point for measuring data quality. Requirements should be defined by you and your data consumers before you start creating your data pipeline.\n",
    "\n",
    "### Examples of Data Quality Requirements\n",
    "* Data must be a certain size\n",
    "* Data must be accurate to some margin of error\n",
    "* Data must arrive within a given timeframe from the start of execution\n",
    "* Pipelines must run on a particular schedule\n",
    "* Data must not contain any sensitive information\n",
    "\n",
    "### QUESTION 1 OF 4\n",
    "Which of the following are true about data quality requirements?\n",
    "- [ ] Requirements tell engineers exactly how to build a data pipeline\n",
    "- [x] Requirements are how we can set and measure quality\n",
    "- [x] Requirements allow both engineering and non-engineering roles to agree on the high-level method for preparing the output.\n",
    "- [x] Requirements tell engineers what the output of their data pipelines should be\n",
    "\n",
    "### QUESTION 2 OF 4\n",
    "How would you set a requirement for ensuring that data arrives within a certain timeframe of a DAG starting?\n",
    "\n",
    "- [x] Use a Service Level Agreement\n",
    "- [ ] Use a schedule\n",
    "- [ ] Use a start date\n",
    "- [ ] Use an end data\n",
    "\n",
    "### QUESTION 3 OF 4\n",
    "What kind of requirement would be violated if no data was produced by a DAG?\n",
    "- [ ] Data must be accurate to some margin of error\n",
    "- [ ] Data must arrive within given timeframe from the start of execution\n",
    "- [ ] Pipelines must run on  a particular Schedule\n",
    "- [ ] Data must not contain any sensitive information\n",
    "- [x] Data must be of a certain size\n",
    "\n",
    "### QUESTION 4 OF 4\n",
    "What kind of requirement would be violated if data arrived after it was needed?\n",
    "- [ ] Data must be accurate to some margin of error\n",
    "- [x] Data must arrive within given timeframe from the start of execution\n",
    "- [ ] Pipelines must run on  a particular Schedule\n",
    "- [ ] Data must not contain any sensitive information\n",
    "- [ ] Data must be of a certain size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Data Quality\n",
    "**Instructions**:\n",
    "1. Set an SLA on our bikeshare traffic calculation operator\n",
    "2. Add data verification step after the load step from s3 to redshift\n",
    "3. Add data verification step after we calculate our output table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exercises/lesson2_exercise4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exercises/lesson2_exercise4.py\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.contrib.hooks.aws_hook import AwsHook\n",
    "from airflow.hooks.postgres_hook import PostgresHook\n",
    "from airflow.operators.postgres_operator import PostgresOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "import sql_statements\n",
    "\n",
    "\n",
    "def load_trip_data_to_redshift(*args, **kwargs):\n",
    "    aws_hook = AwsHook(\"aws_credentials\")\n",
    "    credentials = aws_hook.get_credentials()\n",
    "    redshift_hook = PostgresHook(\"redshift\")\n",
    "    execution_date = kwargs[\"execution_date\"]\n",
    "    sql_stmt = sql_statements.COPY_MONTHLY_TRIPS_SQL.format(\n",
    "        credentials.access_key,\n",
    "        credentials.secret_key,\n",
    "        year=execution_date.year,\n",
    "        month=execution_date.month\n",
    "    )\n",
    "    redshift_hook.run(sql_stmt)\n",
    "\n",
    "\n",
    "def load_station_data_to_redshift(*args, **kwargs):\n",
    "    aws_hook = AwsHook(\"aws_credentials\")\n",
    "    credentials = aws_hook.get_credentials()\n",
    "    redshift_hook = PostgresHook(\"redshift\")\n",
    "    sql_stmt = sql_statements.COPY_STATIONS_SQL.format(\n",
    "        credentials.access_key,\n",
    "        credentials.secret_key,\n",
    "    )\n",
    "    redshift_hook.run(sql_stmt)\n",
    "\n",
    "\n",
    "def check_greater_than_zero(*args, **kwargs):\n",
    "    table = kwargs[\"params\"][\"table\"]\n",
    "    redshift_hook = PostgresHook(\"redshift\")\n",
    "    records = redshift_hook.get_records(f\"SELECT COUNT(*) FROM {table}\")\n",
    "    if len(records) < 1 or len(records[0]) < 1:\n",
    "        logging.error(f\"No records present in destination {table}\")\n",
    "        raise ValueError(f\"Data quality check failed. {table} returned no results\")\n",
    "    num_records = records[0][0]\n",
    "        \n",
    "    logging.info(f\"Data quality on table {table} check passed with {records[0][0]} records\")\n",
    "\n",
    "\n",
    "dag = DAG(\n",
    "    'lesson2.exercise4',\n",
    "    start_date=datetime.datetime(2018, 1, 1, 0, 0, 0, 0),\n",
    "    end_date=datetime.datetime(2019, 1, 1, 0, 0, 0, 0),\n",
    "    schedule_interval='@monthly',\n",
    "    max_active_runs=1\n",
    ")\n",
    "\n",
    "create_trips_table = PostgresOperator(\n",
    "    task_id=\"create_trips_table\",\n",
    "    dag=dag,\n",
    "    postgres_conn_id=\"redshift\",\n",
    "    sql=sql_statements.CREATE_TRIPS_TABLE_SQL\n",
    ")\n",
    "\n",
    "copy_trips_task = PythonOperator(\n",
    "    task_id='load_trips_from_s3_to_redshift',\n",
    "    dag=dag,\n",
    "    python_callable=load_trip_data_to_redshift,\n",
    "    provide_context=True,\n",
    ")\n",
    "\n",
    "check_trips = PythonOperator(\n",
    "    task_id='check_trips_data',\n",
    "    dag=dag,\n",
    "    python_callable=check_greater_than_zero,\n",
    "    provide_context=True,\n",
    "    params={\n",
    "        'table': 'trips',\n",
    "    }\n",
    ")\n",
    "\n",
    "create_stations_table = PostgresOperator(\n",
    "    task_id=\"create_stations_table\",\n",
    "    dag=dag,\n",
    "    postgres_conn_id=\"redshift\",\n",
    "    sql=sql_statements.CREATE_STATIONS_TABLE_SQL,\n",
    ")\n",
    "\n",
    "copy_stations_task = PythonOperator(\n",
    "    task_id='load_stations_from_s3_to_redshift',\n",
    "    dag=dag,\n",
    "    python_callable=load_station_data_to_redshift,\n",
    ")\n",
    "\n",
    "check_stations = PythonOperator(\n",
    "    task_id='check_stations_data',\n",
    "    dag=dag,\n",
    "    python_callable=check_greater_than_zero,\n",
    "    provide_context=True,\n",
    "    params={\n",
    "        'table': 'stations',\n",
    "    }\n",
    ")\n",
    "\n",
    "create_trips_table >> copy_trips_task >> check_trips\n",
    "create_stations_table >> copy_stations_task >> check_stations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp exercises/lesson2_exercise4.py $AIRFLOW_HOME/dags"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
