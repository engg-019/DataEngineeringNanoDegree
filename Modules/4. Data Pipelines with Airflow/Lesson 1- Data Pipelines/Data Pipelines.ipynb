{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Data Pipeline?\n",
    "<u>Definition</u>: A series of steps in which data is processed. \n",
    "\n",
    "Depending on the data requirement for each step, some steps may occur in parallel. Data pipelines also typically occur on a schedule which can be once in hour, once a day, every minute or once a year. It depends on how frequently the data is delivered and how often the data consumer need new insights. Schedules are the most common mechanisms of triggering an execution of a data pipeline, external triggers and events can also be used to execute data pipelines. \n",
    "### Real World Data Pipelines\n",
    "Following are some examples of real world data pipelines\n",
    "* Automated  marketing emails\n",
    "* Real-time pricing in rideshare apps\n",
    "* Targeted advertising based on browsing history\n",
    "\n",
    "### Example\n",
    "Pretend we work at a bikeshare company and want to email customers who didn't complete a purchase.\n",
    "\n",
    "A data pipeline to accomplish this task would like:\n",
    "1. Load application event data from a source such as S3 or Kafka\n",
    "2. Load the data into an analytic warehouse such as Redshift\n",
    "3. Perform data transformations that identify high-traffice bike docs so the business can determine where to build additional locations.\n",
    "\n",
    "### QUIZ QUESTION\n",
    "What is a data pipeline?\n",
    "- [ ] A visual way of displaying data to business users\n",
    "- [ ] An algorithm that classifies data.\n",
    "- [x] A series of steps in which data is processed.\n",
    "- [ ] A type of database.\n",
    "\n",
    "### Extract Transform Load (ETL) and Extract Load Transform (ELT):\n",
    "\"ETL is normally a continuous, ongoing process with a well-defined workflow. ETL first extracts data from homogeneous or heterogeneous data sources. Then, data is cleansed, enriched, transformed, and stored either back in the lake or in a data warehouse.\n",
    "\n",
    "\"ELT (Extract, Load, Transform) is a variant of ETL wherein the extracted data is first loaded into the target system. Transformations are performed after the data is loaded into the data warehouse. ELT typically works well when the target system is powerful enough to handle transformations. Analytical databases like Amazon Redshift and Google BigQ.\"\n",
    "Source: [Xplenty.com](https://www.xplenty.com/blog/etl-vs-elt/)\n",
    "\n",
    "This [Quora post](https://www.quora.com/What-is-the-difference-between-the-ETL-and-ELT) is also helpful if you'd like to read more.\n",
    "\n",
    "### What is S3?\n",
    "\"Amazon S3 has a simple web services interface that you can use to store and retrieve any amount of data, at any time, from anywhere on the web. It gives any developer access to the same highly scalable, reliable, fast, inexpensive data storage infrastructure that Amazon uses to run its own global network of web sites.\"\n",
    "Source: [Amazon Web Services Documentation](https://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html).\n",
    "\n",
    "If you want to learn more, start [here](https://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html).\n",
    "\n",
    "### What is Kafka?\n",
    "\"Apache Kafka is an **open-source stream-processing software platform** developed by Linkedin and donated to the Apache Software Foundation, written in Scala and Java. The project aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds. Its storage layer is essentially a massively scalable pub/sub message queue designed as a distributed transaction log, making it highly valuable for enterprise infrastructures to process streaming data.\"\n",
    "Source: Wikipedia.\n",
    "\n",
    "If you want to learn more, start [here](https://kafka.apache.org/intro).\n",
    "\n",
    "### What is RedShift?\n",
    "\"Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. You can start with just a few hundred gigabytes of data and scale to a petabyte or more... The first step to create a data warehouse is to launch a set of nodes, called an Amazon Redshift cluster. After you provision your cluster, you can upload your data set and then perform data analysis queries. Regardless of the size of the data set, Amazon Redshift offers fast query performance using the same SQL-based tools and business intelligence applications that you use today.\n",
    "\n",
    "If you want to learn more, start [here](https://docs.aws.amazon.com/redshift/latest/mgmt/welcome.html).\n",
    "\n",
    "So in other words, S3 is an example of the final data store where data might be loaded (e.g. ETL). While Redshift is an example of a data warehouse product, provided specifically by Amazon.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Validation\n",
    "Data Validation is the process of ensuring that data is present, correct & meaningful. Ensuring the quality of your data through automated validation checks is a critical step in building data pipelines at any organization.\n",
    "\n",
    "Data validation can be done manually by quality assurance, data engineers or even data customers. It's much preferable to perform data validation in an automated fashion. Validation can and should become part of your pipeline definitions. \n",
    "\n",
    "### What could go wrong?\n",
    "In our previous bikeshare example we loaded event data, analyzed it, and ranked out busiest locations to determine where to build additional capacity.\n",
    "\n",
    "What would happen if the data was wrong?\n",
    "What would happen if our system miscalculate the location ranking? \n",
    "What if no data was produced at all? \n",
    "\n",
    "When we do a mistake in our data pipeline it can lead to some serious problems for our businesses, for our customers and for people who depend on that kind of data. \n",
    "So it's really important that we perform data validation to ensure that the data we're creating is accurate and correct.\n",
    "\n",
    "### Data Validation in Action\n",
    "In our bikesharing example, we could have added the following validation steps:\n",
    "\n",
    "After loading from S3 ro redshift:\n",
    "* Validate the number of rows in Redshift match the number of records in S3\n",
    "\n",
    "Once location business analysis is complete:\n",
    "* Validate that all locations have a daily visit greater than 0\n",
    "* Validate the number of locations in our output table match the number of tables in the input table.\n",
    "\n",
    "### Why is it important?\n",
    "* Data pipelines provide a set of logical guidelines and a common set of terminology.\n",
    "* The conceptual framework of data pipelines will help you better organize and execute everyday data engineering tasks.\n",
    "\n",
    "### QUIZ QUESTION\n",
    "Which of the following are examples of data validation?\n",
    "- [x] Ensuring that the number of rows in Redshift match the number of records in S3\n",
    "- [x] Ensuring that the number of rows in a table are greater than zero\n",
    "- [ ] Ensuring that the output table matches the needs of the data consumer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAGs and Data Pipelines\n",
    "\n",
    "### Definitions\n",
    "* **Directed Acyclic Graphs (DAGs)**: DAGs are a special subset of graphs in which the edges between nodes have a specific direction, and no cycles exist. When we say “no cycles exist” what we mean is the nodes cant create a path back to themselves.\n",
    "* **Nodes**: A step in the data pipeline process.\n",
    "* **Edges**: The dependencies or relationships other between nodes.\n",
    "\n",
    "<img src=\"images/dags.png\">\n",
    "\n",
    "### Data Pipelines as DAGs\n",
    "In ETL, each step of the process typically depends on the last.\n",
    "\n",
    "Each step is a node and dependencies on prior steps are directed edges.\n",
    "### Common Questions\n",
    "#### Are there real world cases where a data pipeline is not DAG?\n",
    "\n",
    "It is possible to model a data pipeline that is not a DAG, meaning that it contains a cycle within the process. However, the vast majority of use cases for data pipelines can be described as a directed acyclic graph (DAG). This makes the code more understandable and maintainable.\n",
    "\n",
    "#### Can we have two different pipelines for the same data and can we merge them back together?\n",
    "\n",
    "Yes. It's not uncommon for a data pipeline to take the same dataset, perform two different processes to analyze the it, then merge the results of those two processes back together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bikeshare DAG\n",
    "\n",
    "* First we're going to extract the data from S3 and load the data from S3 into Redshift\n",
    "* Perform analysis in Redshift using SQL.\n",
    "* Deliver data to some destination server.\n",
    "\n",
    "<img src=\"images/bikeshare_dag_1.png\">\n",
    "\n",
    "What happens if we need to add another data source?\n",
    "\n",
    "* Let's say that we want to integrate the data from the city's API. This steps needs to be completed before we perform Redshift analysis as shown below:\n",
    "\n",
    "<img src=\"images/bikeshare_dag_2.png\">\n",
    "\n",
    "### QUESTION 1 OF 3\n",
    "What are the two components of ALL graphs?\n",
    "- [ ] Cycle\n",
    "- [x] Node\n",
    "- [x] Edge\n",
    "- [ ] Direction\n",
    "\n",
    "### QUESTION 2 OF 3\n",
    "Which of the following are features which define a Directed Acyclic Graph?\n",
    "- [ ] Has Cycles\n",
    "- [x] No Cycles\n",
    "- [x] Nodes may have more than one edge that connects to them\n",
    "- [x] Edges between nodes imply a directed relationship\n",
    "\n",
    "### QUESTION 3 OF 3\n",
    "Which graph(s) shown below are directed acyclic graphs (DAG)?\n",
    "\n",
    "<img src=\"images/dag-quiz.png\">\n",
    "\n",
    "- [ ] Graph 1\n",
    "- [x] Graph 2\n",
    "- [ ] Graph 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Apache Airflow\n",
    "\n",
    "### Apache Airflow\n",
    "* \"Airflow is a platform to programmatically author, schedule and monitor workflows. Use airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed. When workflows are defined as code, they become more maintainable, versionable, testable, and collaborative.\"\n",
    "\n",
    "* Airflow allows users to write DAGs in Python that run on a schedule and/or from an external trigger.\n",
    "\n",
    "* Airflow is simple to maintain and can run data analysis itself or trigger external tools (Redshift, Spark, Presto, Hadoop, etc) during execution.\n",
    "\n",
    "Airflow also provides a web-based UI for users to visualize and interact with their data pipelines.\n",
    "\n",
    "If you'd like to learn more, start [here](https://airflow.apache.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Exercises Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('exercises'):\n",
    "    os.makedirs('exercises')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow Installation\n",
    "\n",
    "Aiflow installation is straightforward.\n",
    "\n",
    "```bash\n",
    "# airflow needs a home, ~/airflow is the default,\n",
    "# but you can lay foundation somewhere else if you prefer\n",
    "# (optional)\n",
    "export AIRFLOW_HOME=~/airflow\n",
    "\n",
    "# install from pypi using pip\n",
    "pip install apache-airflow\n",
    "\n",
    "# initialize the database\n",
    "airflow initdb\n",
    "\n",
    "# start the web server, default port is 8080\n",
    "airflow webserver -p 8080\n",
    "\n",
    "# start the scheduler\n",
    "airflow scheduler\n",
    "```\n",
    "\n",
    "After installing the airflow create a folder named `dags` inside `AIRFLOW_HOME` folder and put all the your airflow application their"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Airflow DAGs\n",
    "**Instructions**:\n",
    "Define a function that uses the python logger to log a function. Then finish filling in the details of the DAG down below. Once you’ve done that, run \"/opt/airflow/start.sh\" command to start the web server. Once the Airflow web server is ready,  open the Airflow UI using the \"Access Airflow\" button. Turn your DAG “On”, and then Run your DAG. If you get stuck, you can take a look at the solution file or the video walkthrough on the next page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exercises/exercise1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exercises/exercise1.py\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "def hello_world():\n",
    "    logging.info(\"Hello World!\")\n",
    "\n",
    "\n",
    "dag = DAG(\n",
    "        'lesson1.exercise1',\n",
    "        start_date=datetime.datetime.now())\n",
    "\n",
    "greet_task = PythonOperator(\n",
    "   task_id=\"hello_world_task\",\n",
    "   python_callable=hello_world,\n",
    "   dag=dag\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp exercises/exercise1.py $AIRFLOW_HOME/dags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Airflow Works\n",
    "\n",
    "### Components of Airflow\n",
    "* **Scheduler** orchestrates the execution of jobs on a trigger or schedule. The Scheduler chooses how to prioritize the running and execution of tasks within the system. You can learn more about the Scheduler from the official [Apache Airflow documentation](https://airflow.apache.org/scheduler.html).\n",
    "* **Work Queue** is used by the scheduler in most Airflow installations to deliver tasks that need to be run to the **Workers**.\n",
    "* **Worker** processes execute the operations defined in each DAG. In most Airflow installations, workers pull from the **work queue** when it is ready to process a task. When the worker completes the execution of the task, it will attempt to process more work from the **work queue** until there is no further work remaining. When work in the queue arrives, the worker will begin to process it.\n",
    "* **Database** saves credentials, connections, history, and configuration. The database, often referred to as the metadata database, also stores the state of all tasks in the system. Airflow components interact with the database with the Python ORM, [SQLAlchemy](https://www.sqlalchemy.org/).\n",
    "* **Web Interface** provides a control dashboard for users and maintainers. Throughout this course you will see how the web interface allows users to perform tasks such as stopping and starting DAGs, retrying failed tasks, configuring credentials, The web interface is built using the [Flask web-development microframework](http://flask.pocoo.org/).\n",
    "\n",
    "<img src=\"images/airflow-diagram.png\">\n",
    "\n",
    "Airflow itself is not a data processing framework. In airflow we don't pass data in memory between steps in your DAG. Instead we use airflow to coordinate the data between other data storage and data processing tools. Database only stores the meta data. The workers in the airflow when they execute will work with Redshift, Spark and other systems. Also we'll not typically run heavy processing workloads on Airflow. Airflow is only limited to the processing power of a single machine. This is why airflow developers prefer airflow to trigger heavy processing steps in analytic warehouses like Redshift or dataframe works like spark.\n",
    "\n",
    "### Order of Operations For an Airflow DAG\n",
    "\n",
    "* The Airflow Scheduler starts DAGs based on time or external triggers.\n",
    "* Once a DAG is started, the Scheduler looks at the steps within the DAG and determines which steps can run by looking at their dependencies.\n",
    "* The Scheduler places runnable steps in the queue.\n",
    "* Workers pick up those tasks and run them.\n",
    "* Once the worker has finished running the step, the final status of the task is recorded and additional tasks are placed by the scheduler until all tasks are complete.\n",
    "* Once all tasks have been completed, the DAG is complete.\n",
    "\n",
    "<img src=\"images/how-airflow-works.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Airflow Runtime Architecture\n",
    "### QUESTION 1 OF 4\n",
    "What are the five components of Airflow’s architecture?\n",
    "- [x] Scheduler\n",
    "- [ ] Data Warehouse\n",
    "- [x] Workers\n",
    "- [x] UI/Web Server\n",
    "- [x] Queue\n",
    "- [ ] Streaming Server\n",
    "- [x] Database\n",
    "\n",
    "### QUESTION 2 OF 4\n",
    "What does the Airflow UI do?\n",
    "- [ ] Allow the user to construct data pipelines graphically in the UI\n",
    "- [x] Provides a control interface for users and maintainers\n",
    "- [ ] Allows the user to write queries against databases\n",
    "- [ ] Runs and records the outcome of individual pipeline tasks\n",
    "\n",
    "### QUESTION 3 OF 4\n",
    "What does the Airflow Scheduler do?\n",
    "- [ ] Runs and records the outcome of individual pipeline tasks\n",
    "- [ ] Provides a control interface for users and maintainers\n",
    "- [ ] Sends email on a scheduled basis\n",
    "- [x] Starts DAGs based on triggers or schedules and moves them towards completion\n",
    "\n",
    "### QUESTION 4 OF 4\n",
    "What do the Airflow Workers do?\n",
    "- [x] Runs and records the outcome of individual pipeline tasks\n",
    "- [ ] Provides a control interface for users and maintainers\n",
    "- [ ] Sends email on a scheduled basis\n",
    "- [ ] Starts DAGs based on triggers or schedules and moves them towards completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Data Pipeline\n",
    "### Creating a DAG\n",
    "Creating a DAG is easy. Give it a name, a description, a start date, and an interval.\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "\n",
    "divvy_dag = DAG(\n",
    "    'divvy',\n",
    "    description='Analyzes Divvy Bikeshare Data',\n",
    "    start_date=datetime(2019, 2, 4),\n",
    "    schedule_interval='@daily')\n",
    "\n",
    "```\n",
    "\n",
    "### Creating Operators to Perform Tasks\n",
    "**Operators** define the atomic steps of work that make up a DAG. Instantiated operators are referred to as **Tasks**.\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "def hello_world():\n",
    "    print(\"Hello World\")\n",
    "\n",
    "divvy_dag = DAG(...)\n",
    "task = PythonOperator(\n",
    "    task_id=’hello_world’,\n",
    "    python_callable=hello_world,\n",
    "    dag=divvy_dag)\n",
    "```\n",
    "\n",
    "### Schedules\n",
    "**Schedules** are optional, and may be defined with cron strings or Airflow Presets. Airflow provides the following presets:\n",
    "\n",
    "* `@once` - Run a DAG once and then never again\n",
    "* `@hourly` - Run the DAG every hour\n",
    "* `@daily` - Run the DAG every day\n",
    "* `@weekly` - Run the DAG every week\n",
    "* `@monthly` - Run the DAG every month\n",
    "* `@yearly`- Run the DAG every year\n",
    "* `None` - Only run the DAG when the user initiates it\n",
    "\n",
    "**Start Date:** If your start date is in the past, Airflow will run your DAG as many times as there are schedule intervals between that start date and the current date.\n",
    "\n",
    "**End Date:** Unless you specify an optional end date, Airflow will continue to run your DAGs until you disable or delete the DAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Run the Schedules\n",
    "**Instructions**: Complete the DAG so that it runs once a day. Once you’ve done that, open the Airflow UI. go to the Airflow UI and turn the last exercise off, then turn this exercise on. Wait a moment and refresh the UI to see Airflow automatically run your DAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exercises/exercise2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exercises/exercise2.py\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "\n",
    "def hello_world():\n",
    "    logging.info(\"Hello World\")\n",
    "\n",
    "dag = DAG(\n",
    "        \"lesson1.exercise2\",\n",
    "        start_date=datetime.datetime.now() - datetime.timedelta(days=2),\n",
    "        schedule_interval='@daily')\n",
    "\n",
    "task = PythonOperator(\n",
    "        task_id=\"hello_world_task\",\n",
    "        python_callable=hello_world,\n",
    "        dag=dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp exercises/exercise2.py $AIRFLOW_HOME/dags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operators and Tasks\n",
    "\n",
    "### Operators \n",
    "Operators define the atomic steps of work that make up a DAG. Airflow comes with many Operators that can perform common operations. Here are a handful of common ones:\n",
    "\n",
    "* `PythonOperator`\n",
    "* `PostgresOperator`\n",
    "* `RedshiftToS3Operator`\n",
    "* `S3ToRedshiftOperator`\n",
    "* `BashOperator`\n",
    "* `SimpleHttpOperator`\n",
    "* `Sensor`\n",
    "\n",
    "### Task Dependencies\n",
    "In Airflow DAGs:\n",
    "\n",
    "* Nodes = Tasks\n",
    "* Edges = Ordering and dependencies between tasks\n",
    "\n",
    "Task dependencies can be described programmatically in Airflow using `>>` and `<<`\n",
    "\n",
    "* a `>>` b means a comes before b\n",
    "* a `<<` b means a comes after b\n",
    "\n",
    "```python\n",
    "hello_world_task = PythonOperator(task_id=’hello_world’, ...)\n",
    "goodbye_world_task = PythonOperator(task_id=’goodbye_world’, ...)\n",
    "...\n",
    "# Use >> to denote that goodbye_world_task depends on hello_world_task\n",
    "hello_world_task >> goodbye_world_task\n",
    "```\n",
    "\n",
    "Tasks dependencies can also be set with \"set_downstream\" and \"set_upstream\"\n",
    "\n",
    "* `a.set_downstream(b)` means a comes before b\n",
    "* `a.set_upstream(b)` means a comes after b\n",
    "\n",
    "```python\n",
    "\n",
    "hello_world_task = PythonOperator(task_id=’hello_world’, ...)\n",
    "goodbye_world_task = PythonOperator(task_id=’goodbye_world’, ...)\n",
    "...\n",
    "hello_world_task.set_downstream(goodbye_world_task)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Task Dependencies\n",
    "**Instructions** :  Define tasks and graphs in this exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exercises/exercise3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exercises/exercise3.py\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "\n",
    "def hello_world():\n",
    "    logging.info(\"Hello World\")\n",
    "\n",
    "\n",
    "def addition():\n",
    "    logging.info(f\"2 + 2 = {2+2}\")\n",
    "\n",
    "\n",
    "def subtraction():\n",
    "    logging.info(f\"6 -2 = {6-2}\")\n",
    "\n",
    "\n",
    "def division():\n",
    "    logging.info(f\"10 / 2 = {int(10/2)}\")\n",
    "\n",
    "\n",
    "dag = DAG(\n",
    "    \"lesson1.exercise3\",\n",
    "    schedule_interval='@hourly',\n",
    "    start_date=datetime.datetime.now() - datetime.timedelta(days=1))\n",
    "\n",
    "hello_world_task = PythonOperator(\n",
    "    task_id=\"hello_world\",\n",
    "    python_callable=hello_world,\n",
    "    dag=dag)\n",
    "\n",
    "addition_task = PythonOperator(\n",
    "    task_id=\"addition\",\n",
    "    python_callable=addition,\n",
    "    dag=dag\n",
    "    )\n",
    "\n",
    "subtraction_task = PythonOperator(\n",
    "    task_id=\"subtraction\",\n",
    "    python_callable=subtraction,\n",
    "    dag=dag\n",
    "    )\n",
    "#\n",
    "# TODO: Define a division task that calls the `division` function above\n",
    "#\n",
    "\n",
    "division_task = PythonOperator(\n",
    "    task_id=\"division\",\n",
    "    python_callable=division,\n",
    "    dag=dag\n",
    "    )\n",
    "\n",
    "#\n",
    "# Configuring the task dependencies such that the graph looks like the following:\n",
    "#\n",
    "#                    ->  addition_task\n",
    "#                   /                 \\\n",
    "#   hello_world_task                   -> division_task\n",
    "#                   \\                 /\n",
    "#                    ->subtraction_task\n",
    "\n",
    "hello_world_task >> addition_task\n",
    "hello_world_task >> subtraction_task\n",
    "\n",
    "addition_task >> division_task\n",
    "subtraction_task >> division_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp exercises/exercise3.py $AIRFLOW_HOME/dags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection via Airflow Hooks\n",
    "Connections can be accessed in code via hooks. Hooks provide a reusable interface to external systems and databases. With hooks, you don’t have to worry about how and where to store these connection strings and secrets in your code.\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.hooks.postgres_hook import PostgresHook\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "def load():\n",
    "# Create a PostgresHook option using the `demo` connection\n",
    "    db_hook = PostgresHook(‘demo’)\n",
    "    df = db_hook.get_pandas_df('SELECT * FROM rides')\n",
    "    print(f'Successfully used PostgresHook to return {len(df)} records')\n",
    "\n",
    "load_task = PythonOperator(task_id=’load’, python_callable=hello_world, ...)\n",
    "```\n",
    "\n",
    "Airflow comes with many Hooks that can integrate with common systems. Here are a few common ones:\n",
    "\n",
    "* `HttpHook`\n",
    "* `PostgresHook` (works with RedShift)\n",
    "* `MySqlHook`\n",
    "* `SlackHook`\n",
    "* `PrestoHook`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Connections and Hooks\n",
    "\n",
    "**Instructions**: We're going to create a connection and a\n",
    "variable.\n",
    "1. Open your browser to localhost:8080 and open Admin->Variables\n",
    "2. Click \"Create\"\n",
    "3. Set \"Key\" equal to \"s3_bucket\" and set \"Val\" equal to \"udacity-dend\"\n",
    "4. Set \"Key\" equal to \"s3_prefix\" and set \"Val\" equal to \"data-pipelines\"\n",
    "5. Click save\n",
    "6. Open Admin->Connections\n",
    "7. Click \"Create\"\n",
    "8. Set \"Conn Id\" to \"aws_credentials\", \"Conn Type\" to \"Amazon Web Services\"\n",
    "9. Set \"Login\" to your aws_access_key_id and \"Password\" to your aws_secret_key\n",
    "10. Click save\n",
    "11. Run the DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exercises/exercise4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exercises/exercise4.py\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.dummy_operator import DummyOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.hooks import S3_hook\n",
    "from airflow.models import Variable\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "def list_keys():\n",
    "    hook = S3_hook.S3Hook('aws_credentials')\n",
    "    bucket = Variable.get('s3_bucket')\n",
    "    prefix = Variable.get('s3_prefix')\n",
    "    logging.info(f\"Listing Keys from {bucket}/{prefix}\")\n",
    "    keys = hook.list_keys(bucket, prefix=prefix)\n",
    "    for key in keys:\n",
    "        logging.info(f\"- s3://{bucket}/{key}\")\n",
    "\n",
    "dag = DAG('lesson1.exercise4', start_date=datetime.datetime.now() - datetime.timedelta(days=1))\n",
    "\n",
    "list_task = PythonOperator(\n",
    "    task_id=\"list_keys\",\n",
    "    python_callable=list_keys,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "list_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp exercises/exercise4.py $AIRFLOW_HOME/dags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context and Templating\n",
    "[Here](https://airflow.apache.org/macros.html) is the Apache Airflow documentation on **context variables** that can be included as kwargs.\n",
    "\n",
    "Here is a link to a [blog post](https://blog.godatadriven.com/zen-of-python-and-apache-airflow) that also discusses this topic.\n",
    "\n",
    "### Runtime Variables\n",
    "Airflow leverages templating to allow users to \"fill in the blank\" with important runtime variables for tasks.\n",
    "\n",
    "```python\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "def hello_date(*args, **kwargs):\n",
    "    print(f\"Hello {kwargs['execution_date']}\")\n",
    "\n",
    "divvy_dag = DAG(...)\n",
    "task = PythonOperator(\n",
    "    task_id=\"hello_date\",\n",
    "    python_callable=hello_date,\n",
    "    provide_context=True,\n",
    "    dag=divvy_dag)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Context and Templating\n",
    "**Instructions**: Use the Airflow context in the pythonoperator to complete the TODOs below. Once you are done, run your DAG and check the logs to see the context in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exercises/exercise5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exercises/exercise5.py\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.models import Variable\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.hooks.S3_hook import S3Hook\n",
    "\n",
    "\n",
    "def log_details(*args, **kwargs):\n",
    "    #\n",
    "    # TODO: Extract ds, run_id, prev_ds, and next_ds from the kwargs, and log them\n",
    "    # NOTE: Look here for context variables passed in on kwargs:\n",
    "    #       https://airflow.apache.org/macros.html\n",
    "    #\n",
    "    ds = kwargs['ds'] # kwargs[]\n",
    "    run_id = kwargs['run_id'] # kwargs[]\n",
    "    previous_ds = kwargs.get('prev_ds') # kwargs.get('')\n",
    "    next_ds = kwargs.get('next_ds') # kwargs.get('')\n",
    "\n",
    "    logging.info(f\"Execution date is {ds}\")\n",
    "    logging.info(f\"My run id is {run_id}\")\n",
    "    if previous_ds:\n",
    "        logging.info(f\"My previous run was on {previous_ds}\")\n",
    "    if next_ds:\n",
    "        logging.info(f\"My next run will be {next_ds}\")\n",
    "\n",
    "dag = DAG(\n",
    "    'lesson1.exercise5',\n",
    "    schedule_interval=\"@daily\",\n",
    "    start_date=datetime.datetime.now() - datetime.timedelta(days=2)\n",
    ")\n",
    "\n",
    "list_task = PythonOperator(\n",
    "    task_id=\"log_details\",\n",
    "    python_callable=log_details,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp exercises/exercise5.py $AIRFLOW_HOME/dags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Review of Pipeline Components\n",
    "\n",
    "### QUESTION 1 OF 2\n",
    "Match the following definitions to the the component they describe.\n",
    "\n",
    "|DEFINITION|COMPONENT|\n",
    "|-----------|---------|\n",
    "|A collection of nodes and edges that describe the order of operations for a data pipeline|DAG|\n",
    "|An instantiated step in a pipeline fully parameterized for execution|Task|\n",
    "|A reusable connection to an external database or system|Hook|\n",
    "|An abstract building block that can be configured to perform some work|Operator|\n",
    "\n",
    "### QUESTION 2 OF 2\n",
    "Which of the following constructs a DAG that runs task \"B\", then \"C\", then \"A\"?\n",
    "- [ ] A >> B >> C\n",
    "- [x] B >> C >> A\n",
    "- [ ] C >> A >> B\n",
    "- [ ] B >> A >> C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Build the S3 to Redshift DAG\n",
    "\n",
    "**Instructions**: Copy and populate the trips table. Then, add another operator which creates a traffic analysis table from the trips table you created. Note, in this class, we won’t be writing SQL -- all of the SQL statements we run against Redshift are predefined and included in your lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exercises/exercise6.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exercises/exercise6.py\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.contrib.hooks.aws_hook import AwsHook\n",
    "from airflow.hooks.postgres_hook import PostgresHook\n",
    "from airflow.operators.postgres_operator import PostgresOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "import sql_statements\n",
    "\n",
    "\n",
    "def load_data_to_redshift(*args, **kwargs):\n",
    "    aws_hook = AwsHook(\"aws_credentials\")\n",
    "    credentials = aws_hook.get_credentials()\n",
    "    redshift_hook = PostgresHook(\"redshift\")\n",
    "    redshift_hook.run(sql_statements.COPY_ALL_TRIPS_SQL.format(credentials.access_key, credentials.secret_key))\n",
    "\n",
    "\n",
    "dag = DAG(\n",
    "    'lesson1.exercise6',\n",
    "    start_date=datetime.datetime.now() - datetime.timedelta(days=1)\n",
    ")\n",
    "\n",
    "create_table = PostgresOperator(\n",
    "    task_id=\"create_table\",\n",
    "    dag=dag,\n",
    "    postgres_conn_id=\"redshift\",\n",
    "    sql=sql_statements.CREATE_TRIPS_TABLE_SQL\n",
    ")\n",
    "\n",
    "copy_task = PythonOperator(\n",
    "    task_id='load_from_s3_to_redshift',\n",
    "    dag=dag,\n",
    "    python_callable=load_data_to_redshift\n",
    ")\n",
    "\n",
    "location_traffic_task = PostgresOperator(\n",
    "    task_id=\"calculate_location_traffic\",\n",
    "    dag=dag,\n",
    "    postgres_conn_id=\"redshift\",\n",
    "    sql=sql_statements.LOCATION_TRAFFIC_SQL\n",
    ")\n",
    "\n",
    "create_table >> copy_task\n",
    "copy_task >> location_traffic_task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exercises/sql_statements.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exercises/sql_statements.py\n",
    "\n",
    "CREATE_TRIPS_TABLE_SQL = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS trips (\n",
    "trip_id INTEGER NOT NULL,\n",
    "start_time TIMESTAMP NOT NULL,\n",
    "end_time TIMESTAMP NOT NULL,\n",
    "bikeid INTEGER NOT NULL,\n",
    "tripduration DECIMAL(16,2) NOT NULL,\n",
    "from_station_id INTEGER NOT NULL,\n",
    "from_station_name VARCHAR(100) NOT NULL,\n",
    "to_station_id INTEGER NOT NULL,\n",
    "to_station_name VARCHAR(100) NOT NULL,\n",
    "usertype VARCHAR(20),\n",
    "gender VARCHAR(6),\n",
    "birthyear INTEGER,\n",
    "PRIMARY KEY(trip_id))\n",
    "DISTSTYLE ALL;\n",
    "\"\"\"\n",
    "\n",
    "CREATE_STATIONS_TABLE_SQL = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS stations (\n",
    "id INTEGER NOT NULL,\n",
    "name VARCHAR(250) NOT NULL,\n",
    "city VARCHAR(100) NOT NULL,\n",
    "latitude DECIMAL(9, 6) NOT NULL,\n",
    "longitude DECIMAL(9, 6) NOT NULL,\n",
    "dpcapacity INTEGER NOT NULL,\n",
    "online_date TIMESTAMP NOT NULL,\n",
    "PRIMARY KEY(id))\n",
    "DISTSTYLE ALL;\n",
    "\"\"\"\n",
    "\n",
    "COPY_SQL = \"\"\"\n",
    "COPY {}\n",
    "FROM '{}'\n",
    "ACCESS_KEY_ID '{{}}'\n",
    "SECRET_ACCESS_KEY '{{}}'\n",
    "IGNOREHEADER 1\n",
    "DELIMITER ','\n",
    "\"\"\"\n",
    "\n",
    "COPY_MONTHLY_TRIPS_SQL = COPY_SQL.format(\n",
    "    \"trips\",\n",
    "    \"s3://udacity-dend/data-pipelines/divvy/partitioned/{year}/{month}/divvy_trips.csv\"\n",
    ")\n",
    "\n",
    "COPY_ALL_TRIPS_SQL = COPY_SQL.format(\n",
    "    \"trips\",\n",
    "    \"s3://udacity-dend/data-pipelines/divvy/unpartitioned/divvy_trips_2018.csv\"\n",
    ")\n",
    "\n",
    "COPY_STATIONS_SQL = COPY_SQL.format(\n",
    "    \"stations\",\n",
    "    \"s3://udacity-dend/data-pipelines/divvy/unpartitioned/divvy_stations_2017.csv\"\n",
    ")\n",
    "\n",
    "LOCATION_TRAFFIC_SQL = \"\"\"\n",
    "BEGIN;\n",
    "DROP TABLE IF EXISTS station_traffic;\n",
    "CREATE TABLE station_traffic AS\n",
    "SELECT\n",
    "    DISTINCT(t.from_station_id) AS station_id,\n",
    "    t.from_station_name AS station_name,\n",
    "    num_departures,\n",
    "    num_arrivals\n",
    "FROM trips t\n",
    "JOIN (\n",
    "    SELECT\n",
    "        from_station_id,\n",
    "        COUNT(from_station_id) AS num_departures\n",
    "    FROM trips\n",
    "    GROUP BY from_station_id\n",
    ") AS fs ON t.from_station_id = fs.from_station_id\n",
    "JOIN (\n",
    "    SELECT\n",
    "        to_station_id,\n",
    "        COUNT(to_station_id) AS num_arrivals\n",
    "    FROM trips\n",
    "    GROUP BY to_station_id\n",
    ") AS ts ON t.from_station_id = ts.to_station_id\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp exercises/sql_statements.py $AIRFLOW_HOME/dags\n",
    "!cp exercises/exercise6.py $AIRFLOW_HOME/dags"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
